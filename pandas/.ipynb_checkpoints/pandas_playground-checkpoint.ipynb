{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effective Pandas Book Runthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', 30)\n",
    "pd.set_option(\"display.max_colwidth\",3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas df with duckdb\n",
    "import duckdb as db\n",
    "\n",
    "con = db.connect(':memory:')\n",
    "parquet_dir = r\"E:/app_data/sec_apps_data/speed_test/filings_13f_full/filings_parquet\"\n",
    "\n",
    "%time df = db.from_parquet(f\"{parquet_dir}/811360*.parquet\", connection=con).df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas df with vaex\n",
    "\n",
    "import vaex\n",
    "cikmanager = 811360\n",
    "%time df1 = vaex.open(f\"{parquet_dir}/{cikmanager}*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = {\n",
    "'index ':[0, 1, 2, 3],\n",
    "'data ':[145, 142, 38, 13],\n",
    "'name':'songs'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "songs2 = pd.Series([145, 142, 38, 13], name='counts')\n",
    "songs2\n",
    "songs2.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "nan_series = pd.Series([2, np.nan],\n",
    "index=['Ono', 'Clapton '])\n",
    "\n",
    "nan_series\n",
    "nan_series.count()\n",
    "nan_series.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_series2 = pd.Series([2, None],\n",
    "                        index=['Ono', 'Clapton '],\n",
    "                        dtype='Int64')\n",
    "nan_series2\n",
    "nan_series2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_series2.astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_ser  = np.array([145, 142, 38, 13])\n",
    "numpy_ser[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs3 = pd.Series([145, 142, 38, 13],\n",
    "name='counts ',\n",
    "index=['Paul', 'John', 'George ', 'Ringo '])\n",
    "\n",
    "songs3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not filtering, but identifying rows as per condition\n",
    "songs3 > songs3.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to filter rows based on rows values in certain columns\n",
    "songs3[songs3 > songs3.median()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs3.mean()\n",
    "numpy_ser.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = songs3 > songs3.mean() # boolean array\n",
    "mask\n",
    "songs3[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy equivalent of a mask\n",
    "numpy_ser[numpy_ser > np.median(numpy_ser)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories\n",
    "s = pd.Series(['m', 'l', 'xs', 's', 'xl'], dtype='category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s\n",
    "s.cat.ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordering it\n",
    "s2 = pd.Series(['m', 'l', 'xs', 's', 'xl'])\n",
    "size_type = pd.api.types.CategoricalDtype(categories=[ 's', 'm', 'l'], ordered=True)\n",
    "s3 = s2.astype(size_type)\n",
    "s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 > 's'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.cat.reorder_categories(['xs','s','m','l', 'xl'],\n",
    "ordered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.str.upper()\n",
    "s.str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_temp = pd.Series([32, 29, 20, 26, 31, 25, 30], dtype='int64')\n",
    "s_temp\n",
    "s_temp.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_temp[s_temp > s_temp.mean()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_colours = pd.Series(['blue', 'red', 'green', 'yellow', 'orange', 'purple', 'pink'], dtype='category')\n",
    "s_colours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Series Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://github.com/mattharrison/datasets/raw/master/data/' \\\n",
    " 'vehicles.csv.zip'\n",
    "df = pd.read_csv(url)\n",
    "city_mpg = df.city08\n",
    "highway_mpg = df.highway08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_mpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dir(city_mpg))\n",
    "dir(city_mpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(city_mpg + highway_mpg) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.Series([10, 20, 30], index=[1,2,2])\n",
    "s2 = pd.Series([35, 44, 53], index=[2,2,4], name='s2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1.add(s2, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((city_mpg + highway_mpg) / 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using chaining\n",
    "(city_mpg\n",
    "    .add(highway_mpg)\n",
    "    .div(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean\n",
    "city_mpg.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_mpg.is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_mpg.is_monotonic_increasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pd.Series([1, 2, 3, 4 ])\n",
    "f.is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.is_monotonic_increasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_mpg.quantile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_mpg.quantile([0.25, 0.5, 0.75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_mpg.quantile(0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import polars as pl\n",
    "\n",
    "df = pl.read_csv(\"https://j.mp/iriscsv\")\n",
    "print(df.filter(pl.col(\"sepal_length\") > 5)\n",
    "      .groupby(\"species\")\n",
    "      .agg(pl.all().sum())\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# lazy way\n",
    "import polars as pl\n",
    "\n",
    "print(\n",
    "    pl.read_csv(\"https://j.mp/iriscsv\")\n",
    "    .lazy()\n",
    "    .filter(pl.col(\"sepal_length\") > 5)\n",
    "    .groupby(\"species\")\n",
    "    .agg(pl.all().sum())\n",
    "    .collect()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# making df lazy\n",
    "# filtering the df based on one column\n",
    "(\n",
    "    df.lazy()\n",
    "    .filter(pl.col(\"sepal_length\") > 5)\n",
    "    .groupby(\"species\")\n",
    "    .agg(pl.all().sum())\n",
    "    .collect()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.select([\n",
    "    \n",
    "    pl.col(\"species\").sort().head(2),\n",
    "    pl.col(\"sepal_length\").filter(pl.col(\"sepal_length\") == 5).sum()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(12)\n",
    "\n",
    "df = pl.DataFrame(\n",
    "    {\n",
    "        \"nrs\": [1, 2, 3, None, 5],\n",
    "        \"names\": [\"foo\", \"ham\", \"spam\", \"egg\", None],\n",
    "        \"random\": np.random.rand(5),\n",
    "        \"groups\": [\"A\", \"A\", \"B\", \"C\", \"B\"],\n",
    "    }\n",
    ")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# unique values\n",
    "out = df.select(\n",
    "    [\n",
    "        pl.col(\"names\").n_unique().alias(\"unique_names_1\"),\n",
    "        pl.col(\"names\").unique().count().alias(\"unique_names_2\"),\n",
    "    ]\n",
    ")\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "out = df.select(\n",
    "    [\n",
    "        pl.sum(\"random\").alias(\"sum\"),\n",
    "        pl.min(\"random\").alias(\"min\"),\n",
    "        pl.max(\"random\").alias(\"max\"),\n",
    "        pl.col(\"random\").max().alias(\"other_max\"),\n",
    "        pl.std(\"random\").alias(\"std dev\"),\n",
    "        pl.var(\"random\").alias(\"variance\"),\n",
    "        pl.last(\"random\").alias(\"last_var\"),\n",
    "        pl.first(\"random\").alias(\"first_var\"),\n",
    "    ]\n",
    ")\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = df.select(\n",
    "    [\n",
    "        pl.col(\"names\").filter(pl.col(\"names\").str.contains(r\"o$\")).count(),\n",
    "    ]\n",
    ")\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polars with my data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import polars as pl\n",
    "parquet_dir = r\"E:\\app_data\\sec_apps_data\\speed_test\\filings_13f_full\\filings_parquet\"\n",
    "\n",
    "\n",
    "df_filings = (pl.scan_parquet(f\"{parquet_dir}/949509-*.parquet\")\n",
    "              .filter((pl.col(\"cikManager\") == 949509) & (pl.col(\"filedAsOfDate\") >= \"2014-01-01\" ) \n",
    "                      #& (pl.col(\"cusip\") == \"01609W102\" ))\n",
    "              ))\n",
    " \n",
    "    # .filter(pl.col(\"sepal_length\") > 5)\n",
    "    # .groupby(\"species\")\n",
    "    # .agg(pl.all().sum())\n",
    "    #.collect()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filings.fetch(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_berk_2014up = df_filings.collect()\n",
    "df_berk_2014up.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (df_berk_2014up.filter(pl.col(\"report_Year\") == 2014)\n",
    "                  .groupby([\"cikManager\", \"report_Year\", \"report_Quarter\" ])\n",
    "                #   .agg(pl.col(\"sharesHeldAtEndOfQtr\").sum())\n",
    "                  .agg(pl.col(\"sharesHeldAtEndOfQtr\").last()) # \n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = (df_berk_2014up.filter(pl.col(\"report_Year\") == 2014)\n",
    "#                   .groupby([\"cikManager\",\"report_Year\" ])\n",
    "#                 #   .agg(pl.col(\"sharesHeldAtEndOfQtr\").sum())\n",
    "#                   .agg(pl.col(\"sharesHeldAtEndOfQtr\").sum()) # \n",
    "#                   )\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lazy mode\n",
    "(df.lazy()\n",
    "    .filter(pl.col(\"report_Quarter\") != 1)\n",
    "    .groupby(\"report_Quarter\")\n",
    "    .agg(pl.all().sum())\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Expressions\n",
    "pl.col(\"foo\").sort(reverse=True).head(2)\n",
    "\n",
    "df.select([pl.col(\"report_Quarter\").sort(reverse=True).head(1),\n",
    "           pl.col(\"sharesHeldAtEndOfQtr\").filter(pl.col(\"report_Quarter\") == 3).min()\n",
    "           \n",
    "           \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## unique values\n",
    "out = df_filings.select([\n",
    "    pl.col(\"cusip\").n_unique().alias(\"unique_cusip_1\"),\n",
    "    pl.col(\"cusip\").unique().count().alias(\"unique_cusip_2\"),\n",
    "                         \n",
    "                         \n",
    "                         ])\n",
    "print(out.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## aggregations\n",
    "\n",
    "out = df_filings.select([\n",
    "    pl.sum(\"sharesHeldAtEndOfQtr\").alias(\"shares_sum\"),\n",
    "    pl.min(\"sharesHeldAtEndOfQtr\").alias(\"shares_min\"),\n",
    "    pl.max(\"sharesHeldAtEndOfQtr\").alias(\"shares_max\"),\n",
    "    pl.col(\"sharesHeldAtEndOfQtr\").max().alias(\"shares_other_max\"),\n",
    "    pl.std(\"sharesHeldAtEndOfQtr\").alias(\"shares_std_dev\"),\n",
    "    pl.var(\"sharesHeldAtEndOfQtr\").alias(\"variance\"),\n",
    "    pl.col(\"report_Quarter\").alias(\"quarter\"),\n",
    "    pl.col(\"sharesHeldAtEndOfQtr\").alias(\"shares\"),\n",
    "    pl.col(\"sharesHeldAtEndOfQtr\").shift(-1).alias(\"shares_prev\"),\n",
    "    pl.col(\"cusip\").str.to_uppercase().alias(\"cusip\")\n",
    "                         \n",
    "                         \n",
    "                         \n",
    "                         ])\n",
    "print(out.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.Config.set_tbl_rows(50)\n",
    "pl.Config.set_tbl_cols(50)\n",
    "pl.Config.set_tbl_width_chars(50)\n",
    "pl.Config.set_fmt_str_lengths(50)\n",
    "\n",
    "(df_filings \n",
    "    .sort([\"cikManager\", \"cusip\", \"periodOfReport\",\"filedAsOfDate\", \"accessionNumber\"], reverse=True)\n",
    "    .select([\n",
    "      \n",
    "    pl.col(\"cusip\"),\n",
    "    (pl.col(\"periodOfReport\").dt.year()+\"Q\"+pl.col(\"periodOfReport\").dt.quarter()).alias(\"quarter\"),\n",
    "    pl.col(\"periodOfReport\"),\n",
    "    #pl.col(\"filedAsOfDate\").alias(\"fdate\"),\n",
    "    pl.col(\"sharesHeldAtEndOfQtr\").first().over([\"cikManager\", \"cusip\"]).alias(\"shares\"),\n",
    "    # pl.col(\"sharesHeldAtEndOfQtr\").sum().over([\"cikManager\", \"cusip\", \"periodOfReport\"]).alias(\"shares_sum\"),\n",
    "    pl.col(\"sharesHeldAtEndOfQtr\").list().over([\"cusip\", \"periodOfReport\"]).alias(\"shares_list\")\n",
    "    ])\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_filings\n",
    "    .groupby([\"cusip\", (pl.col(\"periodOfReport\").dt.year()+\"Q\"+pl.col(\"periodOfReport\").dt.quarter()).alias(\"quarter\")])\n",
    "    .agg([pl.col(\"sharesHeldAtEndOfQtr\").first(),\n",
    "          #pl.col(\"cusip\").unique(),\n",
    "          pl.col(\"filedAsOfDate\").first(),\n",
    "          pl.col(\"accessionNumber\").first()\n",
    "          ])\n",
    "    .sort([\"cusip\", \"quarter\",\"filedAsOfDate\", \"accessionNumber\"], reverse=True)\n",
    "    ).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_cusips = [\"000000000\", \"\", \"0\"]\n",
    "# (pl.col(\"cusip\") != \"000000000\") & (pl.col(\"cusip\") != \"\") & (pl.col(\"cusip\") != \"0\")\n",
    "(df_filings\n",
    "    .filter((pl.col(\"cusip\").is_in(bad_cusips).is_not())) # filter out bad cusips\n",
    "    .groupby([pl.col(\"periodOfReport\").dt.year()])\n",
    "    .agg([\n",
    "        pl.col(\"accessionNumber\").first(),\n",
    "        pl.col(\"accessionNumber\").str.slice(-6).cast(pl.Int32).first().alias(\"new_accession_number\"),\n",
    "        pl.count().alias(\"count\"),\n",
    "        pl.col(\"submissionType\"),\n",
    "        pl.first(\"cusip\")       \n",
    "    ])\n",
    "    \n",
    "    .sort(\"accessionNumber\", reverse=True)\n",
    "    .limit(5)\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(df_filings\n",
    "    .filter((pl.col(\"cusip\").is_in(bad_cusips).is_not())) # filter out bad cusips\n",
    "    .sort([\"cusip\", \"periodOfReport\", \"filedAsOfDate\", \"accessionNumber\"], reverse=True)\n",
    "    .groupby([\"cusip\", (pl.col(\"periodOfReport\").dt.year()+\"Q\"+pl.col(\"periodOfReport\").dt.quarter()).alias(\"quarter\")])\n",
    "    .agg([\n",
    "        pl.col(\"periodOfReport\").first().alias(\"periodOfReport\"),\n",
    "        pl.count().alias(\"count\"),\n",
    "        pl.col(\"ID\").count().alias(\"ID_count\"),\n",
    "        pl.col(\"filedAsOfDate\").list().alias(\"list_of_fdates\"),\n",
    "        pl.col(\"sharesHeldAtEndOfQtr\").list().alias(\"list_of_shares\"),\n",
    "        pl.col(\"sharesHeldAtEndOfQtr\").first().alias(\"new_accession_number\"),\n",
    "        \n",
    "        pl.col(\"filedAsOfDate\").first().alias(\"filedAsOfDate\")\n",
    "        \n",
    "       \n",
    "    ])\n",
    "    \n",
    "    .sort([\"periodOfReport\", \"filedAsOfDate\"], reverse=True)\n",
    "    #.limit(1)\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Conditional aggregations\n",
    "import polars as pl\n",
    "\n",
    "bad_cusips = [\"000000000\", \"\", \"0\"]\n",
    "(df_filings\n",
    "    .filter((pl.col(\"cusip\").is_in(bad_cusips).is_not())) # filter out bad cusips\n",
    "    .sort([\"cusip\", \"periodOfReport\", \"filedAsOfDate\", \"accessionNumber\"], reverse=True)\n",
    "    .groupby([\"cusip\", (pl.col(\"periodOfReport\").dt.year()+\"Q\"+pl.col(\"periodOfReport\").dt.quarter()).alias(\"quarter\")])\n",
    "    .agg([\n",
    "        pl.col(\"periodOfReport\").first().alias(\"periodOfReport\"),\n",
    "        \n",
    "        pl.col(\"ID\").count().alias(\"ID_count\"),\n",
    "        pl.col(\"filedAsOfDate\").list().alias(\"list_of_fdates\"),\n",
    "        pl.col(\"sharesHeldAtEndOfQtr\").list().alias(\"list_of_shares\"),\n",
    "        pl.col(\"sharesHeldAtEndOfQtr\").first().alias(\"new_accession_number\"),\n",
    "        pl.col(\"filedAsOfDate\").first().alias(\"filedAsOfDate\"),\n",
    "        pl.col(\"accessionNumber\").str.slice(-6).cast(pl.Int32).first().alias(\"accession_rank\"),\n",
    "        (pl.col(\"submissionType\") == \"13F-HR\").sum().alias(\"original\"),\n",
    "        (pl.col(\"submissionType\") == \"13F-HR/A\").sum().alias(\"amendement\")\n",
    "        \n",
    "       \n",
    "    ])\n",
    "    \n",
    "    .sort([\"periodOfReport\", \"filedAsOfDate\"], reverse=True)\n",
    "    #.limit(1)\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "bad_cusips = [\"000000000\", \"\", \"0\"]\n",
    "(df_filings\n",
    "    .filter((pl.col(\"cusip\").is_in(bad_cusips).is_not())) # filter out bad cusips\n",
    "    .sort([\"cusip\", \"periodOfReport\", \"filedAsOfDate\", \"accessionNumber\"], reverse=True)\n",
    "    .groupby([\"cusip\", \"submissionType\"])\n",
    "    .agg([(pl.col(\"submissionType\").count()).alias(\"submission_count\")\n",
    "    .filter((pl.col(\"submissionType\") == \"13F-HR\") | (pl.col(\"submissionType\") == \"13F-HR/A\"))\n",
    "    \n",
    "      \n",
    "       \n",
    "    ])\n",
    "    .sort(\"submission_count\", reverse=True)\n",
    "    \n",
    "    \n",
    "    #.limit(1)\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply filtering within a group and use a custom function\n",
    "import polars as pl\n",
    "\n",
    "def filing_age() -> pl.Expr:\n",
    "    # return pl.col(\"filedAsOfDate\").cast(pl.DateType()) - pl.col(\"periodOfReport\").cast(pl.DateType())\n",
    "    return pl.col(\"filedAsOfDate\") - pl.col(\"periodOfReport\")\n",
    "\n",
    "def avg_count_submission_type(type: str) -> pl.Expr:\n",
    "    return filing_age().filter(pl.col(\"submissionType\") == type).mean().alias(f\"Average filing age for {type}\")\n",
    "\n",
    "\n",
    "\n",
    "bad_cusips = [\"000000000\", \"\", \"0\"]\n",
    "(df_filings\n",
    "    .groupby([\"cusip\", \"periodOfReport\"])\n",
    "    .agg([\n",
    "    avg_count_submission_type(\"13F-HR\"),\n",
    "    avg_count_submission_type(\"13F-HR/A\"),\n",
    "    (pl.col(\"submissionType\") == \"13F-HR\").sum().alias(\"original\"),\n",
    "    (pl.col(\"submissionType\") == \"13F-HR/A\").sum().alias(\"amendement\"),\n",
    "       \n",
    "    ])\n",
    "    \n",
    "    #.limit(1)\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sorting and then grouping aaaand dorting again within a group\n",
    "# we can also sort by another column in a group (still needs mental clarification )\n",
    "\n",
    "\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "def get_cusip_filedat() -> pl.Expr:\n",
    "    return pl.lit(\"Filed on..\") + pl.col(\"filedAsOfDate\")\n",
    "\n",
    "bad_cusips = [\"000000000\", \"\", \"0\"]\n",
    "(df_filings\n",
    "    .sort([\"cusip\",\"periodOfReport\", \"filedAsOfDate\"], reverse=True)\n",
    "    .groupby([\"cusip\", \"periodOfReport\"])\n",
    "    .agg([\n",
    "        get_cusip_filedat().first().alias(\"last filing\"),\n",
    "        get_cusip_filedat().last().alias(\"first filing\"),\n",
    "        get_cusip_filedat().sort().last().alias(\"alphabetical filing\"),\n",
    "        pl.col(\"submissionType\").sort_by(\"filedAsOfDate\").first().alias(\"first filing type\"),\n",
    "        \n",
    "          \n",
    "    ])\n",
    "    .limit(5)\n",
    "    \n",
    "    ).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold\n",
    "out = df_filings.select(\n",
    "    pl.fold(acc=pl.lit(0), f=lambda acc, x: acc + x, exprs=pl.col(\"*\")).alias(\"sum\"),\n",
    ")\n",
    "out.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## not sure how this works\n",
    "\n",
    "out = df_filings.filter(\n",
    "    pl.fold(acc=pl.lit(True), f=lambda acc, x: acc & x, exprs=pl.col(\"report_Quarter\") >1),\n",
    ")\n",
    "out.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fold and concat_str\n",
    "\n",
    "out = df_filings.select([\n",
    "        pl.concat_str([\"report_Year\", pl.lit(\"Q\"), \"report_Quarter\"])\n",
    "    \n",
    "                 ])\n",
    "out.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split the string and get the necessary element of the list\n",
    "\n",
    "out = df_filings.select([\n",
    "        pl.concat_str([\"report_Year\", pl.lit(\"Q\"), \"report_Quarter\"]),\n",
    "        pl.col(\"accessionNumber\").str.split(by=\"-\").arr.get(-1).cast(pl.UInt32)\n",
    "    \n",
    "                 ])\n",
    "out.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## window functions\n",
    "\n",
    "from aioitertools import groupby\n",
    "\n",
    "\n",
    "out = df_filings.select([\n",
    "    \"report_Year\",\n",
    "    \"report_Quarter\",\n",
    "    \"sharesHeldAtEndOfQtr\",\n",
    "    pl.col(\"valueTotal\").mean().over(\"report_Year\").alias(\"mean_valueTotal_by_year\"),\n",
    "    pl.col(\"sharesHeldAtEndOfQtr\").mean().over([\"report_Year\"]).alias(\"mean_shares_by_year\"),\n",
    "    pl.col(\"sharesHeldAtEndOfQtr\").mean().over([\"report_Year\", \"report_Quarter\"]).alias(\"mean_shares_by_year_and_quarter\"),\n",
    "    pl.col(\"sharesHeldAtEndOfQtr\").mean().alias(\"mean_shares\")\n",
    "    \n",
    "    \n",
    "    \n",
    "])\n",
    "(out\n",
    ".groupby([\"report_Year\", \"report_Quarter\"]).agg([pl.col(\"sharesHeldAtEndOfQtr\").first()])\n",
    ".sort([\"report_Year\", \"report_Quarter\"], reverse=True)).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## filter and select columns\n",
    "\n",
    "filtered = \\\n",
    "df_filings.filter(pl.col(\"cusip\") != \"\").select([\n",
    "    \"report_Year\",\n",
    "    \"cusip\",\n",
    "    \"sharesHeldAtEndOfQtr\",\n",
    "    \n",
    "])\n",
    "\n",
    "filtered.fetch(3)\n",
    "\n",
    "\n",
    "# let's sort it by cusip and then shares\n",
    "out = filtered.with_columns([\n",
    "    pl.col([\"cusip\", \"sharesHeldAtEndOfQtr\"]).sort(reverse=True).over(\"cusip\")\n",
    "    ])  \n",
    "out.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort and select\n",
    "\n",
    "# the different groppings by is confusing for fe for now\n",
    "out = df_filings.sort([\"cusip\", \"periodOfReport\"]).select([\n",
    "    pl.col(\"cusip\").head(3).list().over(\"cusip\").flatten(),\n",
    "    pl.col(\"periodOfReport\").sort_by(pl.col(\"sharesHeldAtEndOfQtr\"), reverse=True).head(3).list().over(\"cusip\").flatten().alias(\"fastest/group\")\n",
    "    #pl.col(\"\")\n",
    "    \n",
    "    \n",
    "])\n",
    "out.collect()\n",
    "\n",
    "# from the guide\n",
    "# out = df.sort(\"Type 1\").select(\n",
    "#     [\n",
    "#         pl.col(\"Type 1\").head(3).list().over(\"Type 1\").flatten(),\n",
    "#         pl.col(\"Name\").sort_by(pl.col(\"Speed\")).head(3).list().over(\"Type 1\").flatten().alias(\"fastest/group\"),\n",
    "#         pl.col(\"Name\").sort_by(pl.col(\"Attack\")).head(3).list().over(\"Type 1\").flatten().alias(\"strongest/group\"),\n",
    "#         pl.col(\"Name\").sort().head(3).list().over(\"Type 1\").flatten().alias(\"sorted_by_alphabet\"),\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "df_filings.select([\n",
    "    (pl.col(\"submissionType\") + pl.col(\"cikManager\")).alias(\"original\"),\n",
    "    (pl.col(\"sharesValue\") + pl.col(\"valueTotal\")).alias(\"sum\")\n",
    "    \n",
    "    \n",
    "]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from IPython.display import display\n",
    "import polars as pl\n",
    "pl.Config.set_tbl_rows(20)\n",
    "pl.Config.set_tbl_width_chars(50)\n",
    "pl.Config.set_fmt_str_lengths(50)\n",
    "#display(out.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "parquet_dir = r\"E:\\app_data\\sec_apps_data\\speed_test\\filings_13f_full\\filings_parquet\"\n",
    "\n",
    "\n",
    "df_filings = (pl.scan_parquet(f\"{parquet_dir}/949509-*.parquet\")\n",
    "              .filter((pl.col(\"cikManager\") == 949509) & (pl.col(\"filedAsOfDate\") >= \"2014-01-01\" ) \n",
    "                      #& (pl.col(\"cusip\") == \"01609W102\" ))\n",
    "              ))\n",
    " \n",
    "    # .filter(pl.col(\"sepal_length\") > 5)\n",
    "    # .groupby(\"species\")\n",
    "    # .agg(pl.all().sum())\n",
    "    #.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filings.collect().head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = grades.select([\n",
    "#     pl.concat_list(pl.all().exclude(\"student\")).alias(\"all_grades\")\n",
    "# ])\n",
    "\n",
    "# concatenating to list\n",
    "out = df_filings.select([pl.concat_list([\"valueTotal\", \"sharesValue\"]).alias(\"sum\")])\n",
    "out.fetch(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_pct = pl.element().rank(reverse=True) / pl.col(\"\").count()\n",
    "\n",
    "df_filings.with_column(\n",
    "    pl.concat_list([\"valueTotal\", \"sharesValue\"]).alias(\"values\")).select([\n",
    "        pl.all().exclude(\"values\"),\n",
    "        pl.col(\"values\").arr.eval(rank_pct, parallel=True).alias(\"values_rank\")\n",
    "    ]).collect().head(3)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas Course - Pluralsite - B3. Cleaning Data. Python Data Playbook (Chris Achard, 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"course/data/exercise/artwork_data.csv\")\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data.acquisitionYear.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.acquisitionYear.dtype\n",
    "\n",
    "data.acquisitionYear = data.acquisitionYear.astype(float)\n",
    "data.acquisitionYear.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulldf = pd.read_csv(r\"course/data/exercise/artwork_data.csv\", low_memory=False)\n",
    "fulldf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import full\n",
    "\n",
    "\n",
    "fulldf.dtypes\n",
    "# fulldf.height.astype(float)\n",
    "\n",
    "# pd.to_numeric(fulldf.height)\n",
    "fulldf.height[41339]\n",
    "\n",
    "pd.to_numeric(fulldf.height, errors=\"coerce\")\n",
    "pd.to_numeric(fulldf.height, errors=\"coerce\")[41339] # it turened the 'mm' to nan\n",
    "\n",
    "fulldf.height.dtype # still an object, we need to convert to float\n",
    "fulldf.height = pd.to_numeric(fulldf.height, errors=\"coerce\")\n",
    "\n",
    "fulldf.height.dtype # now it's a float\n",
    " \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"course/data/exercise/artwork_sample.csv\", low_memory=False)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.year = pd.to_numeric(data.year, errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes,\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.year.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.year.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.year.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.year.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.year.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.artist.min() # min() could be applied to a string too, but mean() would not make sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if to sum() string values, it will just concatenate them\n",
    "data.artist.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sum() # not much sense in doing it, but we can use sum() on the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.agg(\"min\") # this would get the min() of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.agg(['min', 'max']) # this would get the min() and max() of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.agg(['min', 'max', 'mean', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flipping the axis to get the mean of each column and not a row\n",
    "data.agg({'id' : ['sum', 'min'], 'depth' : ['sum', 'max']}) # this would get the mean of each numerical column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### example of my data and groupby\n",
    "# pandas reading\n",
    "import glob\n",
    "parquet_dir = r\"E:/app_data/sec_apps_data/speed_test/filings_13f_full/filings_parquet/\"\n",
    "import pandas as pd\n",
    "pandas_df = pd.read_parquet(glob.glob(f\"{parquet_dir}949509-*.parquet\"))\n",
    "pandas_df.sort_values(by=['cusip', 'periodOfReport', 'filedAsOfDate'],ascending=False) \\\n",
    "#groupby(['cusip', 'periodOfReport', 'filedAsOfDate'])['sharesHeldAtEndOfQtr'].agg(pd.Series.max).tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"course/data/exercise/artwork_sample.csv\", low_memory=False)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.height.mean()\n",
    "data.height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the height\n",
    "height = data.height\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = (height - height.mean()) / height.std()\n",
    "norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the height so it's between 0 and 1\n",
    "minmax = (height - height.min()) / (height.max() - height.min())\n",
    "minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax.min(), minmax.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.height = minmax\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['standardized_height'] = norm\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transforming data\n",
    "import pandas as pd\n",
    "data = pd.read_csv(r\"course/data/exercise/artwork_sample.csv\", low_memory=False)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.height.transform(lambda x: print(  x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.height.transform(lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.height.transform(lambda x: x/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " # transform on groupby\n",
    " data.groupby('artist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data.groupby('artist').transform('nunique')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data.artist.nunique()\n",
    "# data.groupby(['artist', 'medium']).transform('nunique')\n",
    "\n",
    "data.groupby('artist')['height'].transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['mean_height_by_artist'] = data.groupby('artist')['height'].transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data\n",
    "\n",
    "import pandas as pd\n",
    "data = pd.read_csv(r\"course/data/exercise/artwork_data.csv\", low_memory=False)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reducing the dataframe to only the columns we need\n",
    "data.filter(items=['id','artist', 'artistId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if I don't know the exact columns name\n",
    "data.filter(like='year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case insensitive regex\n",
    "data.filter(regex=\"(?i)year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use filter on rows too by using axis=0\n",
    "data.filter(axis=0, regex=\"^100.$\") # only fiters on index's values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # only even numbers of index\n",
    "data.filter(axis=0, regex=\"^100(0|2|4|6|8)$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dropping columns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"course/data/exercise/artwork_sample.csv\", low_memory=False)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(0)  #drop the first row with the index 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop a column\n",
    "data.drop('id',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['id', 'height', 'width', 'depth']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to drop rows we pass a list of labels\n",
    "data.drop(labels=[0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to drop permanently we use inplace=True\n",
    "data.drop(columns=['id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to avoid working with the columns we don't need in the first place, we can drop them when reading the data in\n",
    "data = pd.read_csv(r\"course/data/exercise/artwork_sample.csv\", low_memory=False, usecols=['artist', 'title'])\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chaning the casing of the columns\n",
    "data = pd.read_csv(r\"course/data/exercise/artwork_sample.csv\", low_memory=False)\n",
    "data.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[x.lower() for x in data.columns]\n",
    "data.columns = [x.lower() for x in data.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with map()\n",
    "data = pd.read_csv(r\"course/data/exercise/artwork_sample.csv\", low_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = map(lambda x: x.lower(), data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"course/data/exercise/artwork_sample.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with regex : we look for uppercase letters, and add '_' right before it and cast all to lowercase\n",
    "import re\n",
    "data.columns = [re.sub(r'([A-Z])', r'_\\1', x).lower() for x in data.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reanming columns\n",
    "data = pd.read_csv(r\"course/data/exercise/artwork_sample.csv\", low_memory=False)\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns={\"thumbnailUrl\": \"thumbnail\"})\n",
    "data.rename(columns={\"thumbnailUrl\": \"thumbnail\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns=lambda x: x.lower(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = ['id', 'Accession_number', 'Artist', 'Artistrole', 'artistid', 'title',\n",
    "       'datetext', 'medium', 'creditline', 'year', 'acquisitionyear',\n",
    "       'dimensions', 'width', 'height', 'depth', 'units', 'inscription',\n",
    "       'thumbnailcopyright', 'thumbnail', 'url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"course/data/exercise/artwork_sample.csv\", low_memory=False, names=\\\n",
    "    ['id', 'Accession_number', 'Artist', 'Artistrole', 'artistid', 'title',\n",
    "       'datetext', 'medium', 'creditline', 'year', 'acquisitionyear',\n",
    "       'dimensions', 'width', 'height', 'depth', 'units', 'inscription',\n",
    "       'thumbnailcopyright', 'thumbnail', 'url'], header=0)\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing and Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(r\"course/data/exercise/artwork_sample.csv\", low_memory=False)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['id'] # for one column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['id'][1] # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get multiple columns\n",
    "data[['artist', 'title']]\n",
    "# data.filter(items=['artist', 'title']) # same as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0:5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['artist', 'title']][1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[1:5]['artist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter on rows\n",
    "data[data['year'] > 1800] # we filter on row value and select all df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we want to filter on row and select only a subset of columns\n",
    "data[data.year > 1800][['artist', 'title']]\n",
    "data[data.year > 1800]['year']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loc and iloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(r\"course/data/exercise/artwork_sample.csv\", low_memory=False)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loc[rows, columns]\n",
    "# loc uses labels to select rows and columns. In case of index, in our case it's the row number, but it can be anythin else\n",
    "data.loc[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[0:1, ['artist', 'title']]\n",
    "\n",
    "data.loc[0:2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[0:2,'title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[0:2, ['artist', 'title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also use precise rows we need. Not the range of rows\n",
    "data.loc[[1,5], ['artist', 'title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also slise columns\n",
    "data.loc[0:1, 'id': 'title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also filter with loc\n",
    "data.loc[data.artist == 'Blake, Robert',:]\n",
    "data.loc[data.artist == 'Blake, Robert', 'title']\n",
    "data.loc[data.artist == 'Blake, Robert', 'title':'year']\n",
    "data.loc[data.artist == 'Blake, Robert', ['artist','year']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(r\"course/data/exercise/artwork_sample.csv\", low_memory=False)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iloc uses integer positions of rows and columns\n",
    "data.iloc[0,:]\n",
    "data.iloc[0:3,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to better see the difference it makes for the index, we can use a different index\n",
    "data.set_index('id', inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[0:3,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[1035:1037,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same integer positin is used for columns\n",
    "data.iloc[0:3,0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we reset the index inplace, all will go back to the original index\n",
    "data.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data\n",
    "data.iloc[0:3,0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select spedific rows and columns and not the range\n",
    "data.iloc[[5,8],[0,3, 7]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering with str.contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(r\"course/data/exercise/artwork_sample.csv\", low_memory=False)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.medium.str.contains('Graphite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can pass this to .loc\n",
    "data.loc[data.medium.str.contains('Graphite'), ['artist','title', 'medium']] # default ir case sensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data.medium.str.contains('Graphite', case=False), ['artist','title', 'medium']] # case insensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can do the same with regex\n",
    "data.loc[data.medium.str.contains('(?i)Graphite', regex=True), ['artist','title', 'medium']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple filters with regex\n",
    "data.loc[data.medium.str.contains('(?i)Graphite', regex=True) \\\n",
    "         | data.medium.str.contains('(?i)Line', regex=True), ['artist','title', 'medium']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simpler version\n",
    "data.loc[data.medium.str.contains('graphite|line', case=False, regex=True), ['artist','title', 'medium']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to use a str method on not a string, we first need to convert it to a string\n",
    "data.year.astype(str).str.contains('1826')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to filter\n",
    "data.loc[data.year.astype(str).str.contains('1826')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"course/data/exercise/artwork_data.csv\", low_memory=False)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when working with strings or non strings converted to strings, if there are NaNs, we need actiate na=False\n",
    "data.loc[data.dimensions.str.contains('support', na=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(r\"course/data/exercise/artwork_data.csv\", low_memory=False)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with whitespaces\n",
    "# this identifies rows where in the column 'title' there are whitespaces at the end of the string\n",
    "data.loc[data.title.str.contains('\\s$', regex=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this line deletes the whitespaces from the columln 'title'\n",
    "data.title.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.title = data.title.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data.title.str.contains('\\s$', regex=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.title.str.lstrip() # left strip\n",
    "data.title.str.rstrip() # right strip\n",
    "# with transform\n",
    "# a reminder - this only identifies rows, not deletes them\n",
    "data.title.transform(lambda x: x.str.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace bad data with NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(r\"course/data/exercise/artwork_data.csv\", low_memory=False)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the df has nans\n",
    "data.isna().any()\n",
    "# data.isna().all() # if an entire row has nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many nans in which columns\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().mean().mul(100) # percentage of nans in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask to identify the rows where there are NaNs for the column 'medium'\n",
    "pd.isna(data.loc[:, 'medium'])\n",
    "\n",
    "# and show the entire df but filter out only where there are nans in 'medium' \n",
    "data.loc[pd.isna(data.loc[:, 'medium'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.replace({'dateText': {'date not known': nan}}, inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.read_csv(r\"course/data/exercise/artwork_data.csv\", low_memory=False)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way to deal with selecting rows with what we see as bad data, then selecting only the needed column \n",
    "data.loc[data.dateText == 'date not known', ['dateText']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and then# assigning NaN to the bad rows of that column\n",
    "data.loc[data.dateText == 'date not known', ['dateText']] = nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select rows where the year is NOT nan and also has a value other than a number\n",
    "data.loc[data.year.notnull() & data.year.astype(str).str.contains('[^0-9]', regex=True)] \n",
    "# data.loc[data.year.astype(str).str.contains('[^0-9]', regex=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# be very careful with not forgetting to indicate which columns exactly needs to be set as NaN. If not, all df \n",
    "# will be set to NaN\n",
    "data.loc[data.year.notnull() & data.year.astype(str).str.contains('[^0-9]', regex=True), ['year']] = nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[67968:67969]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling missing data with value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"course/data/exercise/artwork_data.csv\", low_memory=False)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna(0)\n",
    "data.depth.fillna(0)\n",
    "data.fillna(value={'depth':0})\n",
    "data.fillna(value={'depth':0}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping rows of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"course/data/exercise/artwork_data.csv\", low_memory=False)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to drop rows with nan in the entire df, if a nan is in any column\n",
    "data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it leaves only rows without nans\n",
    "data.dropna(how='any').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.dropna() is the same as data.dropna(how='any')\n",
    " # dropna(how='all') drops rows where all the values in all the columns in a row are NaN\n",
    "data.dropna(how='all').shape # since the shape is the same as the original one, it means there are no rows with all nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(thresh=14).shape # thresh sets up how many columns must have a NaN in the dame row for it to be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to only drop rows with NaN in specific columns\n",
    "# data.dropna(subset=['year', 'acquisitionYear']).shape\n",
    "# the same as with 'any'\n",
    "# data.dropna(subset=['year', 'acquisitionYear'], how='any').shape # any of the columns must have a NaN\n",
    "data.dropna(subset=['year', 'acquisitionYear'], how='all').shape # all of the columns must have a NaN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to actually change the df\n",
    "data.dropna(subset=['year', 'acquisitionYear'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"course/data/exercise/artwork_sample.csv\", low_memory=False)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates() # drops rows with all columns in a row being duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(subset=['artist']) # drops rows with only some colulmns being duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(subset=['artist'], keep='first') # we can specify which rows to keep if there are duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(subset=['artist'], keep='last') # we can specify which rows to keep if there are duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(subset=['artist'], keep=False) # or we can drop both duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(subset=['artist'], keep='first', inplace=True) # we can specify which rows to keep if there are duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with full dataset\n",
    "data = pd.read_csv(r\"course/data/exercise/artwork_data.csv\", low_memory=False)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to just see the duplicates\n",
    "data.duplicated()  # returns only for fully duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show duplicats in specific columns\n",
    "data.duplicated(subset=['artist', 'title'], keep=False) # this is crucial to identify the duplicates. \n",
    "# We need to set the keep to False, so the duplicates are highlighted as True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data.duplicated(subset=['artist', 'title'], keep=False) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# findd a row with certain value in a column\n",
    "data.loc[data.title.str.contains('The Circle of the Lustful: Francesca da ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another Course on pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.\n",
    "\n",
    "(r\"E:/Downloads/_Pandas/Pandas Playbook. Manipulating Data (Reindert-Jan Ekker, 2018)/weather.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PRESSURE'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out the most common temperature\n",
    "df.TEMP.mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the values and their counts for a column\n",
    "df['TEMP'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some plotting\n",
    "df['TEMP'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TEMP'].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TEMP'].plot.hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r\"E:/Downloads/_Pandas/Pandas Playbook. Manipulating Data (Reindert-Jan Ekker, 2018)/weather.csv\").head()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TEMP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " df['TEMP'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = df.T\n",
    "dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft[2]['TIME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft[2][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.DataFrame([['John'], ['Bob'], ['Anne']], index=[4, 3, 4])\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[0][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['PRESSURE', 'TIME', 'TEMP']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TIME'][[3, 1, 4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[2:4][['TEMP', 'PRESSURE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['TEMP', 'PRESSURE']][2:4] # slices are only used for rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft[3:][[1, 3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PRESSURE'][2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft[:2] # with integer clicing, the end is not included, but with string slicing, the end IS included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft['TIME':'PRESSURE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loc and iloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capitals = pd.DataFrame(\n",
    "    [\n",
    "    [\"Ngerulmud\",391,1.87],\n",
    "    [\"Vatican City\",826,100],\n",
    "    [\"Yaren\",1100,10.91],\n",
    "    [\"Funafuti\",4492,45.48],\n",
    "    [\"City of San Marino\",4493]\n",
    "    ], \n",
    "    index = [\"Palau\", \"Vatican City\", \"Nauru\", \"Tuvalu\", \"San Marino\"],\n",
    "    columns=['Capital', 'Population', 'Percentage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "capitals.loc['Nauru', 'Population']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same as above\n",
    "capitals['Population']['Nauru']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capitals.loc[['Nauru', 'Tuvalu'], ['Population', 'Percentage']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capitals.loc['Palau': 'Nauru', ['Population', 'Percentage']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capitals.loc[['San Marino', 'Vatican City']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capitals.iloc[[4, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capitals.iloc[[4, 1], 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capitals.iloc[:, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering with boolean indexing\n",
    "capitals[[True, True, False, True, False]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capitals['Percentage'] > 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capitals[capitals['Percentage'] > 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades = pd.DataFrame([[6, 4], [7, 8], [6, 7], [6, 5], [5, 2]], \n",
    "                       index = ['Mary', 'John', 'Ann', 'Pete', 'Laura'],\n",
    "                       columns = ['test_1', 'test_2'])\n",
    "grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades['test_2'] <= grades['test_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grades[grades['test_2'] <= grades['test_1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades.mean() > 5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades.loc[:, grades.mean() > 5.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades.loc[['Laura', 'John'], 'test_2'] += 1\n",
    "# the same as above\n",
    "grades.loc[['Laura', 'John'], 'test_2'] = grades.loc[['Laura', 'John'], 'test_2'] + 1\n",
    "grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades['test_1'] += .5\n",
    "grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades.loc['Mary'] += 2\n",
    "grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades.loc['Pete'] = [7, 9]\n",
    "grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrong way to do it as the seconds command will work on strings and not on numbers\n",
    " \n",
    "grades[grades < 6] = 'Fail'\n",
    "grades[grades >= 6] = 'Pass'\n",
    "grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades = pd.DataFrame([[6, 4], [7, 8], [6, 7], [6, 5], [5, 2]], \n",
    "                       index = ['Mary', 'John', 'Ann', 'Pete', 'Laura'],\n",
    "                       columns = ['test_1', 'test_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# right way to do it\n",
    "failing = grades < 6\n",
    "passing = grades >= 6\n",
    "grades[failing] = 'Fail'\n",
    "grades[passing] = 'Pass'\n",
    "grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades = pd.DataFrame([[6, 4], [7, 8], [6, 7], [6, 5], [5, 2]], \n",
    "                       index = ['Mary', 'John', 'Ann', 'Pete', 'Laura'],\n",
    "                       columns = ['test_1', 'test_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average of the grades by rows\n",
    "grades.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades.mean(axis=1) > 6 # boolean indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a new column\n",
    "grades['passed'] = grades.mean(axis=1) > 6\n",
    "grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning new values\n",
    "# do not use.... grades['test_2][\"Ann\"] = 7\n",
    "\n",
    "# correct way is with loc\n",
    "grades.loc['Ann', 'test_2'] = 8\n",
    "grades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sortings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capitals.sort_index(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capitals.sort_index(ascending=False, inplace=True)\n",
    "capitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capitals.sort_index(axis=1)\n",
    "capitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capitals.sort_values(by='Percentage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades.sort_values(by=['test_1', 'test_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades.sort_values(by=['test_1', 'test_2'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the ascending or descending order for each column\n",
    "grades.sort_values(by=['test_1', 'test_2'], ascending=[True, False], inplace=True)\n",
    "grades\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pandas through with duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb as db\n",
    "con = db.connect(':memory:')\n",
    "dir = r\"E:/Downloads/_Pandas/Pandas Playbook. Manipulating Data (Reindert-Jan Ekker, 2018)/weather.csv\"\n",
    "df = con.execute(f\"SELECT * FROM read_csv_auto('{dir}')\").df()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['MONTH', 'DAY'], ascending=[True, False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r\"E:/Downloads/_Pandas/Pandas Playbook. Manipulating Data (Reindert-Jan Ekker, 2018)/weather_m4.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n",
    "df[['MIN_TEMP_GROUND',\n",
    "       'VIEW_RANGE', 'CLOUD',\n",
    "       'WEATHER_CODE']].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask with boolean indexing on nan values\n",
    "df.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().any() # for columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().any(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if thera columns with all values as nan\n",
    "df.isnull().all() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same for rows\n",
    "df.isnull().all(axis=1) \n",
    "# to see only those rows with true\n",
    "df.isnull().all(axis=1).any() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.isnull().all(axis=1) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the opposite of isnull\n",
    "df.notnull().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MIN_TEMP_GROUND'].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "every_6th_row = pd.Series(range(5, len(df), 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MIN_TEMP_GROUND'].loc[every_6th_row] # select evey 6th row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check that indeed all of the 6th rows have proper values\n",
    "df['MIN_TEMP_GROUND'].loc[every_6th_row].notnull().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check that the values in the rest of the column (those that are not in the 6th row) are indeed nan\n",
    "df['MIN_TEMP_GROUND'].drop(every_6th_row).isnull().all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to remove a column\n",
    "df.drop(columns='WEATHER_CODE', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing values\n",
    "df['MIN_TEMP_GROUND'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MIN_TEMP_GROUND'].fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MIN_TEMP_GROUND'].fillna(method='bfill', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check if the values are indeed filled\n",
    "df.isnull().any()\n",
    "# 'MIN_TEMP_GROUND' shows False, meaning there are no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see rows with missing data\n",
    "df[df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check the dates on which the data is missing we can use loc\n",
    "df.loc[df.isnull().any(axis=1), 'YYYYMMDD']\n",
    "# the dates are repeating a lot, so we can use value_counts to see how many times each date appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.isnull().any(axis=1), 'YYYYMMDD'].value_counts()\n",
    "# it means there are 20 rows for 20160821 with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with na\n",
    "nulls_dropped = df.dropna()\n",
    "nulls_dropped.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_thresh = df.dropna(thresh=7)\n",
    "drop_thresh[drop_thresh.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_thresh.fillna(0, inplace=True)\n",
    "drop_thresh[drop_thresh.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select all rows with null values\n",
    "rows_to_fill = df.isnull().any(axis=1)\n",
    "df.loc[rows_to_fill]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill with the mean of the column\n",
    "nulls_filled = df.fillna(df.mean())\n",
    "# check the filled rows. we use previously created rows_to_fill\n",
    "nulls_filled.loc[rows_to_fill]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill with mode insted of mean\n",
    "df.fillna(df.mode().iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(df.mode().iloc[0], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling bad data other than nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athletes = pd.read_csv(r\"E:/Downloads/_Pandas/Pandas Playbook. Manipulating Data (Reindert-Jan Ekker, 2018)/athletes.csv\")\n",
    "athletes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "athletes.plot.scatter(x='height', y='weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cProfile import label\n",
    "\n",
    "\n",
    "heights = athletes['height']\n",
    "heights.plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = heights.quantile(0.25)\n",
    "q3 = heights.quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "\n",
    "pmin = q1 - 1.5 * iqr\n",
    "pmax = q3 + 1.5 * iqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwh = heights.where(heights.between(pmin, pmax), heights.mean()) # boolean mask that selects\n",
    "# true for values that are between pmin and pmax and puts the mean value for the rest\n",
    "# IF nothing is specified for the false values, they are replaced with NaN!!\n",
    "# heights.where(heights.between(pmin, pmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the original and the new values after the transformation\n",
    "compare  = pd.DataFrame({'before': heights, 'after': nwh})\n",
    "compare.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare  = pd.DataFrame({'before': heights, 'after': nwh})\n",
    "compare.plot.box()\n",
    "compare.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heights.where(heights.between(pmin, pmax), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athletes.plot.scatter(x='height', y='weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are duplicate rows\n",
    "athletes.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the duplicated rows\n",
    "athletes[athletes.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the duplicated rows\n",
    "athletes.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athletes.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to explore the data and see unique values\n",
    "athletes['nationality'].drop_duplicates() # returns cleaned df\n",
    "# athletes['nationality'].unique() # returns numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athletes['nationality'].drop_duplicates().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value counts\n",
    "athletes['nationality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athletes['sex'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athletes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athletes[['gold', 'silver', 'bronze']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum() on a string, just concatenates it\n",
    "athletes['bronze'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athletes['bronze'].astype(int)\n",
    "# error - ValueError: invalid literal for int() with base 10: 'O'\n",
    "# there is a row with string 'O' and not a zero. let's fix it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's find all the rows with string 'O' in column 'bronze'\n",
    "athletes[athletes['bronze'] == 'O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's now change the string 'O' to 0 for this specific row\n",
    "athletes.loc[7521, ['gold', 'silver', 'bronze']] = 0\n",
    "\n",
    "# and now, let's change the type of these columns to int \n",
    "athletes[['gold', 'silver', 'bronze']] = athletes[['gold', 'silver', 'bronze']].astype(int)\n",
    "athletes[['gold', 'silver', 'bronze']].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athletes.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athletes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athletes.index = athletes['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athletes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# setting an existing column as index\n",
    "athletes.set_index('id', drop=True, inplace=True)\n",
    "athletes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming columns\n",
    "athletes.rename(columns={'nationality': 'country', 'sport': 'discipline'}, inplace=True)\n",
    "athletes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming rows\n",
    "athletes.rename(index={736041664: 000, 532037425: 1111}, inplace=True)\n",
    "athletes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reseting index\n",
    "df = pd.read_csv(r\"E:/Downloads/_Pandas/Pandas Playbook. Manipulating Data (Reindert-Jan Ekker, 2018)/weather_m4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.ones([5,4]), columns=['a', 'b', 'c', 'd'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actually changing the original df\n",
    "df *= 2\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a row and dibide it by 2\n",
    "df.loc[1] /= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or a column\n",
    "df['b'] -= 1\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # new df\n",
    "df2 = pd.DataFrame(np.ones([3,2]), columns=['d', 'e'], index=[2,4,5])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df + df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[2] * df2.loc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df - df.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df - pd.Series({'a': 5, 'b': 5, 'e': 5, 'f': 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtract mean of rows from each row\n",
    "df.sub(df.mean(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'sin': np.arange(0, 5*np.pi, 0.01),\n",
    "                   'cos':np.arange(0.5*np.pi, 5.5*np.pi, 0.01)})\n",
    "df\n",
    "#' apply sine function to the df\n",
    "df = np.sin(df)\n",
    "%matplotlib inline\n",
    "\n",
    "df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom function\n",
    "# calculate the interquartile range\n",
    "def iqr(col):\n",
    "    return col.quantile(0.75) - col.quantile(0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing the custom function to the apply method\n",
    "df.apply(iqr) # this way it's applied to columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now to rows\n",
    "df.apply(iqr, axis=1) # this way it's applied to rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def somefunc(x):\n",
    "    return np.abs(x+.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's apply the function to every cell in the df. We can't use apply as it works on rows and columns\n",
    "df.applymap(somefunc) # important not to use () around the function name as it's the applymap that will trigger it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.applymap(somefunc).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athletes = pd.read_csv(r\"E:/Downloads/_Pandas/Pandas Playbook. Manipulating Data (Reindert-Jan Ekker, 2018)/athletes_clean.csv\")\n",
    "athletes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = athletes.groupby('nationality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " g = athletes.groupby('nationality')[['gold', 'silver', 'bronze']]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = athletes.groupby('sport')[['weight', 'height']]\n",
    "g.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = athletes.groupby(['sport', 'sex'])[['weight', 'height']]\n",
    "g.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = athletes.groupby(['sport', 'sex'])[['weight', 'height']]\n",
    "# g.last()\n",
    "# g.first()\n",
    "g.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stack, unstack, pivot, pivot_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = pd.read_csv(r\"E:/Downloads/_Pandas/Pandas Playbook. Manipulating Data (Reindert-Jan Ekker, 2018)/monthly_data.csv\")\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change index\n",
    "# m.set_index('YYYY', inplace=True)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to move values from all the 12 moths columns into a single column\n",
    "m.stack().head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.stack().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unstack\n",
    "w = athletes.groupby(['sport', 'sex'])['weight'].mean()\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we'll move it from all the weights being in one column to separate columns for female and male, since\n",
    "# these are are the values from the internal group - sex\n",
    "w.unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot\n",
    "\n",
    "p = pd.DataFrame({'id': [823905, 823905,\n",
    "                         235897, 235897, 235897,\n",
    "                         983422, 983422],\n",
    "                  'item': ['prize', 'unit', \n",
    "                           'prize', 'unit', 'stock', \n",
    "                           'prize', 'stock'],\n",
    "                  'value': [3.49, 'kg',\n",
    "                            12.89, 'l', 50,\n",
    "                            0.49, 4]})\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot\n",
    "p.pivot('id', 'item', 'value') \n",
    "# 'id' will be the column for each individual record. It will use different values from 'item' as different columlns\n",
    "# and it will fill these new coumns from values from 'value'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades = pd.DataFrame([[6, 4, 5], [7, 8, 7], [6, 7, 9], [6, 5, 5], [5, 2, 7]], \n",
    "                       index = ['Mary', 'John', 'Ann', 'Pete', 'Laura'],\n",
    "                       columns = ['test_1', 'test_2', 'test_3'])\n",
    "grades.reset_index(inplace=True)\n",
    "grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades.melt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades.melt(id_vars=['index']) # specify the index column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining DataF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades = pd.DataFrame([[6, 4, 5], [7, 8, 7], [6, 7, 9], [6, 5, 5], [5, 2, 7]], \n",
    "                       index = ['Mary', 'John', 'Ann', 'Pete', 'Laura'],\n",
    "                       columns = ['test_1', 'test_2', 'test_3'])\n",
    "grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new column to an existing df\n",
    "grades['test_4'] = pd.Series({'John': 5, 'Ann': 8, 'Pete': 9, 'Mary': 7, 'Laura': 10})\n",
    "grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now adding a new row\n",
    "grades.loc['Bob'] = [2, 3, 4, 5]\n",
    "grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append\n",
    "new_row = pd.Series({'test_1': 8, 'test_2': 9, 'test_3': 7, 'test_4': 8}, name='Kim')\n",
    "grades.append(new_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# addging a new column\n",
    "grades['stud_nr'] = [113, 121, 123, 135, 139, 141]\n",
    "# order colulmns as we need them\n",
    "grades = grades[['stud_nr', 'test_1', 'test_2', 'test_3', 'test_4']]\n",
    "grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other = pd.DataFrame([[139, 7, 7],\n",
    "                       [123, 8, 6],\n",
    "                       [142, 4, 5],\n",
    "                       [113, 7, 9],\n",
    "                       [155, 10, 9],\n",
    "                       [121, 6, 4]], \n",
    "                       columns = ['stud_nr', 'exam1', 'exam2'])\n",
    "other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine two df with merge. it's like join in SQL\n",
    "grades.merge(other) # by default it's an inner join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades.merge(other, how='left') # will show those students present in grades df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades.merge(other, how='right') # will show those students present in OTHER df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades.merge(other, how='outer') # will show those students present in both dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(r\"E:\\app_data\\dropbox_13f_files\\crspq.csv\", nrows=100).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T17:16:32.650740Z",
     "iopub.status.busy": "2022-08-09T17:16:32.650740Z",
     "iopub.status.idle": "2022-08-09T18:07:33.597294Z",
     "shell.execute_reply": "2022-08-09T18:07:33.597294Z",
     "shell.execute_reply.started": "2022-08-09T17:16:32.650740Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 43min 31s\n",
      "Wall time: 51min\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "# concatenate different files with the same cik in their name into one file per cik\n",
    "\n",
    "import os, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from fnmatch import fnmatch\n",
    "\n",
    "processed_tables_parquet_till_2013 = r\"E:\\app_data\\dropbox_13f_files\\processed_tables\\processed_tables_parquet_till_2013\"\n",
    "cik_parquet_till_2013 = r\"E:\\app_data\\dropbox_13f_files\\processed_tables\\cik_parquet_till_2013\"\n",
    "\n",
    "# list all files in a folder and subfolders\n",
    "\n",
    "\n",
    "def files_in_dir(dir, ext=\"*.parquet\"):\n",
    "    l = []\n",
    "    for path, subdirs, files in os.walk(dir):\n",
    "        for name in files:\n",
    "            if fnmatch(name, ext):\n",
    "                l.append(os.path.join(path, name))\n",
    "    return l\n",
    "\n",
    "parquet_list = files_in_dir(processed_tables_parquet_till_2013)\n",
    "\n",
    "\n",
    "# extract a list of ciks from a list of file names \n",
    "def get_list_of_cik(parquet_list: list) -> list:\n",
    "    cik_list = []\n",
    "    for file in parquet_list:\n",
    "        cik_list.append(file.split('\\\\')[-1].split('-')[0])\n",
    "    return sorted(cik_list)\n",
    "\n",
    "cik_list = get_list_of_cik(parquet_list)\n",
    "cik_list = [int(x) for x in list(set(cik_list))]\n",
    "# cik_list = [315066]\n",
    "\n",
    "data_dir = Path(processed_tables_parquet_till_2013)\n",
    "for cik in cik_list:\n",
    "# cik = 9749\n",
    "# temp_dir = r\"E:\\app_data\"\n",
    "# full_df = pd.concat(pd.read_parquet(f) for f in data_dir.glob(f\"{cik}-*.parquet\"))\n",
    "\n",
    "    full_df = pd.concat(pd.read_parquet(f) for f in data_dir.glob(f\"{cik}-*.parquet\"))\n",
    "    full_df = full_df[full_df.filedAsOfDate <= '2013-12-31']\n",
    "\n",
    "    full_df['cusip'] = full_df['cusip9'].astype(str).str.upper()\n",
    "    full_df['titleOfClass'] = full_df.get('titleOfClass',default=np.nan)\n",
    "    full_df['nameOfIssuer'] = full_df.get('titleOfClass',default=np.nan)\n",
    "    full_df['managerName'] = full_df.get('managerName',default=np.nan)\n",
    "    full_df['putCall'] = full_df.get('putCall',default=np.nan)\n",
    "    full_df['shrsOrPrnAmt'] = full_df.get('shrsOrPrnAmt',default=np.nan)\n",
    "    full_df['sharesValue'] = full_df.get('sharesValue',default=np.nan)\n",
    "    full_df['sharesValue'] = full_df['sharesValue'] * 1000\n",
    "    \n",
    "    full_df['sharesHeldAtEndOfQtr'] = full_df.get('sharesHeldAtEndOfQtr', default=np.nan)\n",
    "    full_df['submissionType'] = full_df.get('submissionType',default=np.nan)\n",
    "    full_df['quarter'] = full_df['periodOfReport'].dt.year.astype(str)+\"Q\"+full_df['periodOfReport'].dt.quarter.astype(str)\n",
    "    full_df['updated_at'] = pd.Timestamp.now()\n",
    "    full_df.drop(columns=[\"cusip8\", \"cusip9\"], inplace=True)\n",
    "    \n",
    "    # if there are no rows in the df, then move on to another file\n",
    "    if full_df.shape[0] == 0: continue\n",
    "        \n",
    "    full_df = (full_df.sort_values(by=['cusip', 'periodOfReport', 'filedAsOfDate', 'accessionNumber'], ascending=False))\n",
    "    fdate = full_df.filedAsOfDate.dt.date.max()\n",
    "\n",
    "    dtypes = {'cusip': str, 'nameOfIssuer': str , 'titleOfClass': str, 'periodOfReport': np.datetime64,\n",
    "            'putCall': str, 'shrsOrPrnAmt': str, 'sharesValue': float, 'sharesHeldAtEndOfQtr': float, \n",
    "            'filedAsOfDate': np.datetime64, 'accessionNumber': str, 'cikManager' : int, 'managerName': str,\n",
    "            'quarter': str, 'submissionType': str, 'updated_at': np.datetime64}\n",
    "    \n",
    "    # grouping by cusip and summing up shares and value\n",
    "    attributes = {'accessionNumber': 'last', 'cikManager':'last', 'managerName':'last',\n",
    "    'periodOfReport':'last', 'submissionType':'last', 'filedAsOfDate':'first', 'cusip':'last', 'nameOfIssuer': 'last',\n",
    "    'titleOfClass' :'last', 'sharesValue':'last',  'sharesHeldAtEndOfQtr':\"last\", \"shrsOrPrnAmt\":\"last\",\n",
    "    'putCall':'last', 'xml_flag':'last', 'updated_at':'last', 'quarter':'last'}\n",
    "\n",
    "    full_df  = full_df.groupby([\"cikManager\",\"cusip\", \"quarter\"], as_index=False).agg(attributes)\n",
    "\n",
    "    full_df.astype(dtypes).to_parquet(f\"{cik_parquet_till_2013}/{cik}-{fdate}.parquet\", engine='pyarrow')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T21:24:08.649523Z",
     "iopub.status.busy": "2022-08-09T21:24:08.649523Z",
     "iopub.status.idle": "2022-08-09T21:24:10.037398Z",
     "shell.execute_reply": "2022-08-09T21:24:10.037398Z",
     "shell.execute_reply.started": "2022-08-09T21:24:08.649523Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.38 s\n",
      "Wall time: 1.38 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "from fnmatch import fnmatch\n",
    "\n",
    "processed_tables_csv = r\"E:\\app_data\\dropbox_13f_files\\processed_tables\\processed_tables\"\n",
    "my_filings_parquet = r\"E:\\app_data\\sec_apps_data\\speed_test\\filings_13f_full\\filings_parquet\"\n",
    "\n",
    "def files_in_dir_list(dir, ext=\"*.parquet\"):\n",
    "    l = []\n",
    "    for path, subdirs, files in os.walk(dir):\n",
    "        for name in files:\n",
    "            if fnmatch(name, ext):\n",
    "                l.append(os.path.join(path, name))\n",
    "    return l\n",
    "\n",
    "l = files_in_dir_list(my_filings_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T23:15:10.635587Z",
     "iopub.status.busy": "2022-08-09T23:15:10.635587Z",
     "iopub.status.idle": "2022-08-09T23:15:10.666840Z",
     "shell.execute_reply": "2022-08-09T23:15:10.666840Z",
     "shell.execute_reply.started": "2022-08-09T23:15:10.635587Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 15.6 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "from fnmatch import fnmatch\n",
    "\n",
    "processed_tables_csv = r\"E:\\app_data\\dropbox_13f_files\\processed_tables\\processed_tables\"\n",
    "my_filings_parquet = r\"E:\\app_data\\sec_apps_data\\speed_test\\filings_13f_full\\filings_parquet\"\n",
    "cik_parquet_till_2013 = r\"E:\\app_data\\dropbox_13f_files\\processed_tables\\cik_parquet_till_2013\"\n",
    "\n",
    "def files_in_dir_dict(dir, ext=\"*.parquet\"):\n",
    "    d = {}\n",
    "    for path, subdirs, files in os.walk(dir):\n",
    "        for index, name in enumerate(files):\n",
    "            if fnmatch(name, ext):\n",
    "\n",
    "                    \n",
    "                    # d[index] = os.path.join(path, name)\n",
    "                d[index] = int(name.split(\"-\")[-4].split(\"\\\\\")[-1])\n",
    "                # l.append(os.path.join(path, name))\n",
    "    return d\n",
    "\n",
    "l = files_in_dir_dict(cik_parquet_till_2013)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a value and if doesn't exist assigning a default one\n",
    "full_df['transaction_value'] = full_df.get('transaction_value',default=np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .merge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T20:58:17.596249Z",
     "iopub.status.busy": "2022-09-14T20:58:17.596249Z",
     "iopub.status.idle": "2022-09-14T20:58:17.931274Z",
     "shell.execute_reply": "2022-09-14T20:58:17.930512Z",
     "shell.execute_reply.started": "2022-09-14T20:58:17.596249Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.DataFrame({'name': ['Fred', 'Suzy', 'Suzy', 'Bob'],\n",
    "                    'pet': ['Dog', 'Dog', 'Cat', 'Fish']})\n",
    "\n",
    "df2 = pd.DataFrame({'Name': ['Suzy', 'Suzy', 'Suzy', 'Fred', 'Joe', 'Joe'],\n",
    "                    'Color ': ['Black', 'Blue', 'Red', 'Green', 'Yellow', 'Blue']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T20:58:22.216134Z",
     "iopub.status.busy": "2022-09-14T20:58:22.215134Z",
     "iopub.status.idle": "2022-09-14T20:58:22.234132Z",
     "shell.execute_reply": "2022-09-14T20:58:22.234132Z",
     "shell.execute_reply.started": "2022-09-14T20:58:22.216134Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>pet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fred</td>\n",
       "      <td>Dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Suzy</td>\n",
       "      <td>Dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Suzy</td>\n",
       "      <td>Cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bob</td>\n",
       "      <td>Fish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name   pet\n",
       "0  Fred   Dog\n",
       "1  Suzy   Dog\n",
       "2  Suzy   Cat\n",
       "3   Bob  Fish"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Suzy</td>\n",
       "      <td>Black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Suzy</td>\n",
       "      <td>Blue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Suzy</td>\n",
       "      <td>Red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fred</td>\n",
       "      <td>Green</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Joe</td>\n",
       "      <td>Yellow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Joe</td>\n",
       "      <td>Blue</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Name  Color \n",
       "0  Suzy   Black\n",
       "1  Suzy    Blue\n",
       "2  Suzy     Red\n",
       "3  Fred   Green\n",
       "4   Joe  Yellow\n",
       "5   Joe    Blue"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(df1), display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T20:57:55.674494Z",
     "iopub.status.busy": "2022-09-14T20:57:55.674494Z",
     "iopub.status.idle": "2022-09-14T20:57:55.693493Z",
     "shell.execute_reply": "2022-09-14T20:57:55.693493Z",
     "shell.execute_reply.started": "2022-09-14T20:57:55.674494Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>pet</th>\n",
       "      <th>Name</th>\n",
       "      <th>Color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fred</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Fred</td>\n",
       "      <td>Green</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Suzy</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Suzy</td>\n",
       "      <td>Black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Suzy</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Suzy</td>\n",
       "      <td>Blue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Suzy</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Suzy</td>\n",
       "      <td>Red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Suzy</td>\n",
       "      <td>Cat</td>\n",
       "      <td>Suzy</td>\n",
       "      <td>Black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Suzy</td>\n",
       "      <td>Cat</td>\n",
       "      <td>Suzy</td>\n",
       "      <td>Blue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Suzy</td>\n",
       "      <td>Cat</td>\n",
       "      <td>Suzy</td>\n",
       "      <td>Red</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name  pet  Name Color \n",
       "0  Fred  Dog  Fred  Green\n",
       "1  Suzy  Dog  Suzy  Black\n",
       "2  Suzy  Dog  Suzy   Blue\n",
       "3  Suzy  Dog  Suzy    Red\n",
       "4  Suzy  Cat  Suzy  Black\n",
       "5  Suzy  Cat  Suzy   Blue\n",
       "6  Suzy  Cat  Suzy    Red"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# be careful with names. Only because there was a white space after the column `name `, the code below didn't want to work\n",
    "(df1.merge(df2.assign(name=df2.Name))) # by default it's an inner join\n",
    "\n",
    "# df1.merge(df2, left_on='name', right_on='Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T21:03:28.793673Z",
     "iopub.status.busy": "2022-09-14T21:03:28.792660Z",
     "iopub.status.idle": "2022-09-14T21:03:28.810660Z",
     "shell.execute_reply": "2022-09-14T21:03:28.809661Z",
     "shell.execute_reply.started": "2022-09-14T21:03:28.793673Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>pet</th>\n",
       "      <th>Name</th>\n",
       "      <th>Color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fred</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Fred</td>\n",
       "      <td>Green</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Suzy</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Suzy</td>\n",
       "      <td>Black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Suzy</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Suzy</td>\n",
       "      <td>Blue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Suzy</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Suzy</td>\n",
       "      <td>Red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Suzy</td>\n",
       "      <td>Cat</td>\n",
       "      <td>Suzy</td>\n",
       "      <td>Black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Suzy</td>\n",
       "      <td>Cat</td>\n",
       "      <td>Suzy</td>\n",
       "      <td>Blue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Suzy</td>\n",
       "      <td>Cat</td>\n",
       "      <td>Suzy</td>\n",
       "      <td>Red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bob</td>\n",
       "      <td>Fish</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name   pet  Name Color \n",
       "0  Fred   Dog  Fred  Green\n",
       "1  Suzy   Dog  Suzy  Black\n",
       "2  Suzy   Dog  Suzy   Blue\n",
       "3  Suzy   Dog  Suzy    Red\n",
       "4  Suzy   Cat  Suzy  Black\n",
       "5  Suzy   Cat  Suzy   Blue\n",
       "6  Suzy   Cat  Suzy    Red\n",
       "7   Bob  Fish   NaN    NaN"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df1.merge(df2.assign(name=df2.Name), how='left')) # all value from df1 will be brought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T21:19:14.152198Z",
     "iopub.status.busy": "2022-09-14T21:19:14.152198Z",
     "iopub.status.idle": "2022-09-14T21:19:14.171218Z",
     "shell.execute_reply": "2022-09-14T21:19:14.170199Z",
     "shell.execute_reply.started": "2022-09-14T21:19:14.152198Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>pet</th>\n",
       "      <th>Name</th>\n",
       "      <th>Color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Suzy</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Suzy</td>\n",
       "      <td>Black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Suzy</td>\n",
       "      <td>Cat</td>\n",
       "      <td>Suzy</td>\n",
       "      <td>Black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Suzy</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Suzy</td>\n",
       "      <td>Blue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Suzy</td>\n",
       "      <td>Cat</td>\n",
       "      <td>Suzy</td>\n",
       "      <td>Blue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Suzy</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Suzy</td>\n",
       "      <td>Red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Suzy</td>\n",
       "      <td>Cat</td>\n",
       "      <td>Suzy</td>\n",
       "      <td>Red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Fred</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Fred</td>\n",
       "      <td>Green</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Joe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Joe</td>\n",
       "      <td>Yellow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Joe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Joe</td>\n",
       "      <td>Blue</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name  pet  Name  Color \n",
       "0  Suzy  Dog  Suzy   Black\n",
       "1  Suzy  Cat  Suzy   Black\n",
       "2  Suzy  Dog  Suzy    Blue\n",
       "3  Suzy  Cat  Suzy    Blue\n",
       "4  Suzy  Dog  Suzy     Red\n",
       "5  Suzy  Cat  Suzy     Red\n",
       "6  Fred  Dog  Fred   Green\n",
       "7   Joe  NaN   Joe  Yellow\n",
       "8   Joe  NaN   Joe    Blue"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df1.merge(df2.assign(name=df2.Name), how='right'))\n",
    "    # .assign(pet = df1.pet.fillna(\"no_pet\"),\n",
    "    #        name = df1.name.replace(\"Suzy\", 'No Suzy')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T21:05:29.407827Z",
     "iopub.status.busy": "2022-09-14T21:05:29.407827Z",
     "iopub.status.idle": "2022-09-14T21:05:29.423852Z",
     "shell.execute_reply": "2022-09-14T21:05:29.422856Z",
     "shell.execute_reply.started": "2022-09-14T21:05:29.407827Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>pet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fred</td>\n",
       "      <td>Dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Suzy</td>\n",
       "      <td>Dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Suzy</td>\n",
       "      <td>Cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bob</td>\n",
       "      <td>Fish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name   pet\n",
       "0  Fred   Dog\n",
       "1  Suzy   Dog\n",
       "2  Suzy   Cat\n",
       "3   Bob  Fish"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code snippet is for when we are not sure if the column `transaction_value` exists at all. \n",
    "# If it doesn't, it will get automatically created with the default value\n",
    "full_df['transaction_value'] = full_df.get('transaction_value',default=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T23:16:09.507844Z",
     "iopub.status.busy": "2022-08-09T23:16:09.507844Z",
     "iopub.status.idle": "2022-08-09T23:16:09.523506Z",
     "shell.execute_reply": "2022-08-09T23:16:09.523506Z",
     "shell.execute_reply.started": "2022-08-09T23:16:09.507844Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1 = (df.merge(cusip_md_df[['cusip', 'quarter','ticker', 'cusip_description', 'nameofissuer', 'titleofclass']], on=['cusip', 'quarter'])\n",
    "      .rename(columns={'ticker':'cusip_ticker'}))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "89cd8330c6000ed2b4e9cbcae2986a8c07c6071da3ea6a959671268f16d3c726"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
