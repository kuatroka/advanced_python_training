{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effective Pandas Book Runthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', 30)\n",
    "pd.set_option(\"display.max_colwidth\",3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas df with duckdb\n",
    "import duckdb as db\n",
    "\n",
    "con = db.connect(':memory:')\n",
    "parquet_dir = r\"E:/app_data/sec_apps_data/speed_test/filings_13f_full/filings_parquet\"\n",
    "\n",
    "%time df = db.from_parquet(f\"{parquet_dir}/811360*.parquet\", connection=con).df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas df with vaex\n",
    "\n",
    "import vaex\n",
    "cikmanager = 811360\n",
    "%time df1 = vaex.open(f\"{parquet_dir}/{cikmanager}*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = {\n",
    "'index ':[0, 1, 2, 3],\n",
    "'data ':[145, 142, 38, 13],\n",
    "'name':'songs'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "songs2 = pd.Series([145, 142, 38, 13], name='counts')\n",
    "songs2\n",
    "songs2.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "nan_series = pd.Series([2, np.nan],\n",
    "index=['Ono', 'Clapton '])\n",
    "\n",
    "nan_series\n",
    "nan_series.count()\n",
    "nan_series.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_series2 = pd.Series([2, None],\n",
    "                        index=['Ono', 'Clapton '],\n",
    "                        dtype='Int64')\n",
    "nan_series2\n",
    "nan_series2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_series2.astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_ser  = np.array([145, 142, 38, 13])\n",
    "numpy_ser[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs3 = pd.Series([145, 142, 38, 13],\n",
    "name='counts ',\n",
    "index=['Paul', 'John', 'George ', 'Ringo '])\n",
    "\n",
    "songs3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not filtering, but identifying rows as per condition\n",
    "songs3 > songs3.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to filter rows based on rows values in certain columns\n",
    "songs3[songs3 > songs3.median()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs3.mean()\n",
    "numpy_ser.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = songs3 > songs3.mean() # boolean array\n",
    "mask\n",
    "songs3[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy equivalent of a mask\n",
    "numpy_ser[numpy_ser > np.median(numpy_ser)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories\n",
    "s = pd.Series(['m', 'l', 'xs', 's', 'xl'], dtype='category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s\n",
    "s.cat.ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordering it\n",
    "s2 = pd.Series(['m', 'l', 'xs', 's', 'xl'])\n",
    "size_type = pd.api.types.CategoricalDtype(categories=[ 's', 'm', 'l'], ordered=True)\n",
    "s3 = s2.astype(size_type)\n",
    "s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 > 's'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.cat.reorder_categories(['xs','s','m','l', 'xl'],\n",
    "ordered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.str.upper()\n",
    "s.str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_temp = pd.Series([32, 29, 20, 26, 31, 25, 30], dtype='int64')\n",
    "s_temp\n",
    "s_temp.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_temp[s_temp > s_temp.mean()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_colours = pd.Series(['blue', 'red', 'green', 'yellow', 'orange', 'purple', 'pink'], dtype='category')\n",
    "s_colours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Series Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://github.com/mattharrison/datasets/raw/master/data/' \\\n",
    " 'vehicles.csv.zip'\n",
    "df = pd.read_csv(url)\n",
    "city_mpg = df.city08\n",
    "highway_mpg = df.highway08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_mpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dir(city_mpg))\n",
    "dir(city_mpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(city_mpg + highway_mpg) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.Series([10, 20, 30], index=[1,2,2])\n",
    "s2 = pd.Series([35, 44, 53], index=[2,2,4], name='s2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1.add(s2, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((city_mpg + highway_mpg) / 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using chaining\n",
    "(city_mpg\n",
    "    .add(highway_mpg)\n",
    "    .div(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean\n",
    "city_mpg.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_mpg.is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_mpg.is_monotonic_increasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pd.Series([1, 2, 3, 4 ])\n",
    "f.is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.is_monotonic_increasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_mpg.quantile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_mpg.quantile([0.25, 0.5, 0.75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_mpg.quantile(0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import polars as pl\n",
    "\n",
    "df = pl.read_csv(\"https://j.mp/iriscsv\")\n",
    "print(df.filter(pl.col(\"sepal_length\") > 5)\n",
    "      .groupby(\"species\")\n",
    "      .agg(pl.all().sum())\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# lazy way\n",
    "import polars as pl\n",
    "\n",
    "print(\n",
    "    pl.read_csv(\"https://j.mp/iriscsv\")\n",
    "    .lazy()\n",
    "    .filter(pl.col(\"sepal_length\") > 5)\n",
    "    .groupby(\"species\")\n",
    "    .agg(pl.all().sum())\n",
    "    .collect()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# making df lazy\n",
    "# filtering the df based on one column\n",
    "(\n",
    "    df.lazy()\n",
    "    .filter(pl.col(\"sepal_length\") > 5)\n",
    "    .groupby(\"species\")\n",
    "    .agg(pl.all().sum())\n",
    "    .collect()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.select([\n",
    "    \n",
    "    pl.col(\"species\").sort().head(2),\n",
    "    pl.col(\"sepal_length\").filter(pl.col(\"sepal_length\") == 5).sum()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(12)\n",
    "\n",
    "df = pl.DataFrame(\n",
    "    {\n",
    "        \"nrs\": [1, 2, 3, None, 5],\n",
    "        \"names\": [\"foo\", \"ham\", \"spam\", \"egg\", None],\n",
    "        \"random\": np.random.rand(5),\n",
    "        \"groups\": [\"A\", \"A\", \"B\", \"C\", \"B\"],\n",
    "    }\n",
    ")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# unique values\n",
    "out = df.select(\n",
    "    [\n",
    "        pl.col(\"names\").n_unique().alias(\"unique_names_1\"),\n",
    "        pl.col(\"names\").unique().count().alias(\"unique_names_2\"),\n",
    "    ]\n",
    ")\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "out = df.select(\n",
    "    [\n",
    "        pl.sum(\"random\").alias(\"sum\"),\n",
    "        pl.min(\"random\").alias(\"min\"),\n",
    "        pl.max(\"random\").alias(\"max\"),\n",
    "        pl.col(\"random\").max().alias(\"other_max\"),\n",
    "        pl.std(\"random\").alias(\"std dev\"),\n",
    "        pl.var(\"random\").alias(\"variance\"),\n",
    "        pl.last(\"random\").alias(\"last_var\"),\n",
    "        pl.first(\"random\").alias(\"first_var\"),\n",
    "    ]\n",
    ")\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = df.select(\n",
    "    [\n",
    "        pl.col(\"names\").filter(pl.col(\"names\").str.contains(r\"o$\")).count(),\n",
    "    ]\n",
    ")\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polars with my data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import polars as pl\n",
    "parquet_dir = r\"E:\\app_data\\sec_apps_data\\speed_test\\filings_13f_full\\filings_parquet\"\n",
    "\n",
    "\n",
    "df_filings = (pl.scan_parquet(f\"{parquet_dir}/949509-*.parquet\")\n",
    "              .filter((pl.col(\"cikManager\") == 949509) & (pl.col(\"filedAsOfDate\") >= \"2014-01-01\" ) \n",
    "                      #& (pl.col(\"cusip\") == \"01609W102\" ))\n",
    "              ))\n",
    " \n",
    "    # .filter(pl.col(\"sepal_length\") > 5)\n",
    "    # .groupby(\"species\")\n",
    "    # .agg(pl.all().sum())\n",
    "    #.collect()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filings.fetch(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_berk_2014up = df_filings.collect()\n",
    "df_berk_2014up.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (df_berk_2014up.filter(pl.col(\"report_Year\") == 2014)\n",
    "                  .groupby([\"cikManager\", \"report_Year\", \"report_Quarter\" ])\n",
    "                #   .agg(pl.col(\"sharesHeldAtEndOfQtr\").sum())\n",
    "                  .agg(pl.col(\"sharesHeldAtEndOfQtr\").last()) # \n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = (df_berk_2014up.filter(pl.col(\"report_Year\") == 2014)\n",
    "#                   .groupby([\"cikManager\",\"report_Year\" ])\n",
    "#                 #   .agg(pl.col(\"sharesHeldAtEndOfQtr\").sum())\n",
    "#                   .agg(pl.col(\"sharesHeldAtEndOfQtr\").sum()) # \n",
    "#                   )\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lazy mode\n",
    "(df.lazy()\n",
    "    .filter(pl.col(\"report_Quarter\") != 1)\n",
    "    .groupby(\"report_Quarter\")\n",
    "    .agg(pl.all().sum())\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Expressions\n",
    "pl.col(\"foo\").sort(reverse=True).head(2)\n",
    "\n",
    "df.select([pl.col(\"report_Quarter\").sort(reverse=True).head(1),\n",
    "           pl.col(\"sharesHeldAtEndOfQtr\").filter(pl.col(\"report_Quarter\") == 3).min()\n",
    "           \n",
    "           \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## unique values\n",
    "out = df_filings.select([\n",
    "    pl.col(\"cusip\").n_unique().alias(\"unique_cusip_1\"),\n",
    "    pl.col(\"cusip\").unique().count().alias(\"unique_cusip_2\"),\n",
    "                         \n",
    "                         \n",
    "                         ])\n",
    "print(out.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## aggregations\n",
    "\n",
    "out = df_filings.select([\n",
    "    pl.sum(\"sharesHeldAtEndOfQtr\").alias(\"shares_sum\"),\n",
    "    pl.min(\"sharesHeldAtEndOfQtr\").alias(\"shares_min\"),\n",
    "    pl.max(\"sharesHeldAtEndOfQtr\").alias(\"shares_max\"),\n",
    "    pl.col(\"sharesHeldAtEndOfQtr\").max().alias(\"shares_other_max\"),\n",
    "    pl.std(\"sharesHeldAtEndOfQtr\").alias(\"shares_std_dev\"),\n",
    "    pl.var(\"sharesHeldAtEndOfQtr\").alias(\"variance\"),\n",
    "    pl.col(\"report_Quarter\").alias(\"quarter\"),\n",
    "    pl.col(\"sharesHeldAtEndOfQtr\").alias(\"shares\"),\n",
    "    pl.col(\"sharesHeldAtEndOfQtr\").shift(-1).alias(\"shares_prev\"),\n",
    "    pl.col(\"cusip\").str.to_uppercase().alias(\"cusip\")\n",
    "                         \n",
    "                         \n",
    "                         \n",
    "                         ])\n",
    "print(out.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.Config.set_tbl_rows(50)\n",
    "pl.Config.set_tbl_cols(50)\n",
    "pl.Config.set_tbl_width_chars(50)\n",
    "pl.Config.set_fmt_str_lengths(50)\n",
    "\n",
    "(df_filings \n",
    "    .sort([\"cikManager\", \"cusip\", \"periodOfReport\",\"filedAsOfDate\", \"accessionNumber\"], reverse=True)\n",
    "    .select([\n",
    "      \n",
    "    pl.col(\"cusip\"),\n",
    "    (pl.col(\"periodOfReport\").dt.year()+\"Q\"+pl.col(\"periodOfReport\").dt.quarter()).alias(\"quarter\"),\n",
    "    pl.col(\"periodOfReport\"),\n",
    "    #pl.col(\"filedAsOfDate\").alias(\"fdate\"),\n",
    "    pl.col(\"sharesHeldAtEndOfQtr\").first().over([\"cikManager\", \"cusip\"]).alias(\"shares\"),\n",
    "    # pl.col(\"sharesHeldAtEndOfQtr\").sum().over([\"cikManager\", \"cusip\", \"periodOfReport\"]).alias(\"shares_sum\"),\n",
    "    pl.col(\"sharesHeldAtEndOfQtr\").list().over([\"cusip\", \"periodOfReport\"]).alias(\"shares_list\")\n",
    "    ])\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_filings\n",
    "    .groupby([\"cusip\", (pl.col(\"periodOfReport\").dt.year()+\"Q\"+pl.col(\"periodOfReport\").dt.quarter()).alias(\"quarter\")])\n",
    "    .agg([pl.col(\"sharesHeldAtEndOfQtr\").first(),\n",
    "          #pl.col(\"cusip\").unique(),\n",
    "          pl.col(\"filedAsOfDate\").first(),\n",
    "          pl.col(\"accessionNumber\").first()\n",
    "          ])\n",
    "    .sort([\"cusip\", \"quarter\",\"filedAsOfDate\", \"accessionNumber\"], reverse=True)\n",
    "    ).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_cusips = [\"000000000\", \"\", \"0\"]\n",
    "# (pl.col(\"cusip\") != \"000000000\") & (pl.col(\"cusip\") != \"\") & (pl.col(\"cusip\") != \"0\")\n",
    "(df_filings\n",
    "    .filter((pl.col(\"cusip\").is_in(bad_cusips).is_not())) # filter out bad cusips\n",
    "    .groupby([pl.col(\"periodOfReport\").dt.year()])\n",
    "    .agg([\n",
    "        pl.col(\"accessionNumber\").first(),\n",
    "        pl.col(\"accessionNumber\").str.slice(-6).cast(pl.Int32).first().alias(\"new_accession_number\"),\n",
    "        pl.count().alias(\"count\"),\n",
    "        pl.col(\"submissionType\"),\n",
    "        pl.first(\"cusip\")       \n",
    "    ])\n",
    "    \n",
    "    .sort(\"accessionNumber\", reverse=True)\n",
    "    .limit(5)\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(df_filings\n",
    "    .filter((pl.col(\"cusip\").is_in(bad_cusips).is_not())) # filter out bad cusips\n",
    "    .sort([\"cusip\", \"periodOfReport\", \"filedAsOfDate\", \"accessionNumber\"], reverse=True)\n",
    "    .groupby([\"cusip\", (pl.col(\"periodOfReport\").dt.year()+\"Q\"+pl.col(\"periodOfReport\").dt.quarter()).alias(\"quarter\")])\n",
    "    .agg([\n",
    "        pl.col(\"periodOfReport\").first().alias(\"periodOfReport\"),\n",
    "        pl.count().alias(\"count\"),\n",
    "        pl.col(\"ID\").count().alias(\"ID_count\"),\n",
    "        pl.col(\"filedAsOfDate\").list().alias(\"list_of_fdates\"),\n",
    "        pl.col(\"sharesHeldAtEndOfQtr\").list().alias(\"list_of_shares\"),\n",
    "        pl.col(\"sharesHeldAtEndOfQtr\").first().alias(\"new_accession_number\"),\n",
    "        \n",
    "        pl.col(\"filedAsOfDate\").first().alias(\"filedAsOfDate\")\n",
    "        \n",
    "       \n",
    "    ])\n",
    "    \n",
    "    .sort([\"periodOfReport\", \"filedAsOfDate\"], reverse=True)\n",
    "    #.limit(1)\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Conditional aggregations\n",
    "import polars as pl\n",
    "\n",
    "bad_cusips = [\"000000000\", \"\", \"0\"]\n",
    "(df_filings\n",
    "    .filter((pl.col(\"cusip\").is_in(bad_cusips).is_not())) # filter out bad cusips\n",
    "    .sort([\"cusip\", \"periodOfReport\", \"filedAsOfDate\", \"accessionNumber\"], reverse=True)\n",
    "    .groupby([\"cusip\", (pl.col(\"periodOfReport\").dt.year()+\"Q\"+pl.col(\"periodOfReport\").dt.quarter()).alias(\"quarter\")])\n",
    "    .agg([\n",
    "        pl.col(\"periodOfReport\").first().alias(\"periodOfReport\"),\n",
    "        \n",
    "        pl.col(\"ID\").count().alias(\"ID_count\"),\n",
    "        pl.col(\"filedAsOfDate\").list().alias(\"list_of_fdates\"),\n",
    "        pl.col(\"sharesHeldAtEndOfQtr\").list().alias(\"list_of_shares\"),\n",
    "        pl.col(\"sharesHeldAtEndOfQtr\").first().alias(\"new_accession_number\"),\n",
    "        pl.col(\"filedAsOfDate\").first().alias(\"filedAsOfDate\"),\n",
    "        pl.col(\"accessionNumber\").str.slice(-6).cast(pl.Int32).first().alias(\"accession_rank\"),\n",
    "        (pl.col(\"submissionType\") == \"13F-HR\").sum().alias(\"original\"),\n",
    "        (pl.col(\"submissionType\") == \"13F-HR/A\").sum().alias(\"amendement\")\n",
    "        \n",
    "       \n",
    "    ])\n",
    "    \n",
    "    .sort([\"periodOfReport\", \"filedAsOfDate\"], reverse=True)\n",
    "    #.limit(1)\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "bad_cusips = [\"000000000\", \"\", \"0\"]\n",
    "(df_filings\n",
    "    .filter((pl.col(\"cusip\").is_in(bad_cusips).is_not())) # filter out bad cusips\n",
    "    .sort([\"cusip\", \"periodOfReport\", \"filedAsOfDate\", \"accessionNumber\"], reverse=True)\n",
    "    .groupby([\"cusip\", \"submissionType\"])\n",
    "    .agg([(pl.col(\"submissionType\").count()).alias(\"submission_count\")\n",
    "    .filter((pl.col(\"submissionType\") == \"13F-HR\") | (pl.col(\"submissionType\") == \"13F-HR/A\"))\n",
    "    \n",
    "      \n",
    "       \n",
    "    ])\n",
    "    .sort(\"submission_count\", reverse=True)\n",
    "    \n",
    "    \n",
    "    #.limit(1)\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply filtering within a group and use a custom function\n",
    "import polars as pl\n",
    "\n",
    "def filing_age() -> pl.Expr:\n",
    "    # return pl.col(\"filedAsOfDate\").cast(pl.DateType()) - pl.col(\"periodOfReport\").cast(pl.DateType())\n",
    "    return pl.col(\"filedAsOfDate\") - pl.col(\"periodOfReport\")\n",
    "\n",
    "def avg_count_submission_type(type: str) -> pl.Expr:\n",
    "    return filing_age().filter(pl.col(\"submissionType\") == type).mean().alias(f\"Average filing age for {type}\")\n",
    "\n",
    "\n",
    "\n",
    "bad_cusips = [\"000000000\", \"\", \"0\"]\n",
    "(df_filings\n",
    "    .groupby([\"cusip\", \"periodOfReport\"])\n",
    "    .agg([\n",
    "    avg_count_submission_type(\"13F-HR\"),\n",
    "    avg_count_submission_type(\"13F-HR/A\"),\n",
    "    (pl.col(\"submissionType\") == \"13F-HR\").sum().alias(\"original\"),\n",
    "    (pl.col(\"submissionType\") == \"13F-HR/A\").sum().alias(\"amendement\"),\n",
    "       \n",
    "    ])\n",
    "    \n",
    "    #.limit(1)\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sorting and then grouping aaaand dorting again within a group\n",
    "# we can also sort by another column in a group (still needs mental clarification )\n",
    "\n",
    "\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "def get_cusip_filedat() -> pl.Expr:\n",
    "    return pl.lit(\"Filed on..\") + pl.col(\"filedAsOfDate\")\n",
    "\n",
    "bad_cusips = [\"000000000\", \"\", \"0\"]\n",
    "(df_filings\n",
    "    .sort([\"cusip\",\"periodOfReport\", \"filedAsOfDate\"], reverse=True)\n",
    "    .groupby([\"cusip\", \"periodOfReport\"])\n",
    "    .agg([\n",
    "        get_cusip_filedat().first().alias(\"last filing\"),\n",
    "        get_cusip_filedat().last().alias(\"first filing\"),\n",
    "        get_cusip_filedat().sort().last().alias(\"alphabetical filing\"),\n",
    "        pl.col(\"submissionType\").sort_by(\"filedAsOfDate\").first().alias(\"first filing type\"),\n",
    "        \n",
    "          \n",
    "    ])\n",
    "    .limit(5)\n",
    "    \n",
    "    ).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold\n",
    "out = df_filings.select(\n",
    "    pl.fold(acc=pl.lit(0), f=lambda acc, x: acc + x, exprs=pl.col(\"*\")).alias(\"sum\"),\n",
    ")\n",
    "out.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## not sure how this works\n",
    "\n",
    "out = df_filings.filter(\n",
    "    pl.fold(acc=pl.lit(True), f=lambda acc, x: acc & x, exprs=pl.col(\"report_Quarter\") >1),\n",
    ")\n",
    "out.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fold and concat_str\n",
    "\n",
    "out = df_filings.select([\n",
    "        pl.concat_str([\"report_Year\", pl.lit(\"Q\"), \"report_Quarter\"])\n",
    "    \n",
    "                 ])\n",
    "out.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split the string and get the necessary element of the list\n",
    "\n",
    "out = df_filings.select([\n",
    "        pl.concat_str([\"report_Year\", pl.lit(\"Q\"), \"report_Quarter\"]),\n",
    "        pl.col(\"accessionNumber\").str.split(by=\"-\").arr.get(-1).cast(pl.UInt32)\n",
    "    \n",
    "                 ])\n",
    "out.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## window functions\n",
    "\n",
    "from aioitertools import groupby\n",
    "\n",
    "\n",
    "out = df_filings.select([\n",
    "    \"report_Year\",\n",
    "    \"report_Quarter\",\n",
    "    \"sharesHeldAtEndOfQtr\",\n",
    "    pl.col(\"valueTotal\").mean().over(\"report_Year\").alias(\"mean_valueTotal_by_year\"),\n",
    "    pl.col(\"sharesHeldAtEndOfQtr\").mean().over([\"report_Year\"]).alias(\"mean_shares_by_year\"),\n",
    "    pl.col(\"sharesHeldAtEndOfQtr\").mean().over([\"report_Year\", \"report_Quarter\"]).alias(\"mean_shares_by_year_and_quarter\"),\n",
    "    pl.col(\"sharesHeldAtEndOfQtr\").mean().alias(\"mean_shares\")\n",
    "    \n",
    "    \n",
    "    \n",
    "])\n",
    "(out\n",
    ".groupby([\"report_Year\", \"report_Quarter\"]).agg([pl.col(\"sharesHeldAtEndOfQtr\").first()])\n",
    ".sort([\"report_Year\", \"report_Quarter\"], reverse=True)).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## filter and select columns\n",
    "\n",
    "filtered = \\\n",
    "df_filings.filter(pl.col(\"cusip\") != \"\").select([\n",
    "    \"report_Year\",\n",
    "    \"cusip\",\n",
    "    \"sharesHeldAtEndOfQtr\",\n",
    "    \n",
    "])\n",
    "\n",
    "filtered.fetch(3)\n",
    "\n",
    "\n",
    "# let's sort it by cusip and then shares\n",
    "out = filtered.with_columns([\n",
    "    pl.col([\"cusip\", \"sharesHeldAtEndOfQtr\"]).sort(reverse=True).over(\"cusip\")\n",
    "    ])  \n",
    "out.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort and select\n",
    "\n",
    "# the different groppings by is confusing for fe for now\n",
    "out = df_filings.sort([\"cusip\", \"periodOfReport\"]).select([\n",
    "    pl.col(\"cusip\").head(3).list().over(\"cusip\").flatten(),\n",
    "    pl.col(\"periodOfReport\").sort_by(pl.col(\"sharesHeldAtEndOfQtr\"), reverse=True).head(3).list().over(\"cusip\").flatten().alias(\"fastest/group\")\n",
    "    #pl.col(\"\")\n",
    "    \n",
    "    \n",
    "])\n",
    "out.collect()\n",
    "\n",
    "# from the guide\n",
    "# out = df.sort(\"Type 1\").select(\n",
    "#     [\n",
    "#         pl.col(\"Type 1\").head(3).list().over(\"Type 1\").flatten(),\n",
    "#         pl.col(\"Name\").sort_by(pl.col(\"Speed\")).head(3).list().over(\"Type 1\").flatten().alias(\"fastest/group\"),\n",
    "#         pl.col(\"Name\").sort_by(pl.col(\"Attack\")).head(3).list().over(\"Type 1\").flatten().alias(\"strongest/group\"),\n",
    "#         pl.col(\"Name\").sort().head(3).list().over(\"Type 1\").flatten().alias(\"sorted_by_alphabet\"),\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "df_filings.select([\n",
    "    (pl.col(\"submissionType\") + pl.col(\"cikManager\")).alias(\"original\"),\n",
    "    (pl.col(\"sharesValue\") + pl.col(\"valueTotal\")).alias(\"sum\")\n",
    "    \n",
    "    \n",
    "]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from IPython.display import display\n",
    "import polars as pl\n",
    "pl.Config.set_tbl_rows(20)\n",
    "pl.Config.set_tbl_width_chars(50)\n",
    "pl.Config.set_fmt_str_lengths(50)\n",
    "#display(out.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "parquet_dir = r\"E:\\app_data\\sec_apps_data\\speed_test\\filings_13f_full\\filings_parquet\"\n",
    "\n",
    "\n",
    "df_filings = (pl.scan_parquet(f\"{parquet_dir}/949509-*.parquet\")\n",
    "              .filter((pl.col(\"cikManager\") == 949509) & (pl.col(\"filedAsOfDate\") >= \"2014-01-01\" ) \n",
    "                      #& (pl.col(\"cusip\") == \"01609W102\" ))\n",
    "              ))\n",
    " \n",
    "    # .filter(pl.col(\"sepal_length\") > 5)\n",
    "    # .groupby(\"species\")\n",
    "    # .agg(pl.all().sum())\n",
    "    #.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filings.collect().head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = grades.select([\n",
    "#     pl.concat_list(pl.all().exclude(\"student\")).alias(\"all_grades\")\n",
    "# ])\n",
    "\n",
    "# concatenating to list\n",
    "out = df_filings.select([pl.concat_list([\"valueTotal\", \"sharesValue\"]).alias(\"sum\")])\n",
    "out.fetch(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_pct = pl.element().rank(reverse=True) / pl.col(\"\").count()\n",
    "\n",
    "df_filings.with_column(\n",
    "    pl.concat_list([\"valueTotal\", \"sharesValue\"]).alias(\"values\")).select([\n",
    "        pl.all().exclude(\"values\"),\n",
    "        pl.col(\"values\").arr.eval(rank_pct, parallel=True).alias(\"values_rank\")\n",
    "    ]).collect().head(3)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas Course - Pluralsite - B3. Cleaning Data. Python Data Playbook (Chris Achard, 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"course/data/exercise/artwork_data.csv\")\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data.acquisitionYear.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.acquisitionYear.dtype\n",
    "\n",
    "data.acquisitionYear = data.acquisitionYear.astype(float)\n",
    "data.acquisitionYear.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulldf = pd.read_csv(r\"course/data/exercise/artwork_data.csv\", low_memory=False)\n",
    "fulldf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import full\n",
    "\n",
    "\n",
    "fulldf.dtypes\n",
    "# fulldf.height.astype(float)\n",
    "\n",
    "# pd.to_numeric(fulldf.height)\n",
    "fulldf.height[41339]\n",
    "\n",
    "pd.to_numeric(fulldf.height, errors=\"coerce\")\n",
    "pd.to_numeric(fulldf.height, errors=\"coerce\")[41339] # it turened the 'mm' to nan\n",
    "\n",
    "fulldf.height.dtype # still an object, we need to convert to float\n",
    "fulldf.height = pd.to_numeric(fulldf.height, errors=\"coerce\")\n",
    "\n",
    "fulldf.height.dtype # now it's a float\n",
    " \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"course/data/exercise/artwork_sample.csv\", low_memory=False)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.year = pd.to_numeric(data.year, errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes,\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.year.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.year.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.year.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.year.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.year.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.artist.min() # min() could be applied to a string too, but mean() would not make sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if to sum() string values, it will just concatenate them\n",
    "data.artist.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sum() # not much sense in doing it, but we can use sum() on the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.agg(\"min\") # this would get the min() of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.agg(['min', 'max']) # this would get the min() and max() of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.agg(['min', 'max', 'mean', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flipping the axis to get the mean of each column and not a row\n",
    "data.agg({'id' : ['sum', 'min'], 'depth' : ['sum', 'max']}) # this would get the mean of each numerical column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### example of my data and groupby\n",
    "# pandas reading\n",
    "import glob\n",
    "parquet_dir = r\"E:/app_data/sec_apps_data/speed_test/filings_13f_full/filings_parquet/\"\n",
    "import pandas as pd\n",
    "pandas_df = pd.read_parquet(glob.glob(f\"{parquet_dir}949509-*.parquet\"))\n",
    "pandas_df.sort_values(by=['cusip', 'periodOfReport', 'filedAsOfDate'],ascending=False) \\\n",
    "#groupby(['cusip', 'periodOfReport', 'filedAsOfDate'])['sharesHeldAtEndOfQtr'].agg(pd.Series.max).tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"course/data/exercise/artwork_sample.csv\", low_memory=False)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.height.mean()\n",
    "data.height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the height\n",
    "height = data.height\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = (height - height.mean()) / height.std()\n",
    "norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the height so it's between 0 and 1\n",
    "minmax = (height - height.min()) / (height.max() - height.min())\n",
    "minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax.min(), minmax.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.height = minmax\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['standardized_height'] = norm\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transforming data\n",
    "import pandas as pd\n",
    "data = pd.read_csv(r\"course/data/exercise/artwork_sample.csv\", low_memory=False)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.height.transform(lambda x: print(  x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.height.transform(lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.height.transform(lambda x: x/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " # transform on groupby\n",
    " data.groupby('artist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data.groupby('artist').transform('nunique')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data.artist.nunique()\n",
    "# data.groupby(['artist', 'medium']).transform('nunique')\n",
    "\n",
    "data.groupby('artist')['height'].transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['mean_height_by_artist'] = data.groupby('artist')['height'].transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data\n",
    "\n",
    "import pandas as pd\n",
    "data = pd.read_csv(r\"course/data/exercise/artwork_data.csv\", low_memory=False)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reducing the dataframe to only the columns we need\n",
    "data.filter(items=['id','artist', 'artistId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if I don't know the exact columns name\n",
    "data.filter(like='year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case insensitive regex\n",
    "data.filter(regex=\"(?i)year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use filter on rows too by using axis=0\n",
    "data.filter(axis=0, regex=\"^100.$\") # only fiters on index's values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # only even numbers of index\n",
    "data.filter(axis=0, regex=\"^100(0|2|4|6|8)$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dropping columns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"course/data/exercise/artwork_sample.csv\", low_memory=False)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(0)  #drop the first row with the index 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop a column\n",
    "data.drop('id',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['id', 'height', 'width', 'depth']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to drop rows we pass a list of labels\n",
    "data.drop(labels=[0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to drop permanently we use inplace=True\n",
    "data.drop(columns=['id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to avoid working with the columns we don't need in the first place, we can drop them when reading the data in\n",
    "data = pd.read_csv(r\"course/data/exercise/artwork_sample.csv\", low_memory=False, usecols=['artist', 'title'])\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chaning the casing of the columns\n",
    "data = pd.read_csv(r\"course/data/exercise/artwork_sample.csv\", low_memory=False)\n",
    "data.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[x.lower() for x in data.columns]\n",
    "data.columns = [x.lower() for x in data.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with map()\n",
    "data = pd.read_csv(r\"course/data/exercise/artwork_sample.csv\", low_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = map(lambda x: x.lower(), data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"course/data/exercise/artwork_sample.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with regex : we look for uppercase letters, and add '_' right before it and cast all to lowercase\n",
    "import re\n",
    "data.columns = [re.sub(r'([A-Z])', r'_\\1', x).lower() for x in data.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reanming columns\n",
    "data = pd.read_csv(r\"course/data/exercise/artwork_sample.csv\", low_memory=False)\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns={\"thumbnailUrl\": \"thumbnail\"})\n",
    "data.rename(columns={\"thumbnailUrl\": \"thumbnail\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns=lambda x: x.lower(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = ['id', 'Accession_number', 'Artist', 'Artistrole', 'artistid', 'title',\n",
    "       'datetext', 'medium', 'creditline', 'year', 'acquisitionyear',\n",
    "       'dimensions', 'width', 'height', 'depth', 'units', 'inscription',\n",
    "       'thumbnailcopyright', 'thumbnail', 'url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"course/data/exercise/artwork_sample.csv\", low_memory=False, names=\\\n",
    "    ['id', 'Accession_number', 'Artist', 'Artistrole', 'artistid', 'title',\n",
    "       'datetext', 'medium', 'creditline', 'year', 'acquisitionyear',\n",
    "       'dimensions', 'width', 'height', 'depth', 'units', 'inscription',\n",
    "       'thumbnailcopyright', 'thumbnail', 'url'], header=0)\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing and Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(r\"course/data/exercise/artwork_sample.csv\", low_memory=False)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['id'] # for one column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['id'][1] # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get multiple columns\n",
    "data[['artist', 'title']]\n",
    "# data.filter(items=['artist', 'title']) # same as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0:5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['artist', 'title']][1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[1:5]['artist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter on rows\n",
    "data[data['year'] > 1800] # we filter on row value and select all df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we want to filter on row and select only a subset of columns\n",
    "data[data.year > 1800][['artist', 'title']]\n",
    "data[data.year > 1800]['year']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loc and iloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(r\"course/data/exercise/artwork_sample.csv\", low_memory=False)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loc[rows, columns]\n",
    "# loc uses labels to select rows and columns. In case of index, in our case it's the row number, but it can be anythin else\n",
    "data.loc[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[0:1, ['artist', 'title']]\n",
    "\n",
    "data.loc[0:2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[0:2,'title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[0:2, ['artist', 'title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also use precise rows we need. Not the range of rows\n",
    "data.loc[[1,5], ['artist', 'title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also slise columns\n",
    "data.loc[0:1, 'id': 'title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also filter with loc\n",
    "data.loc[data.artist == 'Blake, Robert',:]\n",
    "data.loc[data.artist == 'Blake, Robert', 'title']\n",
    "data.loc[data.artist == 'Blake, Robert', 'title':'year']\n",
    "data.loc[data.artist == 'Blake, Robert', ['artist','year']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(r\"course/data/exercise/artwork_sample.csv\", low_memory=False)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iloc uses integer positions of rows and columns\n",
    "data.iloc[0,:]\n",
    "data.iloc[0:3,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to better see the difference it makes for the index, we can use a different index\n",
    "data.set_index('id', inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[0:3,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[1035:1037,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same integer positin is used for columns\n",
    "data.iloc[0:3,0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we reset the index inplace, all will go back to the original index\n",
    "data.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data\n",
    "data.iloc[0:3,0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select spedific rows and columns and not the range\n",
    "data.iloc[[5,8],[0,3, 7]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering with str.contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(r\"course/data/exercise/artwork_sample.csv\", low_memory=False)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.medium.str.contains('Graphite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can pass this to .loc\n",
    "data.loc[data.medium.str.contains('Graphite'), ['artist','title', 'medium']] # default ir case sensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data.medium.str.contains('Graphite', case=False), ['artist','title', 'medium']] # case insensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can do the same with regex\n",
    "data.loc[data.medium.str.contains('(?i)Graphite', regex=True), ['artist','title', 'medium']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple filters with regex\n",
    "data.loc[data.medium.str.contains('(?i)Graphite', regex=True) \\\n",
    "         | data.medium.str.contains('(?i)Line', regex=True), ['artist','title', 'medium']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simpler version\n",
    "data.loc[data.medium.str.contains('graphite|line', case=False, regex=True), ['artist','title', 'medium']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to use a str method on not a string, we first need to convert it to a string\n",
    "data.year.astype(str).str.contains('1826')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to filter\n",
    "data.loc[data.year.astype(str).str.contains('1826')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"course/data/exercise/artwork_data.csv\", low_memory=False)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when working with strings or non strings converted to strings, if there are NaNs, we need actiate na=False\n",
    "data.loc[data.dimensions.str.contains('support', na=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(r\"course/data/exercise/artwork_data.csv\", low_memory=False)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with whitespaces\n",
    "# this identifies rows where in the column 'title' there are whitespaces at the end of the string\n",
    "data.loc[data.title.str.contains('\\s$', regex=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this line deletes the whitespaces from the columln 'title'\n",
    "data.title.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.title = data.title.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data.title.str.contains('\\s$', regex=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.title.str.lstrip() # left strip\n",
    "data.title.str.rstrip() # right strip\n",
    "# with transform\n",
    "# a reminder - this only identifies rows, not deletes them\n",
    "data.title.transform(lambda x: x.str.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace bad data with NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(r\"course/data/exercise/artwork_data.csv\", low_memory=False)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the df has nans\n",
    "data.isna().any()\n",
    "# data.isna().all() # if an entire row has nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many nans in which columns\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().mean().mul(100) # percentage of nans in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask to identify the rows where there are NaNs for the column 'medium'\n",
    "pd.isna(data.loc[:, 'medium'])\n",
    "\n",
    "# and show the entire df but filter out only where there are nans in 'medium' \n",
    "data.loc[pd.isna(data.loc[:, 'medium'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.replace({'dateText': {'date not known': nan}}, inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.read_csv(r\"course/data/exercise/artwork_data.csv\", low_memory=False)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way to deal with selecting rows with what we see as bad data, then selecting only the needed column \n",
    "data.loc[data.dateText == 'date not known', ['dateText']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and then# assigning NaN to the bad rows of that column\n",
    "data.loc[data.dateText == 'date not known', ['dateText']] = nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select rows where the year is NOT nan and also has a value other than a number\n",
    "data.loc[data.year.notnull() & data.year.astype(str).str.contains('[^0-9]', regex=True)] \n",
    "# data.loc[data.year.astype(str).str.contains('[^0-9]', regex=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# be very careful with not forgetting to indicate which columns exactly needs to be set as NaN. If not, all df \n",
    "# will be set to NaN\n",
    "data.loc[data.year.notnull() & data.year.astype(str).str.contains('[^0-9]', regex=True), ['year']] = nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[67968:67969]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling missing data with value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"course/data/exercise/artwork_data.csv\", low_memory=False)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna(0)\n",
    "data.depth.fillna(0)\n",
    "data.fillna(value={'depth':0})\n",
    "data.fillna(value={'depth':0}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping rows of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"course/data/exercise/artwork_data.csv\", low_memory=False)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to drop rows with nan in the entire df, if a nan is in any column\n",
    "data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it leaves only rows without nans\n",
    "data.dropna(how='any').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.dropna() is the same as data.dropna(how='any')\n",
    " # dropna(how='all') drops rows where all the values in all the columns in a row are NaN\n",
    "data.dropna(how='all').shape # since the shape is the same as the original one, it means there are no rows with all nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(thresh=14).shape # thresh sets up how many columns must have a NaN in the dame row for it to be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to only drop rows with NaN in specific columns\n",
    "# data.dropna(subset=['year', 'acquisitionYear']).shape\n",
    "# the same as with 'any'\n",
    "# data.dropna(subset=['year', 'acquisitionYear'], how='any').shape # any of the columns must have a NaN\n",
    "data.dropna(subset=['year', 'acquisitionYear'], how='all').shape # all of the columns must have a NaN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to actually change the df\n",
    "data.dropna(subset=['year', 'acquisitionYear'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"course/data/exercise/artwork_sample.csv\", low_memory=False)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates() # drops rows with all columns in a row being duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(subset=['artist']) # drops rows with only some colulmns being duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(subset=['artist'], keep='first') # we can specify which rows to keep if there are duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(subset=['artist'], keep='last') # we can specify which rows to keep if there are duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(subset=['artist'], keep=False) # or we can drop both duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(subset=['artist'], keep='first', inplace=True) # we can specify which rows to keep if there are duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with full dataset\n",
    "data = pd.read_csv(r\"course/data/exercise/artwork_data.csv\", low_memory=False)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to just see the duplicates\n",
    "data.duplicated()  # returns only for fully duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show duplicats in specific columns\n",
    "data.duplicated(subset=['artist', 'title'], keep=False) # this is crucial to identify the duplicates. \n",
    "# We need to set the keep to False, so the duplicates are highlighted as True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data.duplicated(subset=['artist', 'title'], keep=False) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# findd a row with certain value in a column\n",
    "data.loc[data.title.str.contains('The Circle of the Lustful: Francesca da ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another Course on pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.\n",
    "\n",
    "(r\"E:/Downloads/_Pandas/Pandas Playbook. Manipulating Data (Reindert-Jan Ekker, 2018)/weather.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PRESSURE'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out the most common temperature\n",
    "df.TEMP.mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the values and their counts for a column\n",
    "df['TEMP'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some plotting\n",
    "df['TEMP'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TEMP'].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TEMP'].plot.hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r\"E:/Downloads/_Pandas/Pandas Playbook. Manipulating Data (Reindert-Jan Ekker, 2018)/weather.csv\").head()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TEMP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " df['TEMP'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = df.T\n",
    "dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft[2]['TIME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft[2][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.DataFrame([['John'], ['Bob'], ['Anne']], index=[4, 3, 4])\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[0][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['PRESSURE', 'TIME', 'TEMP']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TIME'][[3, 1, 4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[2:4][['TEMP', 'PRESSURE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['TEMP', 'PRESSURE']][2:4] # slices are only used for rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft[3:][[1, 3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PRESSURE'][2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft[:2] # with integer clicing, the end is not included, but with string slicing, the end IS included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft['TIME':'PRESSURE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loc and iloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capitals = pd.DataFrame(\n",
    "    [\n",
    "    [\"Ngerulmud\",391,1.87],\n",
    "    [\"Vatican City\",826,100],\n",
    "    [\"Yaren\",1100,10.91],\n",
    "    [\"Funafuti\",4492,45.48],\n",
    "    [\"City of San Marino\",4493]\n",
    "    ], \n",
    "    index = [\"Palau\", \"Vatican City\", \"Nauru\", \"Tuvalu\", \"San Marino\"],\n",
    "    columns=['Capital', 'Population', 'Percentage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "capitals.loc['Nauru', 'Population']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same as above\n",
    "capitals['Population']['Nauru']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capitals.loc[['Nauru', 'Tuvalu'], ['Population', 'Percentage']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capitals.loc['Palau': 'Nauru', ['Population', 'Percentage']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capitals.loc[['San Marino', 'Vatican City']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capitals.iloc[[4, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capitals.iloc[[4, 1], 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capitals.iloc[:, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering with boolean indexing\n",
    "capitals[[True, True, False, True, False]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capitals['Percentage'] > 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capitals[capitals['Percentage'] > 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades = pd.DataFrame([[6, 4], [7, 8], [6, 7], [6, 5], [5, 2]], \n",
    "                       index = ['Mary', 'John', 'Ann', 'Pete', 'Laura'],\n",
    "                       columns = ['test_1', 'test_2'])\n",
    "grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades['test_2'] <= grades['test_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grades[grades['test_2'] <= grades['test_1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades.mean() > 5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades.loc[:, grades.mean() > 5.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades.loc[['Laura', 'John'], 'test_2'] += 1\n",
    "# the same as above\n",
    "grades.loc[['Laura', 'John'], 'test_2'] = grades.loc[['Laura', 'John'], 'test_2'] + 1\n",
    "grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades['test_1'] += .5\n",
    "grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades.loc['Mary'] += 2\n",
    "grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades.loc['Pete'] = [7, 9]\n",
    "grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrong way to do it as the seconds command will work on strings and not on numbers\n",
    " \n",
    "grades[grades < 6] = 'Fail'\n",
    "grades[grades >= 6] = 'Pass'\n",
    "grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades = pd.DataFrame([[6, 4], [7, 8], [6, 7], [6, 5], [5, 2]], \n",
    "                       index = ['Mary', 'John', 'Ann', 'Pete', 'Laura'],\n",
    "                       columns = ['test_1', 'test_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# right way to do it\n",
    "failing = grades < 6\n",
    "passing = grades >= 6\n",
    "grades[failing] = 'Fail'\n",
    "grades[passing] = 'Pass'\n",
    "grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades = pd.DataFrame([[6, 4], [7, 8], [6, 7], [6, 5], [5, 2]], \n",
    "                       index = ['Mary', 'John', 'Ann', 'Pete', 'Laura'],\n",
    "                       columns = ['test_1', 'test_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average of the grades by rows\n",
    "grades.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades.mean(axis=1) > 6 # boolean indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a new column\n",
    "grades['passed'] = grades.mean(axis=1) > 6\n",
    "grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning new values\n",
    "# do not use.... grades['test_2][\"Ann\"] = 7\n",
    "\n",
    "# correct way is with loc\n",
    "grades.loc['Ann', 'test_2'] = 8\n",
    "grades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sortings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capitals.sort_index(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capitals.sort_index(ascending=False, inplace=True)\n",
    "capitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capitals.sort_index(axis=1)\n",
    "capitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capitals.sort_values(by='Percentage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades.sort_values(by=['test_1', 'test_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades.sort_values(by=['test_1', 'test_2'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the ascending or descending order for each column\n",
    "grades.sort_values(by=['test_1', 'test_2'], ascending=[True, False], inplace=True)\n",
    "grades\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pandas through with duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb as db\n",
    "con = db.connect(':memory:')\n",
    "dir = r\"E:/Downloads/_Pandas/Pandas Playbook. Manipulating Data (Reindert-Jan Ekker, 2018)/weather.csv\"\n",
    "df = con.execute(f\"SELECT * FROM read_csv_auto('{dir}')\").df()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['MONTH', 'DAY'], ascending=[True, False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r\"E:/Downloads/_Pandas/Pandas Playbook. Manipulating Data (Reindert-Jan Ekker, 2018)/weather_m4.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n",
    "df[['MIN_TEMP_GROUND',\n",
    "       'VIEW_RANGE', 'CLOUD',\n",
    "       'WEATHER_CODE']].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask with boolean indexing on nan values\n",
    "df.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().any() # for columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().any(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if thera columns with all values as nan\n",
    "df.isnull().all() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same for rows\n",
    "df.isnull().all(axis=1) \n",
    "# to see only those rows with true\n",
    "df.isnull().all(axis=1).any() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.isnull().all(axis=1) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the opposite of isnull\n",
    "df.notnull().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MIN_TEMP_GROUND'].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "every_6th_row = pd.Series(range(5, len(df), 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MIN_TEMP_GROUND'].loc[every_6th_row] # select evey 6th row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check that indeed all of the 6th rows have proper values\n",
    "df['MIN_TEMP_GROUND'].loc[every_6th_row].notnull().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check that the values in the rest of the column (those that are not in the 6th row) are indeed nan\n",
    "df['MIN_TEMP_GROUND'].drop(every_6th_row).isnull().all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to remove a column\n",
    "df.drop(columns='WEATHER_CODE', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing values\n",
    "df['MIN_TEMP_GROUND'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MIN_TEMP_GROUND'].fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MIN_TEMP_GROUND'].fillna(method='bfill', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check if the values are indeed filled\n",
    "df.isnull().any()\n",
    "# 'MIN_TEMP_GROUND' shows False, meaning there are no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see rows with missing data\n",
    "df[df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check the dates on which the data is missing we can use loc\n",
    "df.loc[df.isnull().any(axis=1), 'YYYYMMDD']\n",
    "# the dates are repeating a lot, so we can use value_counts to see how many times each date appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.isnull().any(axis=1), 'YYYYMMDD'].value_counts()\n",
    "# it means there are 20 rows for 20160821 with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with na\n",
    "nulls_dropped = df.dropna()\n",
    "nulls_dropped.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_thresh = df.dropna(thresh=7)\n",
    "drop_thresh[drop_thresh.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_thresh.fillna(0, inplace=True)\n",
    "drop_thresh[drop_thresh.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select all rows with null values\n",
    "rows_to_fill = df.isnull().any(axis=1)\n",
    "df.loc[rows_to_fill]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill with the mean of the column\n",
    "nulls_filled = df.fillna(df.mean())\n",
    "# check the filled rows. we use previously created rows_to_fill\n",
    "nulls_filled.loc[rows_to_fill]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill with mode insted of mean\n",
    "df.fillna(df.mode().iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(df.mode().iloc[0], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling bad data other than nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athletes = pd.read_csv(r\"E:/Downloads/_Pandas/Pandas Playbook. Manipulating Data (Reindert-Jan Ekker, 2018)/athletes.csv\")\n",
    "athletes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "athletes.plot.scatter(x='height', y='weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cProfile import label\n",
    "\n",
    "\n",
    "heights = athletes['height']\n",
    "heights.plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = heights.quantile(0.25)\n",
    "q3 = heights.quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "\n",
    "pmin = q1 - 1.5 * iqr\n",
    "pmax = q3 + 1.5 * iqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwh = heights.where(heights.between(pmin, pmax), heights.mean()) # boolean mask that selects\n",
    "# true for values that are between pmin and pmax and puts the mean value for the rest\n",
    "# IF nothing is specified for the false values, they are replaced with NaN!!\n",
    "# heights.where(heights.between(pmin, pmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the original and the new values after the transformation\n",
    "compare  = pd.DataFrame({'before': heights, 'after': nwh})\n",
    "compare.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare  = pd.DataFrame({'before': heights, 'after': nwh})\n",
    "compare.plot.box()\n",
    "compare.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heights.where(heights.between(pmin, pmax), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athletes.plot.scatter(x='height', y='weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are duplicate rows\n",
    "athletes.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the duplicated rows\n",
    "athletes[athletes.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the duplicated rows\n",
    "athletes.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athletes.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to explore the data and see unique values\n",
    "athletes['nationality'].drop_duplicates() # returns cleaned df\n",
    "# athletes['nationality'].unique() # returns numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athletes['nationality'].drop_duplicates().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value counts\n",
    "athletes['nationality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athletes['sex'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athletes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athletes[['gold', 'silver', 'bronze']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum() on a string, just concatenates it\n",
    "athletes['bronze'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athletes['bronze'].astype(int)\n",
    "# error - ValueError: invalid literal for int() with base 10: 'O'\n",
    "# there is a row with string 'O' and not a zero. let's fix it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's find all the rows with string 'O' in column 'bronze'\n",
    "athletes[athletes['bronze'] == 'O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's now change the string 'O' to 0 for this specific row\n",
    "athletes.loc[7521, ['gold', 'silver', 'bronze']] = 0\n",
    "\n",
    "# and now, let's change the type of these columns to int \n",
    "athletes[['gold', 'silver', 'bronze']] = athletes[['gold', 'silver', 'bronze']].astype(int)\n",
    "athletes[['gold', 'silver', 'bronze']].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athletes.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athletes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athletes.index = athletes['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athletes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# setting an existing column as index\n",
    "athletes.set_index('id', drop=True, inplace=True)\n",
    "athletes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming columns\n",
    "athletes.rename(columns={'nationality': 'country', 'sport': 'discipline'}, inplace=True)\n",
    "athletes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming rows\n",
    "athletes.rename(index={736041664: 000, 532037425: 1111}, inplace=True)\n",
    "athletes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reseting index\n",
    "df = pd.read_csv(r\"E:/Downloads/_Pandas/Pandas Playbook. Manipulating Data (Reindert-Jan Ekker, 2018)/weather_m4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.ones([5,4]), columns=['a', 'b', 'c', 'd'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actually changing the original df\n",
    "df *= 2\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a row and dibide it by 2\n",
    "df.loc[1] /= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or a column\n",
    "df['b'] -= 1\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # new df\n",
    "df2 = pd.DataFrame(np.ones([3,2]), columns=['d', 'e'], index=[2,4,5])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df + df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[2] * df2.loc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df - df.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df - pd.Series({'a': 5, 'b': 5, 'e': 5, 'f': 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtract mean of rows from each row\n",
    "df.sub(df.mean(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'sin': np.arange(0, 5*np.pi, 0.01),\n",
    "                   'cos':np.arange(0.5*np.pi, 5.5*np.pi, 0.01)})\n",
    "df\n",
    "#' apply sine function to the df\n",
    "df = np.sin(df)\n",
    "%matplotlib inline\n",
    "\n",
    "df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom function\n",
    "# calculate the interquartile range\n",
    "def iqr(col):\n",
    "    return col.quantile(0.75) - col.quantile(0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing the custom function to the apply method\n",
    "df.apply(iqr) # this way it's applied to columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now to rows\n",
    "df.apply(iqr, axis=1) # this way it's applied to rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def somefunc(x):\n",
    "    return np.abs(x+.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's apply the function to every cell in the df. We can't use apply as it works on rows and columns\n",
    "df.applymap(somefunc) # important not to use () around the function name as it's the applymap that will trigger it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.applymap(somefunc).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athletes = pd.read_csv(r\"E:/Downloads/_Pandas/Pandas Playbook. Manipulating Data (Reindert-Jan Ekker, 2018)/athletes_clean.csv\")\n",
    "athletes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = athletes.groupby('nationality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " g = athletes.groupby('nationality')[['gold', 'silver', 'bronze']]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = athletes.groupby('sport')[['weight', 'height']]\n",
    "g.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = athletes.groupby(['sport', 'sex'])[['weight', 'height']]\n",
    "g.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = athletes.groupby(['sport', 'sex'])[['weight', 'height']]\n",
    "# g.last()\n",
    "# g.first()\n",
    "g.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stack, unstack, pivot, pivot_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = pd.read_csv(r\"E:/Downloads/_Pandas/Pandas Playbook. Manipulating Data (Reindert-Jan Ekker, 2018)/monthly_data.csv\")\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change index\n",
    "# m.set_index('YYYY', inplace=True)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to move values from all the 12 moths columns into a single column\n",
    "m.stack().head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.stack().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unstack\n",
    "w = athletes.groupby(['sport', 'sex'])['weight'].mean()\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we'll move it from all the weights being in one column to separate columns for female and male, since\n",
    "# these are are the values from the internal group - sex\n",
    "w.unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot\n",
    "\n",
    "p = pd.DataFrame({'id': [823905, 823905,\n",
    "                         235897, 235897, 235897,\n",
    "                         983422, 983422],\n",
    "                  'item': ['prize', 'unit', \n",
    "                           'prize', 'unit', 'stock', \n",
    "                           'prize', 'stock'],\n",
    "                  'value': [3.49, 'kg',\n",
    "                            12.89, 'l', 50,\n",
    "                            0.49, 4]})\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot\n",
    "p.pivot('id', 'item', 'value') \n",
    "# 'id' will be the column for each individual record. It will use different values from 'item' as different columlns\n",
    "# and it will fill these new coumns from values from 'value'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades = pd.DataFrame([[6, 4, 5], [7, 8, 7], [6, 7, 9], [6, 5, 5], [5, 2, 7]], \n",
    "                       index = ['Mary', 'John', 'Ann', 'Pete', 'Laura'],\n",
    "                       columns = ['test_1', 'test_2', 'test_3'])\n",
    "grades.reset_index(inplace=True)\n",
    "grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades.melt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades.melt(id_vars=['index']) # specify the index column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining DataF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades = pd.DataFrame([[6, 4, 5], [7, 8, 7], [6, 7, 9], [6, 5, 5], [5, 2, 7]], \n",
    "                       index = ['Mary', 'John', 'Ann', 'Pete', 'Laura'],\n",
    "                       columns = ['test_1', 'test_2', 'test_3'])\n",
    "grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new column to an existing df\n",
    "grades['test_4'] = pd.Series({'John': 5, 'Ann': 8, 'Pete': 9, 'Mary': 7, 'Laura': 10})\n",
    "grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now adding a new row\n",
    "grades.loc['Bob'] = [2, 3, 4, 5]\n",
    "grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append\n",
    "new_row = pd.Series({'test_1': 8, 'test_2': 9, 'test_3': 7, 'test_4': 8}, name='Kim')\n",
    "grades.append(new_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# addging a new column\n",
    "grades['stud_nr'] = [113, 121, 123, 135, 139, 141]\n",
    "# order colulmns as we need them\n",
    "grades = grades[['stud_nr', 'test_1', 'test_2', 'test_3', 'test_4']]\n",
    "grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other = pd.DataFrame([[139, 7, 7],\n",
    "                       [123, 8, 6],\n",
    "                       [142, 4, 5],\n",
    "                       [113, 7, 9],\n",
    "                       [155, 10, 9],\n",
    "                       [121, 6, 4]], \n",
    "                       columns = ['stud_nr', 'exam1', 'exam2'])\n",
    "other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine two df with merge. it's like join in SQL\n",
    "grades.merge(other) # by default it's an inner join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades.merge(other, how='left') # will show those students present in grades df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades.merge(other, how='right') # will show those students present in OTHER df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades.merge(other, how='outer') # will show those students present in both dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(r\"E:\\app_data\\dropbox_13f_files\\crspq.csv\", nrows=100).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T17:16:32.650740Z",
     "iopub.status.busy": "2022-08-09T17:16:32.650740Z",
     "iopub.status.idle": "2022-08-09T18:07:33.597294Z",
     "shell.execute_reply": "2022-08-09T18:07:33.597294Z",
     "shell.execute_reply.started": "2022-08-09T17:16:32.650740Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 43min 31s\n",
      "Wall time: 51min\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "# concatenate different files with the same cik in their name into one file per cik\n",
    "\n",
    "import os, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from fnmatch import fnmatch\n",
    "\n",
    "processed_tables_parquet_till_2013 = r\"E:\\app_data\\dropbox_13f_files\\processed_tables\\processed_tables_parquet_till_2013\"\n",
    "cik_parquet_till_2013 = r\"E:\\app_data\\dropbox_13f_files\\processed_tables\\cik_parquet_till_2013\"\n",
    "\n",
    "# list all files in a folder and subfolders\n",
    "\n",
    "\n",
    "def files_in_dir(dir, ext=\"*.parquet\"):\n",
    "    l = []\n",
    "    for path, subdirs, files in os.walk(dir):\n",
    "        for name in files:\n",
    "            if fnmatch(name, ext):\n",
    "                l.append(os.path.join(path, name))\n",
    "    return l\n",
    "\n",
    "parquet_list = files_in_dir(processed_tables_parquet_till_2013)\n",
    "\n",
    "\n",
    "# extract a list of ciks from a list of file names \n",
    "def get_list_of_cik(parquet_list: list) -> list:\n",
    "    cik_list = []\n",
    "    for file in parquet_list:\n",
    "        cik_list.append(file.split('\\\\')[-1].split('-')[0])\n",
    "    return sorted(cik_list)\n",
    "\n",
    "cik_list = get_list_of_cik(parquet_list)\n",
    "cik_list = [int(x) for x in list(set(cik_list))]\n",
    "# cik_list = [315066]\n",
    "\n",
    "data_dir = Path(processed_tables_parquet_till_2013)\n",
    "for cik in cik_list:\n",
    "# cik = 9749\n",
    "# temp_dir = r\"E:\\app_data\"\n",
    "# full_df = pd.concat(pd.read_parquet(f) for f in data_dir.glob(f\"{cik}-*.parquet\"))\n",
    "\n",
    "    full_df = pd.concat(pd.read_parquet(f) for f in data_dir.glob(f\"{cik}-*.parquet\"))\n",
    "    full_df = full_df[full_df.filedAsOfDate <= '2013-12-31']\n",
    "\n",
    "    full_df['cusip'] = full_df['cusip9'].astype(str).str.upper()\n",
    "    full_df['titleOfClass'] = full_df.get('titleOfClass',default=np.nan)\n",
    "    full_df['nameOfIssuer'] = full_df.get('titleOfClass',default=np.nan)\n",
    "    full_df['managerName'] = full_df.get('managerName',default=np.nan)\n",
    "    full_df['putCall'] = full_df.get('putCall',default=np.nan)\n",
    "    full_df['shrsOrPrnAmt'] = full_df.get('shrsOrPrnAmt',default=np.nan)\n",
    "    full_df['sharesValue'] = full_df.get('sharesValue',default=np.nan)\n",
    "    full_df['sharesValue'] = full_df['sharesValue'] * 1000\n",
    "    \n",
    "    full_df['sharesHeldAtEndOfQtr'] = full_df.get('sharesHeldAtEndOfQtr', default=np.nan)\n",
    "    full_df['submissionType'] = full_df.get('submissionType',default=np.nan)\n",
    "    full_df['quarter'] = full_df['periodOfReport'].dt.year.astype(str)+\"Q\"+full_df['periodOfReport'].dt.quarter.astype(str)\n",
    "    full_df['updated_at'] = pd.Timestamp.now()\n",
    "    full_df.drop(columns=[\"cusip8\", \"cusip9\"], inplace=True)\n",
    "    \n",
    "    # if there are no rows in the df, then move on to another file\n",
    "    if full_df.shape[0] == 0: continue\n",
    "        \n",
    "    full_df = (full_df.sort_values(by=['cusip', 'periodOfReport', 'filedAsOfDate', 'accessionNumber'], ascending=False))\n",
    "    fdate = full_df.filedAsOfDate.dt.date.max()\n",
    "\n",
    "    dtypes = {'cusip': str, 'nameOfIssuer': str , 'titleOfClass': str, 'periodOfReport': np.datetime64,\n",
    "            'putCall': str, 'shrsOrPrnAmt': str, 'sharesValue': float, 'sharesHeldAtEndOfQtr': float, \n",
    "            'filedAsOfDate': np.datetime64, 'accessionNumber': str, 'cikManager' : int, 'managerName': str,\n",
    "            'quarter': str, 'submissionType': str, 'updated_at': np.datetime64}\n",
    "    \n",
    "    # grouping by cusip and summing up shares and value\n",
    "    attributes = {'accessionNumber': 'last', 'cikManager':'last', 'managerName':'last',\n",
    "    'periodOfReport':'last', 'submissionType':'last', 'filedAsOfDate':'first', 'cusip':'last', 'nameOfIssuer': 'last',\n",
    "    'titleOfClass' :'last', 'sharesValue':'last',  'sharesHeldAtEndOfQtr':\"last\", \"shrsOrPrnAmt\":\"last\",\n",
    "    'putCall':'last', 'xml_flag':'last', 'updated_at':'last', 'quarter':'last'}\n",
    "\n",
    "    full_df  = full_df.groupby([\"cikManager\",\"cusip\", \"quarter\"], as_index=False).agg(attributes)\n",
    "\n",
    "    full_df.astype(dtypes).to_parquet(f\"{cik_parquet_till_2013}/{cik}-{fdate}.parquet\", engine='pyarrow')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T21:24:08.649523Z",
     "iopub.status.busy": "2022-08-09T21:24:08.649523Z",
     "iopub.status.idle": "2022-08-09T21:24:10.037398Z",
     "shell.execute_reply": "2022-08-09T21:24:10.037398Z",
     "shell.execute_reply.started": "2022-08-09T21:24:08.649523Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.38 s\n",
      "Wall time: 1.38 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "from fnmatch import fnmatch\n",
    "\n",
    "processed_tables_csv = r\"E:\\app_data\\dropbox_13f_files\\processed_tables\\processed_tables\"\n",
    "my_filings_parquet = r\"E:\\app_data\\sec_apps_data\\speed_test\\filings_13f_full\\filings_parquet\"\n",
    "\n",
    "def files_in_dir_list(dir, ext=\"*.parquet\"):\n",
    "    l = []\n",
    "    for path, subdirs, files in os.walk(dir):\n",
    "        for name in files:\n",
    "            if fnmatch(name, ext):\n",
    "                l.append(os.path.join(path, name))\n",
    "    return l\n",
    "\n",
    "l = files_in_dir_list(my_filings_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T23:15:10.635587Z",
     "iopub.status.busy": "2022-08-09T23:15:10.635587Z",
     "iopub.status.idle": "2022-08-09T23:15:10.666840Z",
     "shell.execute_reply": "2022-08-09T23:15:10.666840Z",
     "shell.execute_reply.started": "2022-08-09T23:15:10.635587Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 15.6 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "from fnmatch import fnmatch\n",
    "\n",
    "processed_tables_csv = r\"E:\\app_data\\dropbox_13f_files\\processed_tables\\processed_tables\"\n",
    "my_filings_parquet = r\"E:\\app_data\\sec_apps_data\\speed_test\\filings_13f_full\\filings_parquet\"\n",
    "cik_parquet_till_2013 = r\"E:\\app_data\\dropbox_13f_files\\processed_tables\\cik_parquet_till_2013\"\n",
    "\n",
    "def files_in_dir_dict(dir, ext=\"*.parquet\"):\n",
    "    d = {}\n",
    "    for path, subdirs, files in os.walk(dir):\n",
    "        for index, name in enumerate(files):\n",
    "            if fnmatch(name, ext):\n",
    "\n",
    "                    \n",
    "                    # d[index] = os.path.join(path, name)\n",
    "                d[index] = int(name.split(\"-\")[-4].split(\"\\\\\")[-1])\n",
    "                # l.append(os.path.join(path, name))\n",
    "    return d\n",
    "\n",
    "l = files_in_dir_dict(cik_parquet_till_2013)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a value and if doesn't exist assigning a default one\n",
    "full_df['transaction_value'] = full_df.get('transaction_value',default=np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .merge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-15T15:44:23.642128Z",
     "iopub.status.busy": "2022-09-15T15:44:23.642128Z",
     "iopub.status.idle": "2022-09-15T15:44:23.652884Z",
     "shell.execute_reply": "2022-09-15T15:44:23.651912Z",
     "shell.execute_reply.started": "2022-09-15T15:44:23.642128Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df1 = pd.DataFrame({'name': ['Fred', 'Suzy', 'Suzy', 'Bob'],\n",
    "                    'pet': ['Dog', 'Dog', 'Cat', 'Fish']})\n",
    "\n",
    "df2 = pd.DataFrame({'Name': ['Suzy', 'Suzy', 'Suzy', 'Fred', 'Joe', 'Joe'],\n",
    "                    'Color': ['Black', 'Blue', 'Red', 'Green', 'Yellow', 'Blue']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-15T15:44:25.150216Z",
     "iopub.status.busy": "2022-09-15T15:44:25.149217Z",
     "iopub.status.idle": "2022-09-15T15:44:25.173901Z",
     "shell.execute_reply": "2022-09-15T15:44:25.172853Z",
     "shell.execute_reply.started": "2022-09-15T15:44:25.150216Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>pet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fred</td>\n",
       "      <td>Dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Suzy</td>\n",
       "      <td>Dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Suzy</td>\n",
       "      <td>Cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bob</td>\n",
       "      <td>Fish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name   pet\n",
       "0  Fred   Dog\n",
       "1  Suzy   Dog\n",
       "2  Suzy   Cat\n",
       "3   Bob  Fish"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Suzy</td>\n",
       "      <td>Black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Suzy</td>\n",
       "      <td>Blue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Suzy</td>\n",
       "      <td>Red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fred</td>\n",
       "      <td>Green</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Joe</td>\n",
       "      <td>Yellow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Joe</td>\n",
       "      <td>Blue</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Name   Color\n",
       "0  Suzy   Black\n",
       "1  Suzy    Blue\n",
       "2  Suzy     Red\n",
       "3  Fred   Green\n",
       "4   Joe  Yellow\n",
       "5   Joe    Blue"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(df1), display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-15T15:51:13.116182Z",
     "iopub.status.busy": "2022-09-15T15:51:13.115182Z",
     "iopub.status.idle": "2022-09-15T15:51:13.136211Z",
     "shell.execute_reply": "2022-09-15T15:51:13.136211Z",
     "shell.execute_reply.started": "2022-09-15T15:51:13.116182Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>pet</th>\n",
       "      <th>Name</th>\n",
       "      <th>Color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fred</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Fred</td>\n",
       "      <td>Green</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Suzy</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Suzy</td>\n",
       "      <td>Black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Suzy</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Suzy</td>\n",
       "      <td>Blue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Suzy</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Suzy</td>\n",
       "      <td>Red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Suzy</td>\n",
       "      <td>Cat</td>\n",
       "      <td>Suzy</td>\n",
       "      <td>Black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Suzy</td>\n",
       "      <td>Cat</td>\n",
       "      <td>Suzy</td>\n",
       "      <td>Blue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Suzy</td>\n",
       "      <td>Cat</td>\n",
       "      <td>Suzy</td>\n",
       "      <td>Red</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name  pet  Name  Color\n",
       "0  Fred  Dog  Fred  Green\n",
       "1  Suzy  Dog  Suzy  Black\n",
       "2  Suzy  Dog  Suzy   Blue\n",
       "3  Suzy  Dog  Suzy    Red\n",
       "4  Suzy  Cat  Suzy  Black\n",
       "5  Suzy  Cat  Suzy   Blue\n",
       "6  Suzy  Cat  Suzy    Red"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# be careful with names. Only because there was a white space after the column `name `, the code below didn't want to work\n",
    "# (df1.merge(df2.assign(name=df2.Name) )) # by default it's an inner join\n",
    "\n",
    "df1.merge(df2, left_on='name', right_on='Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T21:42:54.541316Z",
     "iopub.status.busy": "2022-09-14T21:42:54.540349Z",
     "iopub.status.idle": "2022-09-14T21:42:54.555318Z",
     "shell.execute_reply": "2022-09-14T21:42:54.554344Z",
     "shell.execute_reply.started": "2022-09-14T21:42:54.541316Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>pet</th>\n",
       "      <th>Name</th>\n",
       "      <th>Color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fred</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Fred</td>\n",
       "      <td>Green</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Suzy</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Suzy</td>\n",
       "      <td>Black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Suzy</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Suzy</td>\n",
       "      <td>Blue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Suzy</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Suzy</td>\n",
       "      <td>Red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Suzy</td>\n",
       "      <td>Cat</td>\n",
       "      <td>Suzy</td>\n",
       "      <td>Black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Suzy</td>\n",
       "      <td>Cat</td>\n",
       "      <td>Suzy</td>\n",
       "      <td>Blue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Suzy</td>\n",
       "      <td>Cat</td>\n",
       "      <td>Suzy</td>\n",
       "      <td>Red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bob</td>\n",
       "      <td>Fish</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name   pet  Name  Color\n",
       "0  Fred   Dog  Fred  Green\n",
       "1  Suzy   Dog  Suzy  Black\n",
       "2  Suzy   Dog  Suzy   Blue\n",
       "3  Suzy   Dog  Suzy    Red\n",
       "4  Suzy   Cat  Suzy  Black\n",
       "5  Suzy   Cat  Suzy   Blue\n",
       "6  Suzy   Cat  Suzy    Red\n",
       "7   Bob  Fish   NaN    NaN"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge and create new column `name` in df2 so we can merge on it with the df1 that has this column too. \n",
    "(df1.merge(df2.assign(name=df2.Name), how='left')) # all value from df1 will be brought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-15T16:37:47.820076Z",
     "iopub.status.busy": "2022-09-15T16:37:47.819105Z",
     "iopub.status.idle": "2022-09-15T16:37:48.398216Z",
     "shell.execute_reply": "2022-09-15T16:37:48.397244Z",
     "shell.execute_reply.started": "2022-09-15T16:37:47.820076Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accessionNumber</th>\n",
       "      <th>cikManager</th>\n",
       "      <th>managerName</th>\n",
       "      <th>periodOfReport</th>\n",
       "      <th>submissionType</th>\n",
       "      <th>filedAsOfDate</th>\n",
       "      <th>cusip</th>\n",
       "      <th>nameOfIssuer</th>\n",
       "      <th>titleOfClass</th>\n",
       "      <th>value</th>\n",
       "      <th>shares</th>\n",
       "      <th>shrsOrPrnAmt</th>\n",
       "      <th>putCall</th>\n",
       "      <th>xml_flag</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>quarter</th>\n",
       "      <th>tr_type</th>\n",
       "      <th>tr_shares</th>\n",
       "      <th>tr_value</th>\n",
       "      <th>quarter_as_int</th>\n",
       "      <th>prev_quarter_diff</th>\n",
       "      <th>next_quarter_diff</th>\n",
       "      <th>quarter_max_filed</th>\n",
       "      <th>prev_shares_diff</th>\n",
       "      <th>prev_value_diff</th>\n",
       "      <th>next_shares_diff</th>\n",
       "      <th>next_value_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000009015-13-000038</td>\n",
       "      <td>9015</td>\n",
       "      <td>nan</td>\n",
       "      <td>2013-09-30</td>\n",
       "      <td>13F-HR</td>\n",
       "      <td>2013-11-12</td>\n",
       "      <td>Y8565J101</td>\n",
       "      <td>COM</td>\n",
       "      <td>COM</td>\n",
       "      <td>507000.0</td>\n",
       "      <td>15190.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>xml</td>\n",
       "      <td>2022-08-19 13:34:36.331342</td>\n",
       "      <td>2013Q3</td>\n",
       "      <td>OPEN</td>\n",
       "      <td>15190.0</td>\n",
       "      <td>507000.0</td>\n",
       "      <td>174</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013Q3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000009015-10-000020</td>\n",
       "      <td>9015</td>\n",
       "      <td>nan</td>\n",
       "      <td>2010-03-31</td>\n",
       "      <td>13F-HR</td>\n",
       "      <td>2010-05-27</td>\n",
       "      <td>Y8564W103</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>fwf</td>\n",
       "      <td>2022-08-19 13:34:36.331342</td>\n",
       "      <td>2010Q2</td>\n",
       "      <td>CLOSE</td>\n",
       "      <td>-25300.0</td>\n",
       "      <td>-576000.0</td>\n",
       "      <td>160</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013Q3</td>\n",
       "      <td>15400.0</td>\n",
       "      <td>359000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000009015-10-000020</td>\n",
       "      <td>9015</td>\n",
       "      <td>nan</td>\n",
       "      <td>2010-03-31</td>\n",
       "      <td>13F-HR</td>\n",
       "      <td>2010-05-27</td>\n",
       "      <td>Y8564W103</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>576000.0</td>\n",
       "      <td>25300.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>fwf</td>\n",
       "      <td>2022-08-19 13:34:36.331342</td>\n",
       "      <td>2010Q1</td>\n",
       "      <td>OPEN</td>\n",
       "      <td>25300.0</td>\n",
       "      <td>576000.0</td>\n",
       "      <td>160</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013Q3</td>\n",
       "      <td>15400.0</td>\n",
       "      <td>359000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0000009015-09-000225</td>\n",
       "      <td>9015</td>\n",
       "      <td>nan</td>\n",
       "      <td>2009-09-30</td>\n",
       "      <td>13F-HR</td>\n",
       "      <td>2009-11-04</td>\n",
       "      <td>Y8564W103</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>217000.0</td>\n",
       "      <td>9900.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>fwf</td>\n",
       "      <td>2022-08-19 13:34:36.331342</td>\n",
       "      <td>2009Q3</td>\n",
       "      <td>OPEN</td>\n",
       "      <td>9900.0</td>\n",
       "      <td>217000.0</td>\n",
       "      <td>158</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>2013Q3</td>\n",
       "      <td>-1526.0</td>\n",
       "      <td>55000.0</td>\n",
       "      <td>-15400.0</td>\n",
       "      <td>-359000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000009015-09-000117</td>\n",
       "      <td>9015</td>\n",
       "      <td>nan</td>\n",
       "      <td>2009-03-31</td>\n",
       "      <td>13F-HR</td>\n",
       "      <td>2009-05-13</td>\n",
       "      <td>Y8564W103</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>162000.0</td>\n",
       "      <td>11426.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>fwf</td>\n",
       "      <td>2022-08-19 13:34:36.331342</td>\n",
       "      <td>2009Q1</td>\n",
       "      <td>OPEN</td>\n",
       "      <td>11426.0</td>\n",
       "      <td>162000.0</td>\n",
       "      <td>156</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>2013Q3</td>\n",
       "      <td>-135424.0</td>\n",
       "      <td>-6022000.0</td>\n",
       "      <td>1526.0</td>\n",
       "      <td>-55000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0000009015-05-000055</td>\n",
       "      <td>9015</td>\n",
       "      <td>nan</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>13F-HR/A</td>\n",
       "      <td>2005-02-24</td>\n",
       "      <td>Y8564W103</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>fwf</td>\n",
       "      <td>2022-08-19 13:34:36.331342</td>\n",
       "      <td>2005Q1</td>\n",
       "      <td>CLOSE</td>\n",
       "      <td>-146850.0</td>\n",
       "      <td>-6184000.0</td>\n",
       "      <td>139</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013Q3</td>\n",
       "      <td>-49505.0</td>\n",
       "      <td>-2277000.0</td>\n",
       "      <td>135424.0</td>\n",
       "      <td>6022000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0000009015-05-000055</td>\n",
       "      <td>9015</td>\n",
       "      <td>nan</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>13F-HR/A</td>\n",
       "      <td>2005-02-24</td>\n",
       "      <td>Y8564W103</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>6184000.0</td>\n",
       "      <td>146850.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>fwf</td>\n",
       "      <td>2022-08-19 13:34:36.331342</td>\n",
       "      <td>2004Q4</td>\n",
       "      <td>SELL</td>\n",
       "      <td>-49505.0</td>\n",
       "      <td>-49505.0</td>\n",
       "      <td>139</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-17.0</td>\n",
       "      <td>2013Q3</td>\n",
       "      <td>-49505.0</td>\n",
       "      <td>-2277000.0</td>\n",
       "      <td>135424.0</td>\n",
       "      <td>6022000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0000009015-04-000074</td>\n",
       "      <td>9015</td>\n",
       "      <td>nan</td>\n",
       "      <td>2004-09-30</td>\n",
       "      <td>13F-HR/A</td>\n",
       "      <td>2004-11-15</td>\n",
       "      <td>Y8564W103</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>8461000.0</td>\n",
       "      <td>196355.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>fwf</td>\n",
       "      <td>2022-08-19 13:34:36.331342</td>\n",
       "      <td>2004Q3</td>\n",
       "      <td>SELL</td>\n",
       "      <td>-7770.0</td>\n",
       "      <td>-7770.0</td>\n",
       "      <td>138</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2013Q3</td>\n",
       "      <td>-7770.0</td>\n",
       "      <td>831000.0</td>\n",
       "      <td>49505.0</td>\n",
       "      <td>2277000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0000009015-04-000064</td>\n",
       "      <td>9015</td>\n",
       "      <td>nan</td>\n",
       "      <td>2004-06-30</td>\n",
       "      <td>13F-HR</td>\n",
       "      <td>2004-08-11</td>\n",
       "      <td>Y8564W103</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>7630000.0</td>\n",
       "      <td>204125.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>fwf</td>\n",
       "      <td>2022-08-19 13:34:36.331342</td>\n",
       "      <td>2004Q2</td>\n",
       "      <td>BUY</td>\n",
       "      <td>104175.0</td>\n",
       "      <td>104175.0</td>\n",
       "      <td>137</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2013Q3</td>\n",
       "      <td>104175.0</td>\n",
       "      <td>743000.0</td>\n",
       "      <td>7770.0</td>\n",
       "      <td>-831000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0000009015-04-000058</td>\n",
       "      <td>9015</td>\n",
       "      <td>nan</td>\n",
       "      <td>2004-03-31</td>\n",
       "      <td>13F-HR</td>\n",
       "      <td>2004-05-14</td>\n",
       "      <td>Y8564W103</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>6887000.0</td>\n",
       "      <td>99950.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>fwf</td>\n",
       "      <td>2022-08-19 13:34:36.331342</td>\n",
       "      <td>2004Q1</td>\n",
       "      <td>SELL</td>\n",
       "      <td>-8250.0</td>\n",
       "      <td>-8250.0</td>\n",
       "      <td>136</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2013Q3</td>\n",
       "      <td>-8250.0</td>\n",
       "      <td>716000.0</td>\n",
       "      <td>-104175.0</td>\n",
       "      <td>-743000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0000009015-04-000053</td>\n",
       "      <td>9015</td>\n",
       "      <td>nan</td>\n",
       "      <td>2003-12-31</td>\n",
       "      <td>13F-HR</td>\n",
       "      <td>2004-02-13</td>\n",
       "      <td>Y8564W103</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>6171000.0</td>\n",
       "      <td>108200.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>fwf</td>\n",
       "      <td>2022-08-19 13:34:36.331342</td>\n",
       "      <td>2003Q4</td>\n",
       "      <td>OPEN</td>\n",
       "      <td>108200.0</td>\n",
       "      <td>6171000.0</td>\n",
       "      <td>135</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2013Q3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8250.0</td>\n",
       "      <td>-716000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0000009015-05-000072</td>\n",
       "      <td>9015</td>\n",
       "      <td>nan</td>\n",
       "      <td>2005-09-30</td>\n",
       "      <td>13F-HR</td>\n",
       "      <td>2005-11-15</td>\n",
       "      <td>Y6476W104</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>fwf</td>\n",
       "      <td>2022-08-19 13:34:36.331342</td>\n",
       "      <td>2005Q4</td>\n",
       "      <td>CLOSE</td>\n",
       "      <td>-58.0</td>\n",
       "      <td>-1000.0</td>\n",
       "      <td>142</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013Q3</td>\n",
       "      <td>-522482.0</td>\n",
       "      <td>-8370000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0000009015-05-000072</td>\n",
       "      <td>9015</td>\n",
       "      <td>nan</td>\n",
       "      <td>2005-09-30</td>\n",
       "      <td>13F-HR</td>\n",
       "      <td>2005-11-15</td>\n",
       "      <td>Y6476W104</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>fwf</td>\n",
       "      <td>2022-08-19 13:34:36.331342</td>\n",
       "      <td>2005Q3</td>\n",
       "      <td>OPEN</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>142</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013Q3</td>\n",
       "      <td>-522482.0</td>\n",
       "      <td>-8370000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0000009015-04-000074</td>\n",
       "      <td>9015</td>\n",
       "      <td>nan</td>\n",
       "      <td>2004-09-30</td>\n",
       "      <td>13F-HR/A</td>\n",
       "      <td>2004-11-15</td>\n",
       "      <td>Y6476W104</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>8371000.0</td>\n",
       "      <td>522540.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>fwf</td>\n",
       "      <td>2022-08-19 13:34:36.331342</td>\n",
       "      <td>2004Q3</td>\n",
       "      <td>OPEN</td>\n",
       "      <td>522540.0</td>\n",
       "      <td>8371000.0</td>\n",
       "      <td>138</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>2013Q3</td>\n",
       "      <td>-47060.0</td>\n",
       "      <td>3284000.0</td>\n",
       "      <td>522482.0</td>\n",
       "      <td>8370000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0000009015-04-000053</td>\n",
       "      <td>9015</td>\n",
       "      <td>nan</td>\n",
       "      <td>2003-12-31</td>\n",
       "      <td>13F-HR</td>\n",
       "      <td>2004-02-13</td>\n",
       "      <td>Y6476W104</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>fwf</td>\n",
       "      <td>2022-08-19 13:34:36.331342</td>\n",
       "      <td>2004Q1</td>\n",
       "      <td>CLOSE</td>\n",
       "      <td>-569600.0</td>\n",
       "      <td>-5087000.0</td>\n",
       "      <td>135</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013Q3</td>\n",
       "      <td>221450.0</td>\n",
       "      <td>2782000.0</td>\n",
       "      <td>47060.0</td>\n",
       "      <td>-3284000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0000009015-04-000053</td>\n",
       "      <td>9015</td>\n",
       "      <td>nan</td>\n",
       "      <td>2003-12-31</td>\n",
       "      <td>13F-HR</td>\n",
       "      <td>2004-02-13</td>\n",
       "      <td>Y6476W104</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>5087000.0</td>\n",
       "      <td>569600.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>fwf</td>\n",
       "      <td>2022-08-19 13:34:36.331342</td>\n",
       "      <td>2003Q4</td>\n",
       "      <td>BUY</td>\n",
       "      <td>221450.0</td>\n",
       "      <td>221450.0</td>\n",
       "      <td>135</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>2013Q3</td>\n",
       "      <td>221450.0</td>\n",
       "      <td>2782000.0</td>\n",
       "      <td>47060.0</td>\n",
       "      <td>-3284000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0000009015-03-000087</td>\n",
       "      <td>9015</td>\n",
       "      <td>nan</td>\n",
       "      <td>2003-09-30</td>\n",
       "      <td>13F-HR</td>\n",
       "      <td>2003-11-13</td>\n",
       "      <td>Y6476W104</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2305000.0</td>\n",
       "      <td>348150.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>fwf</td>\n",
       "      <td>2022-08-19 13:34:36.331342</td>\n",
       "      <td>2003Q3</td>\n",
       "      <td>SELL</td>\n",
       "      <td>-271450.0</td>\n",
       "      <td>-271450.0</td>\n",
       "      <td>134</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2013Q3</td>\n",
       "      <td>-271450.0</td>\n",
       "      <td>-1512000.0</td>\n",
       "      <td>-221450.0</td>\n",
       "      <td>-2782000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0000009015-03-000084</td>\n",
       "      <td>9015</td>\n",
       "      <td>nan</td>\n",
       "      <td>2003-06-30</td>\n",
       "      <td>13F-HR/A</td>\n",
       "      <td>2003-08-26</td>\n",
       "      <td>Y6476W104</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3817000.0</td>\n",
       "      <td>619600.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>fwf</td>\n",
       "      <td>2022-08-19 13:34:36.331342</td>\n",
       "      <td>2003Q2</td>\n",
       "      <td>BUY</td>\n",
       "      <td>73100.0</td>\n",
       "      <td>73100.0</td>\n",
       "      <td>133</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2013Q3</td>\n",
       "      <td>73100.0</td>\n",
       "      <td>1248000.0</td>\n",
       "      <td>271450.0</td>\n",
       "      <td>1512000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0000009015-03-000072</td>\n",
       "      <td>9015</td>\n",
       "      <td>nan</td>\n",
       "      <td>2003-03-31</td>\n",
       "      <td>13F-HR</td>\n",
       "      <td>2003-05-15</td>\n",
       "      <td>Y6476W104</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2569000.0</td>\n",
       "      <td>546500.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>fwf</td>\n",
       "      <td>2022-08-19 13:34:36.331342</td>\n",
       "      <td>2003Q1</td>\n",
       "      <td>BUY</td>\n",
       "      <td>99500.0</td>\n",
       "      <td>99500.0</td>\n",
       "      <td>132</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2013Q3</td>\n",
       "      <td>99500.0</td>\n",
       "      <td>732000.0</td>\n",
       "      <td>-73100.0</td>\n",
       "      <td>-1248000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0000009015-03-000056</td>\n",
       "      <td>9015</td>\n",
       "      <td>nan</td>\n",
       "      <td>2002-12-31</td>\n",
       "      <td>13F-HR</td>\n",
       "      <td>2003-02-12</td>\n",
       "      <td>Y6476W104</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1837000.0</td>\n",
       "      <td>447000.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>fwf</td>\n",
       "      <td>2022-08-19 13:34:36.331342</td>\n",
       "      <td>2002Q4</td>\n",
       "      <td>OPEN</td>\n",
       "      <td>447000.0</td>\n",
       "      <td>1837000.0</td>\n",
       "      <td>131</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2013Q3</td>\n",
       "      <td>161000.0</td>\n",
       "      <td>627000.0</td>\n",
       "      <td>-99500.0</td>\n",
       "      <td>-732000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0000009015-01-500056</td>\n",
       "      <td>9015</td>\n",
       "      <td>nan</td>\n",
       "      <td>2001-09-30</td>\n",
       "      <td>13F-HR</td>\n",
       "      <td>2001-11-13</td>\n",
       "      <td>Y6476W104</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>fwf</td>\n",
       "      <td>2022-08-19 13:34:36.331342</td>\n",
       "      <td>2001Q4</td>\n",
       "      <td>CLOSE</td>\n",
       "      <td>-286000.0</td>\n",
       "      <td>-1210000.0</td>\n",
       "      <td>126</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013Q3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-161000.0</td>\n",
       "      <td>-627000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0000009015-01-500056</td>\n",
       "      <td>9015</td>\n",
       "      <td>nan</td>\n",
       "      <td>2001-09-30</td>\n",
       "      <td>13F-HR</td>\n",
       "      <td>2001-11-13</td>\n",
       "      <td>Y6476W104</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1210000.0</td>\n",
       "      <td>286000.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>fwf</td>\n",
       "      <td>2022-08-19 13:34:36.331342</td>\n",
       "      <td>2001Q3</td>\n",
       "      <td>OPEN</td>\n",
       "      <td>286000.0</td>\n",
       "      <td>1210000.0</td>\n",
       "      <td>126</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>2013Q3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-161000.0</td>\n",
       "      <td>-627000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0000009015-13-000038</td>\n",
       "      <td>9015</td>\n",
       "      <td>nan</td>\n",
       "      <td>2013-09-30</td>\n",
       "      <td>13F-HR</td>\n",
       "      <td>2013-11-12</td>\n",
       "      <td>Y64354205</td>\n",
       "      <td>COM</td>\n",
       "      <td>COM</td>\n",
       "      <td>9220000.0</td>\n",
       "      <td>506585.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>xml</td>\n",
       "      <td>2022-08-19 13:34:36.331342</td>\n",
       "      <td>2013Q3</td>\n",
       "      <td>BUY</td>\n",
       "      <td>36185.0</td>\n",
       "      <td>36185.0</td>\n",
       "      <td>174</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013Q3</td>\n",
       "      <td>36185.0</td>\n",
       "      <td>409000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0000009015-13-000036</td>\n",
       "      <td>9015</td>\n",
       "      <td>nan</td>\n",
       "      <td>2013-06-30</td>\n",
       "      <td>13F-HR</td>\n",
       "      <td>2013-08-06</td>\n",
       "      <td>Y64354205</td>\n",
       "      <td>COM</td>\n",
       "      <td>COM</td>\n",
       "      <td>8811000.0</td>\n",
       "      <td>470400.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>xml</td>\n",
       "      <td>2022-08-19 13:34:36.331342</td>\n",
       "      <td>2013Q2</td>\n",
       "      <td>BUY</td>\n",
       "      <td>93123.0</td>\n",
       "      <td>93123.0</td>\n",
       "      <td>173</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2013Q3</td>\n",
       "      <td>93123.0</td>\n",
       "      <td>2734000.0</td>\n",
       "      <td>-36185.0</td>\n",
       "      <td>-409000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0000009015-13-000007</td>\n",
       "      <td>9015</td>\n",
       "      <td>nan</td>\n",
       "      <td>2013-03-31</td>\n",
       "      <td>13F-HR</td>\n",
       "      <td>2013-04-22</td>\n",
       "      <td>Y64354205</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>6077000.0</td>\n",
       "      <td>377277.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>fwf</td>\n",
       "      <td>2022-08-19 13:34:36.331342</td>\n",
       "      <td>2013Q1</td>\n",
       "      <td>OPEN</td>\n",
       "      <td>377277.0</td>\n",
       "      <td>6077000.0</td>\n",
       "      <td>172</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2013Q3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-93123.0</td>\n",
       "      <td>-2734000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0000009015-13-000038</td>\n",
       "      <td>9015</td>\n",
       "      <td>nan</td>\n",
       "      <td>2013-09-30</td>\n",
       "      <td>13F-HR</td>\n",
       "      <td>2013-11-12</td>\n",
       "      <td>Y62267102</td>\n",
       "      <td>COM</td>\n",
       "      <td>COM</td>\n",
       "      <td>367000.0</td>\n",
       "      <td>25045.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>xml</td>\n",
       "      <td>2022-08-19 13:34:36.331342</td>\n",
       "      <td>2013Q3</td>\n",
       "      <td>OPEN</td>\n",
       "      <td>25045.0</td>\n",
       "      <td>367000.0</td>\n",
       "      <td>174</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013Q3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0000009015-13-000038</td>\n",
       "      <td>9015</td>\n",
       "      <td>nan</td>\n",
       "      <td>2013-09-30</td>\n",
       "      <td>13F-HR</td>\n",
       "      <td>2013-11-12</td>\n",
       "      <td>Y2745C102</td>\n",
       "      <td>COM</td>\n",
       "      <td>COM</td>\n",
       "      <td>332000.0</td>\n",
       "      <td>10230.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>xml</td>\n",
       "      <td>2022-08-19 13:34:36.331342</td>\n",
       "      <td>2013Q3</td>\n",
       "      <td>OPEN</td>\n",
       "      <td>10230.0</td>\n",
       "      <td>332000.0</td>\n",
       "      <td>174</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013Q3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0000009015-05-000060</td>\n",
       "      <td>9015</td>\n",
       "      <td>nan</td>\n",
       "      <td>2005-03-31</td>\n",
       "      <td>13F-HR</td>\n",
       "      <td>2005-05-16</td>\n",
       "      <td>Y2573F102</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>fwf</td>\n",
       "      <td>2022-08-19 13:34:36.331342</td>\n",
       "      <td>2005Q2</td>\n",
       "      <td>CLOSE</td>\n",
       "      <td>-13000.0</td>\n",
       "      <td>-157000.0</td>\n",
       "      <td>140</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013Q3</td>\n",
       "      <td>-3200.0</td>\n",
       "      <td>-67000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0000009015-05-000060</td>\n",
       "      <td>9015</td>\n",
       "      <td>nan</td>\n",
       "      <td>2005-03-31</td>\n",
       "      <td>13F-HR</td>\n",
       "      <td>2005-05-16</td>\n",
       "      <td>Y2573F102</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>157000.0</td>\n",
       "      <td>13000.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>fwf</td>\n",
       "      <td>2022-08-19 13:34:36.331342</td>\n",
       "      <td>2005Q1</td>\n",
       "      <td>SELL</td>\n",
       "      <td>-3200.0</td>\n",
       "      <td>-3200.0</td>\n",
       "      <td>140</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013Q3</td>\n",
       "      <td>-3200.0</td>\n",
       "      <td>-67000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0000009015-05-000055</td>\n",
       "      <td>9015</td>\n",
       "      <td>nan</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>13F-HR/A</td>\n",
       "      <td>2005-02-24</td>\n",
       "      <td>Y2573F102</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>224000.0</td>\n",
       "      <td>16200.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>fwf</td>\n",
       "      <td>2022-08-19 13:34:36.331342</td>\n",
       "      <td>2004Q4</td>\n",
       "      <td>BUY</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>139</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2013Q3</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>168000.0</td>\n",
       "      <td>3200.0</td>\n",
       "      <td>67000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0000009015-04-000074</td>\n",
       "      <td>9015</td>\n",
       "      <td>nan</td>\n",
       "      <td>2004-09-30</td>\n",
       "      <td>13F-HR/A</td>\n",
       "      <td>2004-11-15</td>\n",
       "      <td>Y2573F102</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>56000.0</td>\n",
       "      <td>4200.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>fwf</td>\n",
       "      <td>2022-08-19 13:34:36.331342</td>\n",
       "      <td>2004Q3</td>\n",
       "      <td>OPEN</td>\n",
       "      <td>4200.0</td>\n",
       "      <td>56000.0</td>\n",
       "      <td>138</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2013Q3</td>\n",
       "      <td>-440050.0</td>\n",
       "      <td>-3582000.0</td>\n",
       "      <td>-12000.0</td>\n",
       "      <td>-168000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0000009015-03-000056</td>\n",
       "      <td>9015</td>\n",
       "      <td>nan</td>\n",
       "      <td>2002-12-31</td>\n",
       "      <td>13F-HR</td>\n",
       "      <td>2003-02-12</td>\n",
       "      <td>Y2573F102</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>fwf</td>\n",
       "      <td>2022-08-19 13:34:36.331342</td>\n",
       "      <td>2003Q1</td>\n",
       "      <td>CLOSE</td>\n",
       "      <td>-444250.0</td>\n",
       "      <td>-3638000.0</td>\n",
       "      <td>131</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013Q3</td>\n",
       "      <td>-39050.0</td>\n",
       "      <td>269000.0</td>\n",
       "      <td>440050.0</td>\n",
       "      <td>3582000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0000009015-03-000056</td>\n",
       "      <td>9015</td>\n",
       "      <td>nan</td>\n",
       "      <td>2002-12-31</td>\n",
       "      <td>13F-HR</td>\n",
       "      <td>2003-02-12</td>\n",
       "      <td>Y2573F102</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3638000.0</td>\n",
       "      <td>444250.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>fwf</td>\n",
       "      <td>2022-08-19 13:34:36.331342</td>\n",
       "      <td>2002Q4</td>\n",
       "      <td>SELL</td>\n",
       "      <td>-39050.0</td>\n",
       "      <td>-39050.0</td>\n",
       "      <td>131</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>2013Q3</td>\n",
       "      <td>-39050.0</td>\n",
       "      <td>269000.0</td>\n",
       "      <td>440050.0</td>\n",
       "      <td>3582000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0000009015-02-000117</td>\n",
       "      <td>9015</td>\n",
       "      <td>nan</td>\n",
       "      <td>2002-09-30</td>\n",
       "      <td>13F-HR</td>\n",
       "      <td>2002-11-13</td>\n",
       "      <td>Y2573F102</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3369000.0</td>\n",
       "      <td>483300.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>fwf</td>\n",
       "      <td>2022-08-19 13:34:36.331342</td>\n",
       "      <td>2002Q3</td>\n",
       "      <td>SELL</td>\n",
       "      <td>-128638.0</td>\n",
       "      <td>-128638.0</td>\n",
       "      <td>130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2013Q3</td>\n",
       "      <td>-128638.0</td>\n",
       "      <td>-994000.0</td>\n",
       "      <td>39050.0</td>\n",
       "      <td>-269000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0000009015-02-000109</td>\n",
       "      <td>9015</td>\n",
       "      <td>nan</td>\n",
       "      <td>2002-06-30</td>\n",
       "      <td>13F-HR</td>\n",
       "      <td>2002-08-13</td>\n",
       "      <td>Y2573F102</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>4363000.0</td>\n",
       "      <td>611938.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>fwf</td>\n",
       "      <td>2022-08-19 13:34:36.331342</td>\n",
       "      <td>2002Q2</td>\n",
       "      <td>SELL</td>\n",
       "      <td>-32892.0</td>\n",
       "      <td>-32892.0</td>\n",
       "      <td>129</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2013Q3</td>\n",
       "      <td>-32892.0</td>\n",
       "      <td>-7405000.0</td>\n",
       "      <td>128638.0</td>\n",
       "      <td>994000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0000009015-02-000103</td>\n",
       "      <td>9015</td>\n",
       "      <td>nan</td>\n",
       "      <td>2002-03-31</td>\n",
       "      <td>13F-HR</td>\n",
       "      <td>2002-05-14</td>\n",
       "      <td>Y2573F102</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>11768000.0</td>\n",
       "      <td>644830.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>fwf</td>\n",
       "      <td>2022-08-19 13:34:36.331342</td>\n",
       "      <td>2002Q1</td>\n",
       "      <td>SELL</td>\n",
       "      <td>-90400.0</td>\n",
       "      <td>-90400.0</td>\n",
       "      <td>128</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2013Q3</td>\n",
       "      <td>-90400.0</td>\n",
       "      <td>-5870000.0</td>\n",
       "      <td>32892.0</td>\n",
       "      <td>7405000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0000009015-02-000076</td>\n",
       "      <td>9015</td>\n",
       "      <td>nan</td>\n",
       "      <td>2001-12-31</td>\n",
       "      <td>13F-HR</td>\n",
       "      <td>2002-02-13</td>\n",
       "      <td>Y2573F102</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>17638000.0</td>\n",
       "      <td>735230.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>fwf</td>\n",
       "      <td>2022-08-19 13:34:36.331342</td>\n",
       "      <td>2001Q4</td>\n",
       "      <td>SELL</td>\n",
       "      <td>-45740.0</td>\n",
       "      <td>-45740.0</td>\n",
       "      <td>127</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2013Q3</td>\n",
       "      <td>-45740.0</td>\n",
       "      <td>4721000.0</td>\n",
       "      <td>90400.0</td>\n",
       "      <td>5870000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0000009015-01-500056</td>\n",
       "      <td>9015</td>\n",
       "      <td>nan</td>\n",
       "      <td>2001-09-30</td>\n",
       "      <td>13F-HR</td>\n",
       "      <td>2001-11-13</td>\n",
       "      <td>Y2573F102</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>12917000.0</td>\n",
       "      <td>780970.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>fwf</td>\n",
       "      <td>2022-08-19 13:34:36.331342</td>\n",
       "      <td>2001Q3</td>\n",
       "      <td>SELL</td>\n",
       "      <td>-195050.0</td>\n",
       "      <td>-195050.0</td>\n",
       "      <td>126</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2013Q3</td>\n",
       "      <td>-195050.0</td>\n",
       "      <td>-12566000.0</td>\n",
       "      <td>45740.0</td>\n",
       "      <td>-4721000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0000009015-01-500029</td>\n",
       "      <td>9015</td>\n",
       "      <td>nan</td>\n",
       "      <td>2001-06-30</td>\n",
       "      <td>13F-HR</td>\n",
       "      <td>2001-08-15</td>\n",
       "      <td>Y2573F102</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>25483000.0</td>\n",
       "      <td>976020.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>fwf</td>\n",
       "      <td>2022-08-19 13:34:36.331342</td>\n",
       "      <td>2001Q2</td>\n",
       "      <td>BUY</td>\n",
       "      <td>74674.0</td>\n",
       "      <td>74674.0</td>\n",
       "      <td>125</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2013Q3</td>\n",
       "      <td>74674.0</td>\n",
       "      <td>1246000.0</td>\n",
       "      <td>195050.0</td>\n",
       "      <td>12566000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0000009015-01-500018</td>\n",
       "      <td>9015</td>\n",
       "      <td>nan</td>\n",
       "      <td>2001-03-31</td>\n",
       "      <td>13F-HR/A</td>\n",
       "      <td>2001-05-15</td>\n",
       "      <td>Y2573F102</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>24237000.0</td>\n",
       "      <td>901346.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>fwf</td>\n",
       "      <td>2022-08-19 13:34:36.331342</td>\n",
       "      <td>2001Q1</td>\n",
       "      <td>BUY</td>\n",
       "      <td>877256.0</td>\n",
       "      <td>877256.0</td>\n",
       "      <td>124</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2013Q3</td>\n",
       "      <td>877256.0</td>\n",
       "      <td>23550000.0</td>\n",
       "      <td>-74674.0</td>\n",
       "      <td>-1246000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         accessionNumber  cikManager managerName periodOfReport  \\\n",
       "0   0000009015-13-000038        9015         nan     2013-09-30   \n",
       "1   0000009015-10-000020        9015         nan     2010-03-31   \n",
       "2   0000009015-10-000020        9015         nan     2010-03-31   \n",
       "3   0000009015-09-000225        9015         nan     2009-09-30   \n",
       "4   0000009015-09-000117        9015         nan     2009-03-31   \n",
       "5   0000009015-05-000055        9015         nan     2004-12-31   \n",
       "6   0000009015-05-000055        9015         nan     2004-12-31   \n",
       "7   0000009015-04-000074        9015         nan     2004-09-30   \n",
       "8   0000009015-04-000064        9015         nan     2004-06-30   \n",
       "9   0000009015-04-000058        9015         nan     2004-03-31   \n",
       "10  0000009015-04-000053        9015         nan     2003-12-31   \n",
       "11  0000009015-05-000072        9015         nan     2005-09-30   \n",
       "12  0000009015-05-000072        9015         nan     2005-09-30   \n",
       "13  0000009015-04-000074        9015         nan     2004-09-30   \n",
       "14  0000009015-04-000053        9015         nan     2003-12-31   \n",
       "15  0000009015-04-000053        9015         nan     2003-12-31   \n",
       "16  0000009015-03-000087        9015         nan     2003-09-30   \n",
       "17  0000009015-03-000084        9015         nan     2003-06-30   \n",
       "18  0000009015-03-000072        9015         nan     2003-03-31   \n",
       "19  0000009015-03-000056        9015         nan     2002-12-31   \n",
       "20  0000009015-01-500056        9015         nan     2001-09-30   \n",
       "21  0000009015-01-500056        9015         nan     2001-09-30   \n",
       "22  0000009015-13-000038        9015         nan     2013-09-30   \n",
       "23  0000009015-13-000036        9015         nan     2013-06-30   \n",
       "24  0000009015-13-000007        9015         nan     2013-03-31   \n",
       "25  0000009015-13-000038        9015         nan     2013-09-30   \n",
       "26  0000009015-13-000038        9015         nan     2013-09-30   \n",
       "27  0000009015-05-000060        9015         nan     2005-03-31   \n",
       "28  0000009015-05-000060        9015         nan     2005-03-31   \n",
       "29  0000009015-05-000055        9015         nan     2004-12-31   \n",
       "30  0000009015-04-000074        9015         nan     2004-09-30   \n",
       "31  0000009015-03-000056        9015         nan     2002-12-31   \n",
       "32  0000009015-03-000056        9015         nan     2002-12-31   \n",
       "33  0000009015-02-000117        9015         nan     2002-09-30   \n",
       "34  0000009015-02-000109        9015         nan     2002-06-30   \n",
       "35  0000009015-02-000103        9015         nan     2002-03-31   \n",
       "36  0000009015-02-000076        9015         nan     2001-12-31   \n",
       "37  0000009015-01-500056        9015         nan     2001-09-30   \n",
       "38  0000009015-01-500029        9015         nan     2001-06-30   \n",
       "39  0000009015-01-500018        9015         nan     2001-03-31   \n",
       "\n",
       "   submissionType filedAsOfDate      cusip nameOfIssuer titleOfClass  \\\n",
       "0          13F-HR    2013-11-12  Y8565J101          COM          COM   \n",
       "1          13F-HR    2010-05-27  Y8564W103         None         None   \n",
       "2          13F-HR    2010-05-27  Y8564W103         None         None   \n",
       "3          13F-HR    2009-11-04  Y8564W103         None         None   \n",
       "4          13F-HR    2009-05-13  Y8564W103         None         None   \n",
       "5        13F-HR/A    2005-02-24  Y8564W103         None         None   \n",
       "6        13F-HR/A    2005-02-24  Y8564W103         None         None   \n",
       "7        13F-HR/A    2004-11-15  Y8564W103         None         None   \n",
       "8          13F-HR    2004-08-11  Y8564W103         None         None   \n",
       "9          13F-HR    2004-05-14  Y8564W103         None         None   \n",
       "10         13F-HR    2004-02-13  Y8564W103         None         None   \n",
       "11         13F-HR    2005-11-15  Y6476W104         None         None   \n",
       "12         13F-HR    2005-11-15  Y6476W104         None         None   \n",
       "13       13F-HR/A    2004-11-15  Y6476W104         None         None   \n",
       "14         13F-HR    2004-02-13  Y6476W104         None         None   \n",
       "15         13F-HR    2004-02-13  Y6476W104         None         None   \n",
       "16         13F-HR    2003-11-13  Y6476W104         None         None   \n",
       "17       13F-HR/A    2003-08-26  Y6476W104         None         None   \n",
       "18         13F-HR    2003-05-15  Y6476W104         None         None   \n",
       "19         13F-HR    2003-02-12  Y6476W104         None         None   \n",
       "20         13F-HR    2001-11-13  Y6476W104         None         None   \n",
       "21         13F-HR    2001-11-13  Y6476W104         None         None   \n",
       "22         13F-HR    2013-11-12  Y64354205          COM          COM   \n",
       "23         13F-HR    2013-08-06  Y64354205          COM          COM   \n",
       "24         13F-HR    2013-04-22  Y64354205         None         None   \n",
       "25         13F-HR    2013-11-12  Y62267102          COM          COM   \n",
       "26         13F-HR    2013-11-12  Y2745C102          COM          COM   \n",
       "27         13F-HR    2005-05-16  Y2573F102         None         None   \n",
       "28         13F-HR    2005-05-16  Y2573F102         None         None   \n",
       "29       13F-HR/A    2005-02-24  Y2573F102         None         None   \n",
       "30       13F-HR/A    2004-11-15  Y2573F102         None         None   \n",
       "31         13F-HR    2003-02-12  Y2573F102         None         None   \n",
       "32         13F-HR    2003-02-12  Y2573F102         None         None   \n",
       "33         13F-HR    2002-11-13  Y2573F102         None         None   \n",
       "34         13F-HR    2002-08-13  Y2573F102         None         None   \n",
       "35         13F-HR    2002-05-14  Y2573F102         None         None   \n",
       "36         13F-HR    2002-02-13  Y2573F102         None         None   \n",
       "37         13F-HR    2001-11-13  Y2573F102         None         None   \n",
       "38         13F-HR    2001-08-15  Y2573F102         None         None   \n",
       "39       13F-HR/A    2001-05-15  Y2573F102         None         None   \n",
       "\n",
       "         value    shares shrsOrPrnAmt putCall xml_flag  \\\n",
       "0     507000.0   15190.0           SH    None      xml   \n",
       "1          0.0       0.0           SH    None      fwf   \n",
       "2     576000.0   25300.0           SH    None      fwf   \n",
       "3     217000.0    9900.0           SH    None      fwf   \n",
       "4     162000.0   11426.0           SH    None      fwf   \n",
       "5          0.0       0.0           SH    None      fwf   \n",
       "6    6184000.0  146850.0           SH    None      fwf   \n",
       "7    8461000.0  196355.0           SH    None      fwf   \n",
       "8    7630000.0  204125.0           SH    None      fwf   \n",
       "9    6887000.0   99950.0           SH    None      fwf   \n",
       "10   6171000.0  108200.0           SH    None      fwf   \n",
       "11         0.0       0.0           SH    None      fwf   \n",
       "12      1000.0      58.0           SH    None      fwf   \n",
       "13   8371000.0  522540.0           SH    None      fwf   \n",
       "14         0.0       0.0           SH    None      fwf   \n",
       "15   5087000.0  569600.0           SH    None      fwf   \n",
       "16   2305000.0  348150.0           SH    None      fwf   \n",
       "17   3817000.0  619600.0           SH    None      fwf   \n",
       "18   2569000.0  546500.0           SH    None      fwf   \n",
       "19   1837000.0  447000.0           SH    None      fwf   \n",
       "20         0.0       0.0           SH    None      fwf   \n",
       "21   1210000.0  286000.0           SH    None      fwf   \n",
       "22   9220000.0  506585.0           SH    None      xml   \n",
       "23   8811000.0  470400.0           SH    None      xml   \n",
       "24   6077000.0  377277.0           SH    None      fwf   \n",
       "25    367000.0   25045.0           SH    None      xml   \n",
       "26    332000.0   10230.0           SH    None      xml   \n",
       "27         0.0       0.0           SH    None      fwf   \n",
       "28    157000.0   13000.0           SH    None      fwf   \n",
       "29    224000.0   16200.0           SH    None      fwf   \n",
       "30     56000.0    4200.0           SH    None      fwf   \n",
       "31         0.0       0.0           SH    None      fwf   \n",
       "32   3638000.0  444250.0           SH    None      fwf   \n",
       "33   3369000.0  483300.0           SH    None      fwf   \n",
       "34   4363000.0  611938.0           SH    None      fwf   \n",
       "35  11768000.0  644830.0           SH    None      fwf   \n",
       "36  17638000.0  735230.0           SH    None      fwf   \n",
       "37  12917000.0  780970.0           SH    None      fwf   \n",
       "38  25483000.0  976020.0           SH    None      fwf   \n",
       "39  24237000.0  901346.0           SH    None      fwf   \n",
       "\n",
       "                   updated_at quarter tr_type  tr_shares   tr_value  \\\n",
       "0  2022-08-19 13:34:36.331342  2013Q3    OPEN    15190.0   507000.0   \n",
       "1  2022-08-19 13:34:36.331342  2010Q2   CLOSE   -25300.0  -576000.0   \n",
       "2  2022-08-19 13:34:36.331342  2010Q1    OPEN    25300.0   576000.0   \n",
       "3  2022-08-19 13:34:36.331342  2009Q3    OPEN     9900.0   217000.0   \n",
       "4  2022-08-19 13:34:36.331342  2009Q1    OPEN    11426.0   162000.0   \n",
       "5  2022-08-19 13:34:36.331342  2005Q1   CLOSE  -146850.0 -6184000.0   \n",
       "6  2022-08-19 13:34:36.331342  2004Q4    SELL   -49505.0   -49505.0   \n",
       "7  2022-08-19 13:34:36.331342  2004Q3    SELL    -7770.0    -7770.0   \n",
       "8  2022-08-19 13:34:36.331342  2004Q2     BUY   104175.0   104175.0   \n",
       "9  2022-08-19 13:34:36.331342  2004Q1    SELL    -8250.0    -8250.0   \n",
       "10 2022-08-19 13:34:36.331342  2003Q4    OPEN   108200.0  6171000.0   \n",
       "11 2022-08-19 13:34:36.331342  2005Q4   CLOSE      -58.0    -1000.0   \n",
       "12 2022-08-19 13:34:36.331342  2005Q3    OPEN       58.0     1000.0   \n",
       "13 2022-08-19 13:34:36.331342  2004Q3    OPEN   522540.0  8371000.0   \n",
       "14 2022-08-19 13:34:36.331342  2004Q1   CLOSE  -569600.0 -5087000.0   \n",
       "15 2022-08-19 13:34:36.331342  2003Q4     BUY   221450.0   221450.0   \n",
       "16 2022-08-19 13:34:36.331342  2003Q3    SELL  -271450.0  -271450.0   \n",
       "17 2022-08-19 13:34:36.331342  2003Q2     BUY    73100.0    73100.0   \n",
       "18 2022-08-19 13:34:36.331342  2003Q1     BUY    99500.0    99500.0   \n",
       "19 2022-08-19 13:34:36.331342  2002Q4    OPEN   447000.0  1837000.0   \n",
       "20 2022-08-19 13:34:36.331342  2001Q4   CLOSE  -286000.0 -1210000.0   \n",
       "21 2022-08-19 13:34:36.331342  2001Q3    OPEN   286000.0  1210000.0   \n",
       "22 2022-08-19 13:34:36.331342  2013Q3     BUY    36185.0    36185.0   \n",
       "23 2022-08-19 13:34:36.331342  2013Q2     BUY    93123.0    93123.0   \n",
       "24 2022-08-19 13:34:36.331342  2013Q1    OPEN   377277.0  6077000.0   \n",
       "25 2022-08-19 13:34:36.331342  2013Q3    OPEN    25045.0   367000.0   \n",
       "26 2022-08-19 13:34:36.331342  2013Q3    OPEN    10230.0   332000.0   \n",
       "27 2022-08-19 13:34:36.331342  2005Q2   CLOSE   -13000.0  -157000.0   \n",
       "28 2022-08-19 13:34:36.331342  2005Q1    SELL    -3200.0    -3200.0   \n",
       "29 2022-08-19 13:34:36.331342  2004Q4     BUY    12000.0    12000.0   \n",
       "30 2022-08-19 13:34:36.331342  2004Q3    OPEN     4200.0    56000.0   \n",
       "31 2022-08-19 13:34:36.331342  2003Q1   CLOSE  -444250.0 -3638000.0   \n",
       "32 2022-08-19 13:34:36.331342  2002Q4    SELL   -39050.0   -39050.0   \n",
       "33 2022-08-19 13:34:36.331342  2002Q3    SELL  -128638.0  -128638.0   \n",
       "34 2022-08-19 13:34:36.331342  2002Q2    SELL   -32892.0   -32892.0   \n",
       "35 2022-08-19 13:34:36.331342  2002Q1    SELL   -90400.0   -90400.0   \n",
       "36 2022-08-19 13:34:36.331342  2001Q4    SELL   -45740.0   -45740.0   \n",
       "37 2022-08-19 13:34:36.331342  2001Q3    SELL  -195050.0  -195050.0   \n",
       "38 2022-08-19 13:34:36.331342  2001Q2     BUY    74674.0    74674.0   \n",
       "39 2022-08-19 13:34:36.331342  2001Q1     BUY   877256.0   877256.0   \n",
       "\n",
       "    quarter_as_int  prev_quarter_diff  next_quarter_diff quarter_max_filed  \\\n",
       "0              174                NaN                NaN            2013Q3   \n",
       "1              160                1.0                0.0            2013Q3   \n",
       "2              160                2.0                NaN            2013Q3   \n",
       "3              158                2.0               -2.0            2013Q3   \n",
       "4              156               17.0               -2.0            2013Q3   \n",
       "5              139                1.0                0.0            2013Q3   \n",
       "6              139                1.0              -17.0            2013Q3   \n",
       "7              138                1.0               -1.0            2013Q3   \n",
       "8              137                1.0               -1.0            2013Q3   \n",
       "9              136                1.0               -1.0            2013Q3   \n",
       "10             135                NaN               -1.0            2013Q3   \n",
       "11             142                1.0                0.0            2013Q3   \n",
       "12             142                4.0                NaN            2013Q3   \n",
       "13             138                3.0               -4.0            2013Q3   \n",
       "14             135                1.0                0.0            2013Q3   \n",
       "15             135                1.0               -3.0            2013Q3   \n",
       "16             134                1.0               -1.0            2013Q3   \n",
       "17             133                1.0               -1.0            2013Q3   \n",
       "18             132                1.0               -1.0            2013Q3   \n",
       "19             131                5.0               -1.0            2013Q3   \n",
       "20             126                1.0                0.0            2013Q3   \n",
       "21             126                NaN               -5.0            2013Q3   \n",
       "22             174                1.0                NaN            2013Q3   \n",
       "23             173                1.0               -1.0            2013Q3   \n",
       "24             172                NaN               -1.0            2013Q3   \n",
       "25             174                NaN                NaN            2013Q3   \n",
       "26             174                NaN                NaN            2013Q3   \n",
       "27             140                1.0                0.0            2013Q3   \n",
       "28             140                1.0                NaN            2013Q3   \n",
       "29             139                1.0               -1.0            2013Q3   \n",
       "30             138                7.0               -1.0            2013Q3   \n",
       "31             131                1.0                0.0            2013Q3   \n",
       "32             131                1.0               -7.0            2013Q3   \n",
       "33             130                1.0               -1.0            2013Q3   \n",
       "34             129                1.0               -1.0            2013Q3   \n",
       "35             128                1.0               -1.0            2013Q3   \n",
       "36             127                1.0               -1.0            2013Q3   \n",
       "37             126                1.0               -1.0            2013Q3   \n",
       "38             125                1.0               -1.0            2013Q3   \n",
       "39             124                1.0               -1.0            2013Q3   \n",
       "\n",
       "    prev_shares_diff  prev_value_diff  next_shares_diff  next_value_diff  \n",
       "0                NaN              NaN               NaN              NaN  \n",
       "1            15400.0         359000.0               NaN              NaN  \n",
       "2            15400.0         359000.0               NaN              NaN  \n",
       "3            -1526.0          55000.0          -15400.0        -359000.0  \n",
       "4          -135424.0       -6022000.0            1526.0         -55000.0  \n",
       "5           -49505.0       -2277000.0          135424.0        6022000.0  \n",
       "6           -49505.0       -2277000.0          135424.0        6022000.0  \n",
       "7            -7770.0         831000.0           49505.0        2277000.0  \n",
       "8           104175.0         743000.0            7770.0        -831000.0  \n",
       "9            -8250.0         716000.0         -104175.0        -743000.0  \n",
       "10               NaN              NaN            8250.0        -716000.0  \n",
       "11         -522482.0       -8370000.0               NaN              NaN  \n",
       "12         -522482.0       -8370000.0               NaN              NaN  \n",
       "13          -47060.0        3284000.0          522482.0        8370000.0  \n",
       "14          221450.0        2782000.0           47060.0       -3284000.0  \n",
       "15          221450.0        2782000.0           47060.0       -3284000.0  \n",
       "16         -271450.0       -1512000.0         -221450.0       -2782000.0  \n",
       "17           73100.0        1248000.0          271450.0        1512000.0  \n",
       "18           99500.0         732000.0          -73100.0       -1248000.0  \n",
       "19          161000.0         627000.0          -99500.0        -732000.0  \n",
       "20               NaN              NaN         -161000.0        -627000.0  \n",
       "21               NaN              NaN         -161000.0        -627000.0  \n",
       "22           36185.0         409000.0               NaN              NaN  \n",
       "23           93123.0        2734000.0          -36185.0        -409000.0  \n",
       "24               NaN              NaN          -93123.0       -2734000.0  \n",
       "25               NaN              NaN               NaN              NaN  \n",
       "26               NaN              NaN               NaN              NaN  \n",
       "27           -3200.0         -67000.0               NaN              NaN  \n",
       "28           -3200.0         -67000.0               NaN              NaN  \n",
       "29           12000.0         168000.0            3200.0          67000.0  \n",
       "30         -440050.0       -3582000.0          -12000.0        -168000.0  \n",
       "31          -39050.0         269000.0          440050.0        3582000.0  \n",
       "32          -39050.0         269000.0          440050.0        3582000.0  \n",
       "33         -128638.0        -994000.0           39050.0        -269000.0  \n",
       "34          -32892.0       -7405000.0          128638.0         994000.0  \n",
       "35          -90400.0       -5870000.0           32892.0        7405000.0  \n",
       "36          -45740.0        4721000.0           90400.0        5870000.0  \n",
       "37         -195050.0      -12566000.0           45740.0       -4721000.0  \n",
       "38           74674.0        1246000.0          195050.0       12566000.0  \n",
       "39          877256.0       23550000.0          -74674.0       -1246000.0  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', 35)\n",
    "pd.set_option(\"display.max_colwidth\",3000)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "# !pip install pyarrow fastparquet\n",
    "df3 = pd.read_parquet(r\"data/9015-2013-11-12.parquet\")\n",
    "df4 = pd.read_parquet(r\"data/cusip_md.parquet\")\n",
    "# df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-15T16:05:25.145834Z",
     "iopub.status.busy": "2022-09-15T16:05:25.145834Z",
     "iopub.status.idle": "2022-09-15T16:05:25.152412Z",
     "shell.execute_reply": "2022-09-15T16:05:25.152412Z",
     "shell.execute_reply.started": "2022-09-15T16:05:25.145834Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['cusip', 'nameOfIssuer', 'titleOfClass', 'value', 'shares',\n",
       "        'shrsOrPrnAmt', 'putCall', 'quarter'],\n",
       "       dtype='object'),\n",
       " Index(['quarter', 'cusip', 'ticker', 'cusip_description', 'nameofissuer',\n",
       "        'titleofclass'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df3 = df3.drop(columns=['accessionNumber', 'cikManager', 'managerName', 'periodOfReport',\n",
    "#         'submissionType', 'filedAsOfDate', 'xml_flag', 'updated_at', 'tr_type', 'tr_shares', 'tr_value',\n",
    "#         'quarter_as_int', 'prev_quarter_diff', 'next_quarter_diff',\n",
    "#         'quarter_max_filed', 'prev_shares_diff', 'prev_value_diff',\n",
    "#         'next_shares_diff', 'next_value_diff'])\n",
    "\n",
    "\n",
    "df3.columns, df4.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-15T16:23:13.944248Z",
     "iopub.status.busy": "2022-09-15T16:23:13.944248Z",
     "iopub.status.idle": "2022-09-15T16:23:14.490293Z",
     "shell.execute_reply": "2022-09-15T16:23:14.489282Z",
     "shell.execute_reply.started": "2022-09-15T16:23:13.944248Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cusip</th>\n",
       "      <th>nameOfIssuer</th>\n",
       "      <th>titleOfClass</th>\n",
       "      <th>value</th>\n",
       "      <th>shares</th>\n",
       "      <th>shrsOrPrnAmt</th>\n",
       "      <th>putCall</th>\n",
       "      <th>quarter</th>\n",
       "      <th>ticker</th>\n",
       "      <th>cusip_description</th>\n",
       "      <th>nameofissuer</th>\n",
       "      <th>titleofclass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Y8565J101</td>\n",
       "      <td>COM</td>\n",
       "      <td>COM</td>\n",
       "      <td>507000.0</td>\n",
       "      <td>15190.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>2013Q3</td>\n",
       "      <td>TOO</td>\n",
       "      <td>TEEKAY OFFSHORE PARTNERS LP CO</td>\n",
       "      <td>TEEKAY OFFSHORE PARTNERS L P</td>\n",
       "      <td>PARTNERSHIP UN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Y8564W103</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>2010Q2</td>\n",
       "      <td>TK</td>\n",
       "      <td>TEEKAY CORPORATION (MARSHALL I</td>\n",
       "      <td>TEEKAY CORPORATION</td>\n",
       "      <td>COM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Y8564W103</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>576000.0</td>\n",
       "      <td>25300.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>2010Q1</td>\n",
       "      <td>TK</td>\n",
       "      <td>TEEKAY CORPORATION (MARSHALL I</td>\n",
       "      <td>TEEKAY CORPORATION</td>\n",
       "      <td>COM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Y8564W103</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>217000.0</td>\n",
       "      <td>9900.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>2009Q3</td>\n",
       "      <td>TK</td>\n",
       "      <td>TEEKAY CORPORATION (MARSHALL I</td>\n",
       "      <td>TEEKAY CORPORATION</td>\n",
       "      <td>COM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Y8564W103</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>162000.0</td>\n",
       "      <td>11426.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>2009Q1</td>\n",
       "      <td>TK</td>\n",
       "      <td>TEEKAY CORPORATION (MARSHALL I</td>\n",
       "      <td>TEEKAY CORPORATION</td>\n",
       "      <td>COM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77015</th>\n",
       "      <td>000360206</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>4145000.0</td>\n",
       "      <td>245000.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>2002Q3</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AADN INC  $0004</td>\n",
       "      <td>COM PAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77016</th>\n",
       "      <td>000360206</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>4276000.0</td>\n",
       "      <td>227800.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>2002Q2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AAON INC  $0.004</td>\n",
       "      <td>COM PAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77017</th>\n",
       "      <td>000360206</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>2002Q1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AAON INC  $0.004</td>\n",
       "      <td>COM PAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77018</th>\n",
       "      <td>000360206</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>6081000.0</td>\n",
       "      <td>248500.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>2001Q4</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AAON INC  $0.004</td>\n",
       "      <td>COM PAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77019</th>\n",
       "      <td>000360206</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3756000.0</td>\n",
       "      <td>128200.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>2001Q3</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>#£AAON INC  $0.004</td>\n",
       "      <td>COM PAR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>77020 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           cusip nameOfIssuer titleOfClass      value    shares shrsOrPrnAmt  \\\n",
       "0      Y8565J101          COM          COM   507000.0   15190.0           SH   \n",
       "1      Y8564W103         None         None        0.0       0.0           SH   \n",
       "2      Y8564W103         None         None   576000.0   25300.0           SH   \n",
       "3      Y8564W103         None         None   217000.0    9900.0           SH   \n",
       "4      Y8564W103         None         None   162000.0   11426.0           SH   \n",
       "...          ...          ...          ...        ...       ...          ...   \n",
       "77015  000360206         None         None  4145000.0  245000.0           SH   \n",
       "77016  000360206         None         None  4276000.0  227800.0           SH   \n",
       "77017  000360206         None         None        0.0       0.0           SH   \n",
       "77018  000360206         None         None  6081000.0  248500.0           SH   \n",
       "77019  000360206         None         None  3756000.0  128200.0           SH   \n",
       "\n",
       "      putCall quarter ticker               cusip_description  \\\n",
       "0        None  2013Q3    TOO  TEEKAY OFFSHORE PARTNERS LP CO   \n",
       "1        None  2010Q2     TK  TEEKAY CORPORATION (MARSHALL I   \n",
       "2        None  2010Q1     TK  TEEKAY CORPORATION (MARSHALL I   \n",
       "3        None  2009Q3     TK  TEEKAY CORPORATION (MARSHALL I   \n",
       "4        None  2009Q1     TK  TEEKAY CORPORATION (MARSHALL I   \n",
       "...       ...     ...    ...                             ...   \n",
       "77015    None  2002Q3   None                            None   \n",
       "77016    None  2002Q2   None                            None   \n",
       "77017    None  2002Q1   None                            None   \n",
       "77018    None  2001Q4   None                            None   \n",
       "77019    None  2001Q3   None                            None   \n",
       "\n",
       "                       nameofissuer    titleofclass  \n",
       "0      TEEKAY OFFSHORE PARTNERS L P  PARTNERSHIP UN  \n",
       "1                TEEKAY CORPORATION             COM  \n",
       "2                TEEKAY CORPORATION             COM  \n",
       "3                TEEKAY CORPORATION             COM  \n",
       "4                TEEKAY CORPORATION             COM  \n",
       "...                             ...             ...  \n",
       "77015               AADN INC  $0004         COM PAR  \n",
       "77016              AAON INC  $0.004         COM PAR  \n",
       "77017              AAON INC  $0.004         COM PAR  \n",
       "77018              AAON INC  $0.004         COM PAR  \n",
       "77019            #£AAON INC  $0.004         COM PAR  \n",
       "\n",
       "[77020 rows x 12 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# when merging, if left, the values that exist on the left but not on the right, will get displayed, but for the right df, there will be nan for the common column\n",
    "\n",
    "(df3.merge(df4, left_on=['quarter', 'cusip'], right_on=['quarter', 'cusip'], how='left'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-15T16:25:01.601499Z",
     "iopub.status.busy": "2022-09-15T16:25:01.600516Z",
     "iopub.status.idle": "2022-09-15T16:25:02.423495Z",
     "shell.execute_reply": "2022-09-15T16:25:02.423495Z",
     "shell.execute_reply.started": "2022-09-15T16:25:01.601499Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cusip</th>\n",
       "      <th>nameOfIssuer</th>\n",
       "      <th>titleOfClass</th>\n",
       "      <th>value</th>\n",
       "      <th>shares</th>\n",
       "      <th>shrsOrPrnAmt</th>\n",
       "      <th>putCall</th>\n",
       "      <th>quarter</th>\n",
       "      <th>ticker</th>\n",
       "      <th>cusip_description</th>\n",
       "      <th>nameofissuer</th>\n",
       "      <th>titleofclass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Y8565J101</td>\n",
       "      <td>COM</td>\n",
       "      <td>COM</td>\n",
       "      <td>507000.0</td>\n",
       "      <td>15190.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>2013Q3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Y8564W103</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>2010Q2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Y8564W103</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>576000.0</td>\n",
       "      <td>25300.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>2010Q1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Y8564W103</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>217000.0</td>\n",
       "      <td>9900.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>2009Q3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Y8564W103</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>162000.0</td>\n",
       "      <td>11426.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>2009Q1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76874</th>\n",
       "      <td>000360206</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>4145000.0</td>\n",
       "      <td>245000.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>2002Q3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76875</th>\n",
       "      <td>000360206</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>4276000.0</td>\n",
       "      <td>227800.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>2002Q2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76876</th>\n",
       "      <td>000360206</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>2002Q1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76877</th>\n",
       "      <td>000360206</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>6081000.0</td>\n",
       "      <td>248500.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>2001Q4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76878</th>\n",
       "      <td>000360206</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3756000.0</td>\n",
       "      <td>128200.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>2001Q3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76879 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           cusip nameOfIssuer titleOfClass      value    shares shrsOrPrnAmt  \\\n",
       "0      Y8565J101          COM          COM   507000.0   15190.0           SH   \n",
       "1      Y8564W103         None         None        0.0       0.0           SH   \n",
       "2      Y8564W103         None         None   576000.0   25300.0           SH   \n",
       "3      Y8564W103         None         None   217000.0    9900.0           SH   \n",
       "4      Y8564W103         None         None   162000.0   11426.0           SH   \n",
       "...          ...          ...          ...        ...       ...          ...   \n",
       "76874  000360206         None         None  4145000.0  245000.0           SH   \n",
       "76875  000360206         None         None  4276000.0  227800.0           SH   \n",
       "76876  000360206         None         None        0.0       0.0           SH   \n",
       "76877  000360206         None         None  6081000.0  248500.0           SH   \n",
       "76878  000360206         None         None  3756000.0  128200.0           SH   \n",
       "\n",
       "      putCall quarter ticker cusip_description nameofissuer titleofclass  \n",
       "0        None  2013Q3    NaN               NaN          NaN          NaN  \n",
       "1        None  2010Q2    NaN               NaN          NaN          NaN  \n",
       "2        None  2010Q1    NaN               NaN          NaN          NaN  \n",
       "3        None  2009Q3    NaN               NaN          NaN          NaN  \n",
       "4        None  2009Q1    NaN               NaN          NaN          NaN  \n",
       "...       ...     ...    ...               ...          ...          ...  \n",
       "76874    None  2002Q3    NaN               NaN          NaN          NaN  \n",
       "76875    None  2002Q2    NaN               NaN          NaN          NaN  \n",
       "76876    None  2002Q1    NaN               NaN          NaN          NaN  \n",
       "76877    None  2001Q4    NaN               NaN          NaN          NaN  \n",
       "76878    None  2001Q3    NaN               NaN          NaN          NaN  \n",
       "\n",
       "[76879 rows x 12 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#different style of .merge()\n",
    "(df3.merge(df4.assign(nameOfIssuer = df4.nameofissuer), how='left'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-15T16:29:08.784436Z",
     "iopub.status.busy": "2022-09-15T16:29:08.784436Z",
     "iopub.status.idle": "2022-09-15T16:29:09.374624Z",
     "shell.execute_reply": "2022-09-15T16:29:09.374624Z",
     "shell.execute_reply.started": "2022-09-15T16:29:08.784436Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cusip</th>\n",
       "      <th>nameOfIssuer</th>\n",
       "      <th>titleOfClass</th>\n",
       "      <th>value</th>\n",
       "      <th>shares</th>\n",
       "      <th>shrsOrPrnAmt</th>\n",
       "      <th>putCall</th>\n",
       "      <th>quarter</th>\n",
       "      <th>ticker</th>\n",
       "      <th>cusip_description</th>\n",
       "      <th>nameofissuer</th>\n",
       "      <th>titleofclass</th>\n",
       "      <th>_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Y8565J101</td>\n",
       "      <td>COM</td>\n",
       "      <td>COM</td>\n",
       "      <td>507000.0</td>\n",
       "      <td>15190.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>2013Q3</td>\n",
       "      <td>TOO</td>\n",
       "      <td>TEEKAY OFFSHORE PARTNERS LP CO</td>\n",
       "      <td>TEEKAY OFFSHORE PARTNERS L P</td>\n",
       "      <td>PARTNERSHIP UN</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Y8564W103</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>2010Q2</td>\n",
       "      <td>TK</td>\n",
       "      <td>TEEKAY CORPORATION (MARSHALL I</td>\n",
       "      <td>TEEKAY CORPORATION</td>\n",
       "      <td>COM</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Y8564W103</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>576000.0</td>\n",
       "      <td>25300.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>2010Q1</td>\n",
       "      <td>TK</td>\n",
       "      <td>TEEKAY CORPORATION (MARSHALL I</td>\n",
       "      <td>TEEKAY CORPORATION</td>\n",
       "      <td>COM</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Y8564W103</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>217000.0</td>\n",
       "      <td>9900.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>2009Q3</td>\n",
       "      <td>TK</td>\n",
       "      <td>TEEKAY CORPORATION (MARSHALL I</td>\n",
       "      <td>TEEKAY CORPORATION</td>\n",
       "      <td>COM</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Y8564W103</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>162000.0</td>\n",
       "      <td>11426.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>2009Q1</td>\n",
       "      <td>TK</td>\n",
       "      <td>TEEKAY CORPORATION (MARSHALL I</td>\n",
       "      <td>TEEKAY CORPORATION</td>\n",
       "      <td>COM</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77015</th>\n",
       "      <td>000360206</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>4145000.0</td>\n",
       "      <td>245000.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>2002Q3</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AADN INC  $0004</td>\n",
       "      <td>COM PAR</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77016</th>\n",
       "      <td>000360206</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>4276000.0</td>\n",
       "      <td>227800.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>2002Q2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AAON INC  $0.004</td>\n",
       "      <td>COM PAR</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77017</th>\n",
       "      <td>000360206</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>2002Q1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AAON INC  $0.004</td>\n",
       "      <td>COM PAR</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77018</th>\n",
       "      <td>000360206</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>6081000.0</td>\n",
       "      <td>248500.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>2001Q4</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AAON INC  $0.004</td>\n",
       "      <td>COM PAR</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77019</th>\n",
       "      <td>000360206</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3756000.0</td>\n",
       "      <td>128200.0</td>\n",
       "      <td>SH</td>\n",
       "      <td>None</td>\n",
       "      <td>2001Q3</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>#£AAON INC  $0.004</td>\n",
       "      <td>COM PAR</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>77020 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           cusip nameOfIssuer titleOfClass      value    shares shrsOrPrnAmt  \\\n",
       "0      Y8565J101          COM          COM   507000.0   15190.0           SH   \n",
       "1      Y8564W103         None         None        0.0       0.0           SH   \n",
       "2      Y8564W103         None         None   576000.0   25300.0           SH   \n",
       "3      Y8564W103         None         None   217000.0    9900.0           SH   \n",
       "4      Y8564W103         None         None   162000.0   11426.0           SH   \n",
       "...          ...          ...          ...        ...       ...          ...   \n",
       "77015  000360206         None         None  4145000.0  245000.0           SH   \n",
       "77016  000360206         None         None  4276000.0  227800.0           SH   \n",
       "77017  000360206         None         None        0.0       0.0           SH   \n",
       "77018  000360206         None         None  6081000.0  248500.0           SH   \n",
       "77019  000360206         None         None  3756000.0  128200.0           SH   \n",
       "\n",
       "      putCall quarter ticker               cusip_description  \\\n",
       "0        None  2013Q3    TOO  TEEKAY OFFSHORE PARTNERS LP CO   \n",
       "1        None  2010Q2     TK  TEEKAY CORPORATION (MARSHALL I   \n",
       "2        None  2010Q1     TK  TEEKAY CORPORATION (MARSHALL I   \n",
       "3        None  2009Q3     TK  TEEKAY CORPORATION (MARSHALL I   \n",
       "4        None  2009Q1     TK  TEEKAY CORPORATION (MARSHALL I   \n",
       "...       ...     ...    ...                             ...   \n",
       "77015    None  2002Q3   None                            None   \n",
       "77016    None  2002Q2   None                            None   \n",
       "77017    None  2002Q1   None                            None   \n",
       "77018    None  2001Q4   None                            None   \n",
       "77019    None  2001Q3   None                            None   \n",
       "\n",
       "                       nameofissuer    titleofclass _merge  \n",
       "0      TEEKAY OFFSHORE PARTNERS L P  PARTNERSHIP UN   both  \n",
       "1                TEEKAY CORPORATION             COM   both  \n",
       "2                TEEKAY CORPORATION             COM   both  \n",
       "3                TEEKAY CORPORATION             COM   both  \n",
       "4                TEEKAY CORPORATION             COM   both  \n",
       "...                             ...             ...    ...  \n",
       "77015               AADN INC  $0004         COM PAR   both  \n",
       "77016              AAON INC  $0.004         COM PAR   both  \n",
       "77017              AAON INC  $0.004         COM PAR   both  \n",
       "77018              AAON INC  $0.004         COM PAR   both  \n",
       "77019            #£AAON INC  $0.004         COM PAR   both  \n",
       "\n",
       "[77020 rows x 13 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "(df3.merge(df4, left_on=['quarter', 'cusip'], right_on=['quarter', 'cusip'], how='left' , indicator=True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T21:42:55.976416Z",
     "iopub.status.busy": "2022-09-14T21:42:55.976416Z",
     "iopub.status.idle": "2022-09-14T21:42:55.989415Z",
     "shell.execute_reply": "2022-09-14T21:42:55.988416Z",
     "shell.execute_reply.started": "2022-09-14T21:42:55.976416Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1 = (df1.merge(df2.assign(name=df2.Name), how='right')\n",
    "    .assign(pet = df1.pet.fillna(\"no_pet\"),\n",
    "            name = df1.name.replace(\"Suzy\", 'No_Suzy')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T21:05:29.407827Z",
     "iopub.status.busy": "2022-09-14T21:05:29.407827Z",
     "iopub.status.idle": "2022-09-14T21:05:29.423852Z",
     "shell.execute_reply": "2022-09-14T21:05:29.422856Z",
     "shell.execute_reply.started": "2022-09-14T21:05:29.407827Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>pet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fred</td>\n",
       "      <td>Dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Suzy</td>\n",
       "      <td>Dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Suzy</td>\n",
       "      <td>Cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bob</td>\n",
       "      <td>Fish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name   pet\n",
       "0  Fred   Dog\n",
       "1  Suzy   Dog\n",
       "2  Suzy   Cat\n",
       "3   Bob  Fish"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# conditions = [\n",
    "#     df['gender'].eq('male') & df['pet1'].eq(df['pet2']),\n",
    "#     df['gender'].eq('female') & df['pet1'].isin(['cat', 'dog'])]\n",
    "\n",
    "# choices = [5,5]\n",
    "# df['points'] = np.select(conditions, choices, default=0)\n",
    "# d1.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T21:45:58.378036Z",
     "iopub.status.busy": "2022-09-14T21:45:58.377037Z",
     "iopub.status.idle": "2022-09-14T21:45:58.396037Z",
     "shell.execute_reply": "2022-09-14T21:45:58.395082Z",
     "shell.execute_reply.started": "2022-09-14T21:45:58.377037Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     True\n",
       "1    False\n",
       "2    False\n",
       "3     True\n",
       "4     True\n",
       "5     True\n",
       "6    False\n",
       "7     True\n",
       "8     True\n",
       "Name: transaction, dtype: bool"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditions = [df1['name'].eq('No_Suzy') | df1['name'].eq(df1['pet']),\n",
    "    df1['Name'].eq('Fred') & df1['Color'].isin(['Green', 'Red'])]\n",
    "\n",
    "choices = ['OPEN','CLOSE']\n",
    "df1['transaction'] = np.select(conditions, choices, default=None)\n",
    "df1.transaction.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code snippet is for when we are not sure if the column `transaction_value` exists at all. \n",
    "# If it doesn't, it will get automatically created with the default value\n",
    "full_df['transaction_value'] = full_df.get('transaction_value',default=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-15T15:37:41.147244Z",
     "iopub.status.busy": "2022-09-15T15:37:41.147244Z",
     "iopub.status.idle": "2022-09-15T15:37:41.169323Z",
     "shell.execute_reply": "2022-09-15T15:37:41.168302Z",
     "shell.execute_reply.started": "2022-09-15T15:37:41.147244Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>foo1</td>\n",
       "      <td>2009-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>foo2</td>\n",
       "      <td>2009-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>foo3</td>\n",
       "      <td>2009-01-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>foo4</td>\n",
       "      <td>2009-01-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>foo5</td>\n",
       "      <td>2009-01-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     A    B     C          D\n",
       "0  0.0  0.0  foo1 2009-01-01\n",
       "1  1.0  1.0  foo2 2009-01-02\n",
       "2  2.0  0.0  foo3 2009-01-05\n",
       "3  3.0  1.0  foo4 2009-01-06\n",
       "4  4.0  0.0  foo5 2009-01-07"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dframe = pd.util.testing.makeMixedDataFrame(); dframe\n",
    "# dframe2 = pd.util.testing.makeDataFrame(); dframe2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T23:16:09.507844Z",
     "iopub.status.busy": "2022-08-09T23:16:09.507844Z",
     "iopub.status.idle": "2022-08-09T23:16:09.523506Z",
     "shell.execute_reply": "2022-08-09T23:16:09.523506Z",
     "shell.execute_reply.started": "2022-08-09T23:16:09.507844Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1 = (df.merge(cusip_md_df[['cusip', 'quarter','ticker', 'cusip_description', 'nameofissuer', 'titleofclass']], on=['cusip', 'quarter'])\n",
    "      .rename(columns={'ticker':'cusip_ticker'}))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "89cd8330c6000ed2b4e9cbcae2986a8c07c6071da3ea6a959671268f16d3c726"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
